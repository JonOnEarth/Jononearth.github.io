<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/02/15/hello-world/"/>
      <url>/2020/02/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2019总结大会</title>
      <link href="/2020/01/01/2019%E6%80%BB%E7%BB%93%E5%A4%A7%E4%BC%9A/"/>
      <url>/2020/01/01/2019%E6%80%BB%E7%BB%93%E5%A4%A7%E4%BC%9A/</url>
      
        <content type="html"><![CDATA[<p>3年前的这一天，拖着沉重的躯体来到美国，充满对未知的害怕与兴奋。<br>设想了自己接下来的道路，饱含自信，却源于对环境和自我的无知。  </p><p>2年前快毕业时，仍然对工作与未来产生恐惧，还是准备不充分吗？应该这么说，永远没有充分的时候。   </p><p>彷佛是宇宙听见了我内心的呼唤，竟然给了一个读博的机会：你总认为自己学的不到位，那好了，给你个博士读读，你该没有借口了吧。  </p><h2 id="对于学习："><a href="#对于学习：" class="headerlink" title="对于学习："></a>对于学习：</h2><p>这一年，应该学的很多，从最初听不懂他们的讨论，到现在可以参与进去，并逐级能探索和主导研究的方向。</p><p>我们组特别棒，尤其是老板。西班牙人，做任何事情，从来都是有条不紊，非常具有亲和力，在他手下，想能学到不少学术之外的东西。</p><h3 id="整理下学了哪些个东西呢："><a href="#整理下学了哪些个东西呢：" class="headerlink" title="整理下学了哪些个东西呢："></a>整理下学了哪些个东西呢：</h3><ol><li>这一年主要是在折腾贝叶斯滤波。kalman, uncertainty, GP等。</li><li>代码能力的提升，从很乱，到现在有条理些。</li><li>神经网络现在认知没怎么提高，学会一个bayesian NN</li><li>科研能力提升。论文看的不再很慢，很不懂。</li><li>参加了ion会议，发表一篇会议论文，并演讲，还引起多个的人兴趣，包括Ford, Apple.</li></ol><h3 id="做的太不够："><a href="#做的太不够：" class="headerlink" title="做的太不够："></a>做的太不够：</h3><ol><li>9月份开始，研究基本停滞，停留在原来的研究上。尽管学了新东西，还没有应用到上面。</li><li>平时并没有全心全意放到科研上。这一块下面分析：当压力来的时候，需要调节。需要习惯的力量，不应该依赖自制力。</li><li>博客教程从下半年开始，基本停滞了。没有借口。</li></ol><h3 id="接下来"><a href="#接下来" class="headerlink" title="接下来"></a>接下来</h3><ol><li>一月份之前论文雏形出来，关于bayesian NN训练的；</li><li>4月份之前完成IPIN会议的论文提交；</li><li>找一份关于研究相关的实习（待）</li><li>按照后面做的6小时工作法，进行学习。</li><li>读至少100篇文论，每个月读10篇至少，一周2~3篇。</li></ol><h2 id="对于生活："><a href="#对于生活：" class="headerlink" title="对于生活："></a>对于生活：</h2><p>不知道对于生活，能聊什么。</p><p>喜欢自己呆着，也喜欢跟朋友出去觅食。<br>也不怎么想认识新朋友，但老朋友快走光了都。 </p><p>有一颗想出去浪的心，但没钱，没时间，没朋友:cry:。 </p><p>想搞对象了，但不愿意将就(扯吧就是找不到)。出国前，谁他妈跟我说出国一抓一大把，我一定是留错了学。</p><h3 id="生活得到："><a href="#生活得到：" class="headerlink" title="生活得到："></a>生活得到：</h3><ol><li>找了个学校对面的住宿。虽然破了点，但太方便了吧。非常适合我这种懒人。</li><li>去哪旅游了？刚去了纽约圣诞，让我翻翻相册，还有哪。有在开会间歇看的迈阿密，墙绘涂鸦很赞；有去附近的海边和小岛。然后没了貌似。</li><li>想买个游戏机，但一直没勇气…后来买了个FB的VR眼镜，果然玩了一会现在都不怎么碰了。</li></ol><h3 id="做的不够："><a href="#做的不够：" class="headerlink" title="做的不够："></a>做的不够：</h3><ol><li>生活首先没有规律，经常性的想放飞自我。</li><li>好久没跟老朋友联系了</li><li>没有认识更多的人</li></ol><h3 id="接下来-1"><a href="#接下来-1" class="headerlink" title="接下来"></a>接下来</h3><ol><li>形成习惯，每天早上健身，保证3个小时学习；</li><li>坚持博客创作，每天写在什么地方，可以周末的时候总结。</li><li>去真诚的认识更多的人，也要跟老朋友不时的联系。</li><li>找到对的那个人。</li><li>再强调一遍，有规律，健身，早睡。</li></ol><h2 id="对于创业"><a href="#对于创业" class="headerlink" title="对于创业"></a>对于创业</h2><p>这一块从加入创业团队，到成型，到上线起到了至关重要的作用。<br>做的不够的地方也很多：主要就是没能够花到大量的时间在上面，所以很多功能不是很完善。  </p><h2 id="其他的瞎逼逼"><a href="#其他的瞎逼逼" class="headerlink" title="其他的瞎逼逼"></a>其他的瞎逼逼</h2><h3 id="专注"><a href="#专注" class="headerlink" title="专注"></a>专注</h3><p>我这个人，对什么都感兴趣，但都是3分钟热度。  </p><p>小时候还没有电脑手机，无聊的时候，我自己一个人怎么打发时间。从看小人书，搭积木，到自己挖泥巴组建恐龙战队，搞水泥造四驱车赛道也改装过四驱车，再到自己发明象棋冰球新玩法等等。也写过小说，画过画，吹过笛子，练过‘武’… 我去，原来那时候的童年这么有趣的嘛</p><p>很怕无聊，需要新兴的刺激，但这些刺激都是内驱的，外在的刺激有时候会反其道行之。而且刺激来的快，去的也快。一旦努力，无聊渐渐大于刺激感后，就很难继续实施下去了。</p><p>喜欢创造，然而特别怕陷入创造过后，精细化，专业化的无聊。所以学过很多东西，都半途而废了。浮于表面的总是充满各种欢乐，潜入底层的才能使创造不断延伸开。</p><p>长大了，道理是明白了些。<br>然人却变得越来越浮躁。再也不能如小时候，那般无所事事的仰望蓝天。所有的人告诉你，你要这么做，那样做，不做就落后，不做就失败。看起来你要学的东西太多了，却缺失了专注。</p><p>我们都说要掌握一技之长，然人变得不再纯粹，一技之长再难更长。</p><p>拒绝外在的干扰，太难。什么都不管不顾，又不是一个成年人该有的生活态度。经常萌发这样的想法，自己躲起来，闭门造车数十载，一招惊世。屁，躲不过1个星期。既然躲不过，何不正心对待。</p><p>推崇斯多葛学派：掌控自己能掌控的，做好自己，接下来的不能掌控的就留给老天，不纠结自己不能掌控的事与人。</p><p>2020，请少玩手机，多看书。杜绝睡前不摸下手机，都睡不着觉的坏习惯。</p><h3 id="厚脸"><a href="#厚脸" class="headerlink" title="厚脸"></a>厚脸</h3><p>其实还是做事。厚脸的人，做事更加容易，做自己其实也更加容易。</p><p>我们的文化总是把厚脸作为一种贬义词。但奇怪的是，做人做事成功者，无一不是厚脸者。中国很多的事情，我都怀疑是成功者不想让人后者上位，有意传播的‘歪门邪道’。</p><p>还比如说盛赞老实人…</p><p>厚脸不好做，它意味着你不惧无端指责，意味着你虚心接受批评，意味着你不在乎他人看法，意味着敢于实践不退缩，意味着为达目的‘不择手段’(注意，此处不择手段的对象是自己)，意味着不怕负担责任…</p><p>2020，要把脸涂厚一点，过滤掉别人的看法或者拒绝，正念面对当下。</p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站备忘</title>
      <link href="/2019/12/30/%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98/"/>
      <url>/2019/12/30/%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98/</url>
      
        <content type="html"><![CDATA[<h3 id="更新于2020-2"><a href="#更新于2020-2" class="headerlink" title="更新于2020.2"></a>更新于2020.2</h3><p>想换到其他静态博客平台，hugo等。但折腾了很长时间，还是回到了hexo平台，原因是hexo支持比较丰富，而且格式使用hexo的方式写的，改到其他的平台，实在是改起来力不从心。<br>换了电脑，hexo同步是一大问题，待解决这个。<br>要有一颗谦卑的心，以为自己弄过就会搞，很容易忘掉，浪费精力，重复劳动。  </p><h3 id="Mac-安装注意事项及教程"><a href="#Mac-安装注意事项及教程" class="headerlink" title="Mac 安装注意事项及教程"></a>Mac 安装注意事项及教程</h3><ul><li><a href="https://bowenli86.github.io/2016/01/23/hexo/Hexo-How-to-install-hexo-on-Mac-with-github-pages/" target="_blank" rel="noopener">安装教程</a>，<a href="https://hexo.io/docs/#Install-Hexo" target="_blank" rel="noopener">安装2</a></li><li>安装npm的 permission 错误及<a href="https://stackoverflow.com/questions/33725639/npm-install-g-less-does-not-work-eacces-permission-denied" target="_blank" rel="noopener">解决办法</a></li><li>安装hexo后，需要运行代码npx hexo .. 不是直接hexo</li></ul><h3 id="hexo同步到github"><a href="#hexo同步到github" class="headerlink" title="hexo同步到github"></a>hexo同步到github</h3><p>把整个文件夹同步到github上，换台电脑直接在clone下来，在该文件夹下创建hexo，不会覆盖clone下来的文件，所以直接就可以用了</p><h3 id="书写格式备忘markdown"><a href="#书写格式备忘markdown" class="headerlink" title="书写格式备忘markdown"></a>书写格式备忘<a href="https://www.jianshu.com/p/b03a8d7b1719" target="_blank" rel="noopener">markdown</a></h3><h4 id="插入链接"><a href="#插入链接" class="headerlink" title="插入链接"></a>插入链接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例1</span><br><span class="line">[markdown](https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;b03a8d7b1719)</span><br><span class="line">例2</span><br><span class="line">[百度2][2]&#123;:target&#x3D;&quot;_blank&quot;&#125;</span><br><span class="line">[2]: http:&#x2F;&#x2F;www.baidu.com&#x2F;   &quot;百度二下&quot;</span><br></pre></td></tr></table></figure><h4 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](.&#x2F;01.png &#39;描述&#39;)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src&#x3D;&quot;http:...&quot; width &#x3D; &quot;100&quot; height &#x3D; &quot;100&quot; div align&#x3D;right &#x2F;&gt;</span><br></pre></td></tr></table></figure><p>也可以输出百分比多少，width =20%, height=20%</p><h4 id="插入图片带有链接"><a href="#插入图片带有链接" class="headerlink" title="插入图片带有链接"></a>插入图片带有链接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[![](.&#x2F;01.png &#39;百度&#39;)](http:&#x2F;&#x2F;www.baidu.com)</span><br></pre></td></tr></table></figure><h4 id="Markdown-在-Atom-中的-preview："><a href="#Markdown-在-Atom-中的-preview：" class="headerlink" title="Markdown 在 Atom 中的 preview："></a>Markdown 在 Atom 中的 preview：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shift + ctrl + m</span><br></pre></td></tr></table></figure><h4 id="markdown创建表格"><a href="#markdown创建表格" class="headerlink" title="markdown创建表格"></a>markdown创建表格</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|id|name|</span><br><span class="line">|:-|:-|</span><br><span class="line">|1|A1|</span><br><span class="line">|2|A2|</span><br><span class="line">|3|A3|</span><br></pre></td></tr></table></figure><p>效果如下：</p><table><thead><tr><th align="left">id</th><th align="left">name</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A1</td></tr><tr><td align="left">2</td><td align="left">A2</td></tr><tr><td align="left">3</td><td align="left">A3</td></tr></tbody></table><p>表格调整：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 默认左对齐</span><br><span class="line">:- 左对齐</span><br><span class="line">-:右对齐</span><br><span class="line">:-:居中</span><br></pre></td></tr></table></figure><p>列宽度调整：<br>-表示列的宽度权重，比如如下，–、-，表示第一列的宽度是第二列的俩倍：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">|id|name|</span><br><span class="line">|:--|:-|</span><br><span class="line">|1|A1|</span><br></pre></td></tr></table></figure><p>效果如下：</p><table><thead><tr><th align="left">id</th><th align="left">name</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A1</td></tr></tbody></table><h3 id="markdown-emoji"><a href="#markdown-emoji" class="headerlink" title="markdown emoji"></a>markdown emoji</h3><p>文字已经不能表达我的愤怒了，来个 :rage:</p><p><a href="https://www.cnblogs.com/chenych/p/8623353.html" target="_blank" rel="noopener">Emoji表情</a><br><a href="https://www.webpagefx.com/tools/emoji-cheat-sheet/" target="_blank" rel="noopener">github emoji</a></p><h3 id="Github和hexo建站-tutorial"><a href="#Github和hexo建站-tutorial" class="headerlink" title="Github和hexo建站 tutorial"></a>Github和hexo建站 tutorial</h3><ul><li><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">官方中文文档</a></p></li><li><p><a href="https://blog.csdn.net/working_harder/article/details/52437783" target="_blank" rel="noopener">github+hexo</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/22498474" target="_blank" rel="noopener">Hexo(2)-部署博客及更新博文</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/78467553" target="_blank" rel="noopener">git怎么部署</a></p></li><li><p><a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="noopener">我用的主题</a></p></li></ul><h3 id="安装时的问题："><a href="#安装时的问题：" class="headerlink" title="安装时的问题："></a>安装时的问题：</h3><ul><li><a href="https://blog.csdn.net/qq_21808961/article/details/84476504" target="_blank" rel="noopener">hexo d命令报错 ERROR Deployer not found: git</a><br>npm install hexo-deployer-git –save</li></ul><h4 id="写博文"><a href="#写博文" class="headerlink" title="写博文"></a>写博文</h4><ul><li><p>创建新页面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;about&quot;</span><br></pre></td></tr></table></figure></li><li><p>创建笔记</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;Hexo教程&quot;</span><br></pre></td></tr></table></figure></li><li><p>创建draft</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new [layout] &lt;title&gt; 这里的layout&#x3D;draft</span><br><span class="line">if you want to pushish the draft</span><br><span class="line">hexo publish draft &quot;title&quot;</span><br></pre></td></tr></table></figure></li><li><p><img src="/images/githubhexo1.png" alt="post_title"></p></li><li><p>引用站内文章<br>方法1：markdown 语法插入已有链接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[引用文章](http:jononearth&#x2F;categoriesname&#x2F;title)</span><br></pre></td></tr></table></figure><p>方法2：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% post_link 文章文件名（不要后缀） 文章标题（可选） %&#125;</span><br><span class="line">如：</span><br><span class="line">&#123;% post_link Hello-World %&#125;</span><br><span class="line">&#123;% post_link Hello-World 你好世界 %&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="发博文"><a href="#发博文" class="headerlink" title="发博文"></a>发博文</h4><p>在 Git Shell 中进入 Hexo 文件夹，执行下面几条命令，将博客部署到 GitHub 上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">(若要本地预览就先执行 hexo server)</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">或者直接用组合</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure><p>快捷命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo g &#x3D;&#x3D; hexo generate</span><br><span class="line">hexo d &#x3D;&#x3D; hexo deploy</span><br><span class="line">hexo s &#x3D;&#x3D; hexo server</span><br><span class="line">hexo n &#x3D;&#x3D; hexo new</span><br></pre></td></tr></table></figure><p>另外还有一些其他方法来发布博文，如 <a href="https://www.zhihu.com/question/27384681/answer/87037317" target="_blank" rel="noopener">hexo-admin插件</a></p><h4 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h4><p><a href="https://blog.csdn.net/sherlockzoom/article/details/43835613" target="_blank" rel="noopener">mathjax + latex</a><br><a href="http://www.codecogs.com/latex/eqneditor.php" target="_blank" rel="noopener">在线转换工具</a></p><p>行间公式用两个 $$ 符号表示，行内用 一个 $ 表示。</p><h4 id="插入视频"><a href="#插入视频" class="headerlink" title="插入视频"></a>插入视频</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;video src&#x3D;&#39;https:&#x2F;&#x2F;youtu.be&#x2F;Boy3zHVrWB4&#39; type&#x3D;&#39;video&#x2F;mp4&#39; controls&#x3D;&#39;controls&#39;  width&#x3D;&#39;100%&#39; height&#x3D;&#39;100%&#39;&gt;&lt;&#x2F;video&gt;</span><br></pre></td></tr></table></figure><p>or  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% raw %&#125;&lt;iframe width&#x3D;&quot;854&quot; height&#x3D;&quot;480&quot; src&#x3D;&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;embed&#x2F;Boy3zHVrWB4&quot; frameborder&#x3D;&quot;0&quot; allow&#x3D;&quot;autoplay; encrypted-media&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure><h3 id="Github和jekyll建站-tutorial"><a href="#Github和jekyll建站-tutorial" class="headerlink" title="Github和jekyll建站 tutorial"></a>Github和jekyll建站 tutorial</h3><p><a href="https://www.jekyll.com.cn/docs/" target="_blank" rel="noopener">jekyll tutorial</a></p><p><a href="https://blog.csdn.net/xudailong_blog/article/details/78762262" target="_blank" rel="noopener">github快速建博客指南</a></p>]]></content>
      
      
      <categories>
          
          <category> 极客时间 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 建站 </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning 资源汇总</title>
      <link href="/2019/12/29/2018-05-07-machine-leaning-index/"/>
      <url>/2019/12/29/2018-05-07-machine-leaning-index/</url>
      
        <content type="html"><![CDATA[<p>机器学习资源整理，网站地图(Website Maps)</p><blockquote><p>Warning</p><blockquote><p>This document is under early stage development. If you find errors, please raise an issue or contribute a better definition!</p></blockquote></blockquote><h2 id="Some-cheatsheets"><a href="#Some-cheatsheets" class="headerlink" title="Some cheatsheets:"></a>Some cheatsheets:</h2><p><a href="https://zhuanlan.zhihu.com/p/28467910" target="_blank" rel="noopener">值得收藏的 27 个机器学习的小抄</a></p><p><a href="http://ml-cheatsheet.readthedocs.io/en/latest/" target="_blank" rel="noopener">Machine Learning Cheatsheet</a></p><p><a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/" target="_blank" rel="noopener">如何在 Kaggle 首战中进入前 10%</a></p><h2 id="非常值得一看的机器学习博客-持续更新-："><a href="#非常值得一看的机器学习博客-持续更新-：" class="headerlink" title="非常值得一看的机器学习博客(持续更新)："></a>非常值得一看的机器学习博客(持续更新)：</h2><ul><li><a href="https://chrisalbon.com/" target="_blank" rel="noopener">chrisalbon博客</a>, 废话不多，但很直观。直接上代码，值得多看看。<img src="/images/chrisalbon博客.png" width = 80% height = 80% div align=center /></li></ul><h2 id="值得一看的教程"><a href="#值得一看的教程" class="headerlink" title="值得一看的教程"></a>值得一看的教程</h2><p><a href="http://www.julyedu.com/video/play/18/429" target="_blank" rel="noopener">机器学习公开课</a></p><p><a href="http://www.julyedu.com/video/play/25" target="_blank" rel="noopener">算法公开课</a></p><h2 id="网站地图："><a href="#网站地图：" class="headerlink" title="网站地图："></a>网站地图：</h2><p>Machine learning with Python 系列：</p><ul><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-1/">machine learning with python(1)</a>: 环境搭建，及Python处理数据基础教程</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-2/">machine learning with python(2)</a>: 理解你的数据常用手段</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-3/">machine learning with python(3)</a>：特征工程</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Machine-learning-with-python-4-classical-algorithm/">Machine learning with python(4)_classical algorithm</a>：几种经典算法整理</li></ul><p>Deep Learning 系列：</p><ul><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Deep-Learning-1/">Deep Learning(1)</a>: 深度学习基本概念</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Deep-Learning-1/">Deep Learning(1)</a>: Keras基本用法，以及几种深度学习算法简介</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/">调参总结</a>: 应用深度学习时的调参总结</li></ul><p>Python语言基础(data方向)：</p><ul><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/python-cheatsheet1/">python 小抄</a></li><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/Numpy-%E5%B0%8F%E6%8A%84/">Numpy 小抄</a></li><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/Pands%E5%B0%8F%E6%8A%84/">Pandas小抄</a></li></ul><p>数据结构和算法：</p><ul><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/Algorithm-1/">Data structure and algorithm(1)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Indoor Positioning Resources Document</title>
      <link href="/2019/12/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E4%B9%8B%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/"/>
      <url>/2019/12/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E4%B9%8B%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>This blog contains the papers I have read for my summer research: Applying machine learning to indoor fingerprinting positioning, especially using deep learning.</p><h2 id="开篇入门-chinese-version-："><a href="#开篇入门-chinese-version-：" class="headerlink" title="开篇入门(chinese version)："></a>开篇入门(chinese version)：</h2><p>Please skip this part if you don’t speak chinese.<br><a href="http://www.cnblogs.com/rubbninja/tag/%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D/" target="_blank" rel="noopener">机器学习与室内定位技术</a></p><h2 id="Overview-of-indoor-positioning"><a href="#Overview-of-indoor-positioning" class="headerlink" title="Overview of indoor positioning"></a>Overview of indoor positioning</h2><ol><li><a href="https://arxiv.org/abs/1709.01015" target="_blank" rel="noopener">A Survey of Indoor Localization Systems and Technologies</a></li><li><a href="https://ieeexplore.iee.org/abstract/document/8400090/" target="_blank" rel="noopener">Overview of indoor positioning system technologies</a></li><li><a href="http://www.mdpi.com/2220-9964/6/5/135/htm" target="_blank" rel="noopener">Indoor Fingerprint Positioning Based on Wi-Fi: An Overview</a></li></ol><h2 id="Dataset-used"><a href="#Dataset-used" class="headerlink" title="Dataset used"></a>Dataset used</h2><p>Crowdsourced WiFi database and benchmark software for indoor positioning</p><ul><li><a href="https://zenodo.org/record/889798#.WvsnbogvzD4" target="_blank" rel="noopener">Dateset1</a>  </li><li><a href="http://www.mdpi.com/2306-5729/2/4/32/htm#B15-data-02-00032" target="_blank" rel="noopener">illustration</a></li></ul><p>Long-Trem WiFi Fingerprinting Dataset for Reserach on Robust Indoor Poitioning</p><ul><li><a href="https://zenodo.org/record/1066041" target="_blank" rel="noopener">Dateset2</a>  </li><li><a href="http://www.mdpi.com/2306-5729/3/1/3/htm" target="_blank" rel="noopener">illustration for dataset2</a></li></ul><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h2><h3 id="deep-learning"><a href="#deep-learning" class="headerlink" title="deep learning"></a>deep learning</h3><ol><li><p>Low-effort place recognition with WiFi fingerprints using deep learning<br><a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8184808048122111760" target="_blank" rel="noopener">https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8184808048122111760</a></p><blockquote><p>This article use deep neural networks and Autoencoders to do the floor Classification, but no positioning prediction. I have done the positioning part, get a good result.<br>code of this paper: <a href="https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning" target="_blank" rel="noopener">github</a></p></blockquote></li><li><p>A Deep Learning Approach to FingerprintingIndoor Localization Solutions<br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215428" target="_blank" rel="noopener">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215428</a></p><blockquote><p>It use two methods to solve the small dataset problem. one is using data augmentation. The sequence of the APs will change(very doubt at this method) ; Anther method is to use transfer learning, only similar dataset can help.<br>github：<a href="https://github.com/MaiziXiao/IndoorLocalization" target="_blank" rel="noopener">https://github.com/MaiziXiao/IndoorLocalization</a></p></blockquote></li><li><p>Large-Scale Location-Aware Services in Access: Hierarchical Building/Floor Classification and Location Estimation Using Wi-Fi Fingerprinting Based on Deep Neural Networks<br><a href="https://www.tandfonline.com/doi/abs/10.1080/01468030.2018.1467515" target="_blank" rel="noopener">https://www.tandfonline.com/doi/abs/10.1080/01468030.2018.1467515</a></p><blockquote><p>get more advance based on paper 1. Not only floor detection, but also positioning estimation.<br><a href="http://kyeongsoo.github.io/research/projects/indoor_localization/index.html" target="_blank" rel="noopener">http://kyeongsoo.github.io/research/projects/indoor_localization/index.html</a></p></blockquote></li><li><p>Indoor Fingerprint Positioning Based on Wi-Fi: An Overview<br><a href="http://www.mdpi.com/2220-9964/6/5/135/htm" target="_blank" rel="noopener">http://www.mdpi.com/2220-9964/6/5/135/htm</a></p><blockquote><p>This is an overview. Two keywords: fingerprint, WiFi</p></blockquote></li><li><p>Learning the Localization Function: Machine Learning Approach to Fingerprinting Localization<br><a href="https://arxiv.org/abs/1803.08153" target="_blank" rel="noopener">https://arxiv.org/abs/1803.08153</a></p><blockquote><p>basically the same as paper 2.</p></blockquote></li></ol><ol start="6"><li><p>CNN based Indoor Localization using RSS Time-Series<br><a href="https://www.researchgate.net/publication/325678644_CNN_based_Indoor_Localization_using_RSS_Time-Series" target="_blank" rel="noopener">https://www.researchgate.net/publication/325678644_CNN_based_Indoor_Localization_using_RSS_Time-Series</a></p><blockquote><p>using CNN to deal with long-term(time—series) dataset</p></blockquote></li><li><p>todo</p></li></ol><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><ol><li><p>Clustering benefits in mobile-centric WiFi positioning in multi-floor buildings<br><a href="https://ieeexplore.ieee.org/abstract/document/7533846/" target="_blank" rel="noopener">https://ieeexplore.ieee.org/abstract/document/7533846/</a></p><blockquote><p>cluster method for our dataset1</p></blockquote></li><li><p>基于K均值聚类算法的位置指纹定位技术(chinese version)<br><a href="https://wenku.baidu.com/view/941a46e0192e45361166f505.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/941a46e0192e45361166f505.html</a></p><blockquote><p>该文章很好的整理了为什么会用到kmeans和knn,以及提到kriging方法可用于创建指纹库</p></blockquote></li><li><p>Comprehensive analysis of distance and similarity measures for Wi-Fi fingerprinting indoor positioning systems<br><a href="https://www.sciencedirect.com/science/article/pii/S0957417415005527" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0957417415005527</a>*</p><blockquote><p>The UJI kNN algorithm for dataset.<br>This article mainly concentrates on the Wi-Fi Indoor positioning systems based on fingerprinting and k-NN. It mentions Non-heard data processing, data preprocessing, and all kinds of distance calculating.</p></blockquote></li></ol><ol start="4"><li>Adaptive K-nearest neighbour algorithm for WiFi fingerprint positioning<br>Website: <a href="https://www.sciencedirect.com/science/article/pii/S240595951830050X" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S240595951830050X</a><blockquote><p>This article focus on how to improve K nearest neighbour algorithm</p></blockquote></li></ol><h3 id="Kriging-algorithm"><a href="#Kriging-algorithm" class="headerlink" title="Kriging algorithm:"></a>Kriging algorithm:</h3><ol><li><p>Method for yielding a database of locationfingerprints in WLAN<br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1522067" target="_blank" rel="noopener">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1522067</a></p><blockquote><p>using kriging to generate fingerprint data</p></blockquote></li><li><p><a href="http://download.atlantis-press.com/php/download_paper.php?id=16366" target="_blank" rel="noopener">Fingerprint Space Building Algorithm with Kriging for Large Positioning Regional Environment</a></p></li><li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7565018" target="_blank" rel="noopener">Applying Kriging Interpolation for WiFiFingerprinting based Indoor Positioning Systems</a></p></li><li><p>kriging tutoril(Chinese version)<br><a href="https://xg1990.com/blog/archives/222" target="_blank" rel="noopener">克里金(Kriging)插值的原理与公式推导</a></p></li></ol><h3 id="other-algorithms-need-arrange"><a href="#other-algorithms-need-arrange" class="headerlink" title="other algorithms need arrange"></a>other algorithms need arrange</h3><ol><li>Dealing with Insufficient Location Fingerprints in Wi-Fi Based Indoor Location Fingerprinting<br><a href="https://www.hindawi.com/journals/wcmc/2017/1268515/" target="_blank" rel="noopener">https://www.hindawi.com/journals/wcmc/2017/1268515/</a></li></ol><h2 id="other-maybe-useful-articles-or-resources"><a href="#other-maybe-useful-articles-or-resources" class="headerlink" title="other maybe useful articles or resources:"></a>other maybe useful articles or resources:</h2><ul><li><p><a href="https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6" target="_blank" rel="noopener">Towards data set augmentation with GANs</a></p><blockquote><p>maybe a way of data augmentation</p></blockquote></li><li><p>App for collecting the data of indoor positioning：<a href="https://github.com/schollz/find" target="_blank" rel="noopener">https://github.com/schollz/find</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 密室逃脱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 室内定位 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每日一吐201812</title>
      <link href="/2019/01/01/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90201812/"/>
      <url>/2019/01/01/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90201812/</url>
      
        <content type="html"><![CDATA[<p>很早之前，坚持过一段时间的，每日吐槽身边的事和人，以及自己。期望自己能够发现生活中有趣，无趣的一面，关注生活，而不是让时间悄悄溜走。</p><p>有了自己的博客，没什么人关注。目前的状态还是蛮喜欢的。努力记录，努力创造。</p><p>主要以生活小见闻，小感悟，以及读书笔记，随笔等为主。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>硕士毕业啦，开启博士啦，放假啦，过节啦，搬家啦，新年啦，加油啦，啦啦啦！</p><h3 id="12-1-2018"><a href="#12-1-2018" class="headerlink" title="12/1/2018"></a>12/1/2018</h3><p>《写在读博之前》<br>想写写关于读博的事情，但不知道自己能写些什么，写多少，所以没必要另开一篇博客来写了。从考虑读博到决定读博的过程中，徘徊犹豫了好久。就像我爱看的奇葩说一样，一会觉得正方读博挺好的，一会觉得反方工作才是上佳的。</p><p>导师问我：“你真的想读博，真的适合读博吗？别回答我，回答你自己”。 我去，还真回答不上来。心底存在着两个声音。一个声音跟我说，适合的，这是个机会，你不是一直很希望自己真正搞懂那些科学技术吗？而不是流于表面。另外一个声音告诉我，适合个屁，那么大岁数了，你看看你同学工作的，创业的，成家的，哪个不比你厉害。还想要在象牙塔里呆多久啊，况且博士你真的能做好吗？</p><p>我是个优柔寡断的人，对于两可的决定，总下不定决心做一个。其实这是贪心，因为不愿意失去任何可以期待得到的事物。但实际上，无论此时做不做决定，最终都会有个决定，而往往，越早做决定，未来的收获会大于此时的不快。短期的不快与长期的收益之间，请尽量选择后者。在考虑读不读博，其实时间上也不允许我能够深入的思考这个问题。记得出国前，一个朋友说，既然你选择了这条路，跪着也要走完。所以选择就好啦。既然选择了，那么就做好呗。</p><p>对于博士，我对自己的期许是：去假存真。再也不是流于表面，学过，会用，了解，一知半解。而现在要做到理解，从原理到公式推导都要明白。现在跟导师交流时，老会夸下海口，学过，会的。其实心里直发虚。希望未来，我能够坦诚的跟导师说，我会或者我不会。做事，做人，期望都要做到去假存真。做人，真心的帮助别人，如果没空，甚至不想帮助的时候，请直接回绝。不要怕如果伤害了别人，你的印象在别人眼中大跌。</p><p>上次偶尔看到，我这样性格的人对自己是蛮苛刻的。如果做不到自己想要的样子，会对自己很失望，进而会失去自身的控制能力。也许这是一个机会，是重新活出你想要的样子的机会。但我希望也不要苛责自己，即使没有活出自己最想要的样子，也请接受自己，那就是我，一个平凡人，与生活中挣扎。</p><p>我会把这次读博士，当作工作来做。但这工作更自由，也多了些趣味。要多接触接触大神，跟同行交流才能获取更多的力量。蒙头搞科研，知识是永远学不完的。</p><p>对自己，要更加自律。很多时候，凭借一时热情和压力在做事情。这种状态深知是不长久的。自律不会剥夺我什么的，想要自由，没有自律的自由是一盘散沙，毫无意义，自律下的自由才是维持内心秩序平和的关键。</p><p>着眼与未来，落实与当下。没有过多的期许，just to be a better man.</p><h3 id="12-7-2018"><a href="#12-7-2018" class="headerlink" title="12/7/2018"></a>12/7/2018</h3><p>从群里面看见有人发一个教授自杀了的文章，起先没注意。接着看了他的阴谋论的论调，让我点开了文章。一下看到了张首晟！很震惊，不会吧，会不会是谣言，上百度搜了下，还真是。一开始不太理解这么伟大的一位科学家为什么会自杀，当看到抑郁症的时候，释然了。内心当时不知道是什么心情，有点难过。可能是我从小喜欢科学的原因，看到这么优秀的科学家去世，太多的遗憾可惜了。但面对抑郁症，你是神也没用。抑郁是病，有时候靠个人的意志去挡，没有外界介入的话，很容易走向终点。教授，走好！你留给人类的遗产是这么丰富，此生你值了！</p><h3 id="12-8-2018"><a href="#12-8-2018" class="headerlink" title="12/8/2018"></a>12/8/2018</h3><p>这期奇葩说辩题：知道了别人的死亡时间，要不要告诉他？</p><p>这是一个探讨我们如何面度死亡的辩题。正方说，他们觉得如果告诉了别人，可以让他们更好的知道接下来的路该怎么走。反方说，如果告诉了他们并不一定会帮助他们，反而引发不必要的麻烦；而且每一个人都对自己的生命负责，想过的好为什么不现在就开始。</p><p>虫仔邱晨讲了她自己的抗癌过程，以及面对癌症的态度。当面对癌症，除了对死亡的恐惧，其实也没那么恐惧，她说，剩下的该怎样还是怎样。<br>康永哥说，一个成熟的人即使知道自己没几天就死了，但他依然会把他认为该做的事情做完，即使这件事是每天都会做的平常事。<br>我们人是一个伟大的物种，能够赋予做某件事的意义，这才是支撑我们花时间做的动力，以及因此也就赋予了生存的意义。<br>所以如果说还有3天可活，那么问问什么是你生存的意义，去做吧。</p><p>写到这，让我想起了爷爷去世前的几天。回光返照，我还以为他稍微好转了一些。但他当时应该很清楚，大限将至。他见了我们每一个人，儿子，女儿，孙儿。一个个交代，说我走了后，葬礼该怎么弄，在哪弄，还有他存的钱该怎么分，甚至他还会叮嘱谁谁有矛盾，需要和解理解。当奶奶相叫他休息时，他甚至会吼叫不去休息，让他把话讲完。或许有条不紊的处理完这些事情，他才会没有遗憾的离去吧。那个时候，死亡并不可怕，可怕的是他惦念的事情没有做完。</p><p>我生存的意义是什么？我不敢问自己，因为真的不知道。<br>如果还有3天可活的话，我肯定会回到父母身边，他们是我最爱的人，也是对我最无私的人。<br>然后我会跟我的朋友们和爱过的人告别吧，因为你们，我的生活变得精彩好多，我希望当面跟你们告别，但时间不允许的话，我会跟你视频。<br>还有我其他的亲人，你们也是我生活必不可少的一部分，我希望跟你们好好的交代几句。但请不要过分的担忧，也不希望你们过多的占据我仅有的时间。<br>接下来，我会列出来对待人和事情，还有哪些遗憾，能当面跟人说的说，不能的写下来。<br>还有，我希望能写一下自己的诰文，策划下自己的葬礼。我很不喜欢传统的葬礼仪式，希望越简单越好，大家坐在一个礼堂或者我家，聊一聊，祭奠下我就行，不要花圈（千万）。我希望朋友能在葬礼上说一些关于我好玩的事情，夸我也行，但不希望是网上抄来的千篇一律（考验你们功底的时候到了）。我希望那个时候，也能留下些什么东西，在葬礼上展示，比如说这篇文章，一些照片。<br>最后，我不会静静的等待死亡来临，特别讨厌等待的感觉，让人无所适从。我希望那个时候，我在跟朋友或许父母聊天，或者在写文章，或者在筹划葬礼，突然就来了那种最好。</p><p>如果还有30天呢？我接着问，<br>如果还有1年呢？<br>如果还有5年呢？<br>如果还有10年呢？<br>如果还有30年？<br>如果还有50年？<br>…</p><p>最后献上邱晨的一句话，</p><blockquote><p>当你真的尝试过竭尽全力之后，你会尝不到一点点遗憾，你会没有一丝的悔恨，你会感受的到的是无限的平静和喜悦” —— 邱晨</p></blockquote><h3 id="12-9-2018"><a href="#12-9-2018" class="headerlink" title="12/9/2018"></a>12/9/2018</h3><p>今天朋友组局去她家吃火锅。感觉好久没出来社交了，出门前还在想怎么跟陌生人去交流，晕~</p><p>蛮佩服这个认识不久的朋友。喜欢到处旅游，到处游山玩水。点开她的朋友圈，都是去哪玩的照片。听她说，她的 schedule 都排的很满，每到周末就忙着去哪里hiking, 爬山，社交等等。</p><p>但她自己本职工作，从来没有因此放松过。这不，说对什么 life insurance 感兴趣，报了一个这个班正在学习。</p><p>就是感觉她过的很自由。高中随家里移民来到美国，大学毕业后试了几份工作，最终确定了现在的方向。在辞职期间，去过很多国家，想回国定居，回来住了很长时间发现不适合自己，又回美国了，最终定居在了波士顿。</p><p>她身边总是有超多的朋友，特别善于组局，闲不下来吧，哈哈… 她总是想到的就去做，没有什么瞻前顾后，但说话也特别会考虑别人感受，所以这就是她组的局，大家都愿意去的原因吧。</p><h3 id="12-10-2018"><a href="#12-10-2018" class="headerlink" title="12/10/2018"></a>12/10/2018</h3><p>奇葩说结束了。老奇葩颜如晶再一次错失了冠军宝座，陈铭也真的是实至名归。</p><p>最后一期的辩题是：我不合群，我要不要改。<br>这是一个对于自己身份认同的题。每个人从小打大，或多或少都会出现身份认同的危机吧。马东说，这是想给正在挣扎的成长的孩子们的一些建议。所以当大多数人都站在不要改的立场时，他站了出来。是的，辩论不是为了一方观点压制另一方，至少在奇葩说，它想传递的是不同的思考，不同的价值观，没有非黑即白，没有高级低级，有的只是不同的适用环境。</p><p>我是一个从小到大，看起来合群却不合群的人。还能这么说吗？从小到大，我在经历中各种各种的群，看似其中一员，但总是融不进去，内心深处特别希望得到别人的认可。听完奇葩说反方辩论，反思自己，其实那些群是不适合自己的吧，都是自己去硬凑的。这就是改的立场吧，但真的成功融入了吗？</p><p>我们的内心深处，都是需要得到认可的，可以是父母的，可以是朋友的，可以是群的，甚至可是某个兴趣的，更有的可以是自己的认可。所以我不合群，需不需要改。看你自己喽。你的自我认可来自哪儿。内心强大的人，都说自己是不合群的。其实合群也蛮好的啊，过的开心，也甚至可以发掘出自己的潜力，跟一帮朋友互相进步。</p><h3 id="12-13-2018"><a href="#12-13-2018" class="headerlink" title="12/13/2018"></a>12/13/2018</h3><p>看了篇报道，现在一些小学生，都会自己购买得到app的课程，自我学习进化了。特别羡慕现在成长起来的孩子。刚出生就接触到我们20多岁才能接触到的事物。他们的成就不可限量，未来什么样子充满期待。但同时感到一阵前浪拍死在沙滩上的感觉。世界是我们的，但终归是你们的。</p><h3 id="12-14-2018"><a href="#12-14-2018" class="headerlink" title="12/14/2018"></a>12/14/2018</h3><p>在我的组织下，我们组第一次正式的聚餐选在了学校不远的中餐厅。导师也终于硬气了一回，”it’s my treat”。记得上学期送访学的学生时，大家在一个不知道哪国的餐厅聚餐，每人点一样，然后各付个的钱。对于初来咋到的我，一个中国人，完全不能理解老外的这种行为啊，好尬。记得国内，聚会不都是导师老板付钱的嘛。</p><p>那次的聚餐氛围也差劲，每人端着个盘子，还有一个其他组教授，远远的坐着，根本聊不到一块去。这次氛围就很好了，不仅是因为我组织的，更多的是中餐的就餐文化。大家都是分享食物，这就在无形之中，拉近了大家的距离（所以说中国人是最会组吃局的呢）。<br>反观老外，每个人端着个盘子。<br>问你吃啥。<br>‘beans’，<br>‘啥beans’，<br>‘green beans’,<br>‘哦’ ,<br>‘it’s very delicious’。<br>…（无语）…<br>查，绿豆有啥好吃的，有啥好讲的，还delicious 。作为吃文化最厉害的中国人，内心不知道鄙视了他们多少次。</p><p>不过他们对于中餐，还是非常推崇的。让我惊讶的是，都会用筷子，甚至巴西哥们用的比我还标准。之前带印度朋友吃饭（印度人非常爱交朋友），基本不会用，最后甚至动上了手。我在想，是不是西方的传统美食有，但不适用每天做，每天吃（看电视上，欧洲古代，吃就是面包就汤，有钱的就加上肉）。而亚洲的传统美食，从上到下都能美美的吃上一顿有味的饭。印度这个神奇的国度，吃的文化可能也比较厚重，所以中餐的渗透还是需要些时间吧。</p><p>嗯，不用怀疑，中餐会统一世界。</p><h3 id="12-15-2018"><a href="#12-15-2018" class="headerlink" title="12/15/2018"></a>12/15/2018</h3><p>今天一大早就醒了，睡了5个多小时，睡不着了。不知道是因为要搬家的事情，还是因为昨天感到碌碌无为而上心。每年总有那么几次，突然的觉得不爽，不爽自己的一事无成，不爽自己虚度光阴。但这种不爽来的快，去的也快。很快就淹没在茫茫的生活（存）中。</p><p>到底为什么而活？问这种终极问题的人不是有点抑郁倾向，就是在其他人看来无病呻吟。有抑郁倾向的人，甚至怀疑自己的整个人生，活和不活是个问题。无病呻吟的那种，在别人看来，没啥本事，不干活，劲瞎想。有本事，先活得精彩一些先看看啊。</p><p>但即使是很多所谓的成功人士，也会在某个时间点，陷入到这种虚无主义中。只是大家都不说了。生活还得继续，能给自己多找点意义，就多找点。<br>嗯？你说找不到。那也继续吧，一阵的瞎想很快就淹没在生活（存）中了。因为毕竟生存是动物的本能啊。</p><h3 id="12-17-2018"><a href="#12-17-2018" class="headerlink" title="12/17/2018"></a>12/17/2018</h3><p>终于补完了科幻美剧《Origin》。讲的是一伙身份各异的人移民外星球，在途中遭遇外星寄生生物而发生的故事。</p><p>很喜欢科幻。最近些年看的科幻剧中，最喜欢《Expanse》。场景宏大，剧情也非常不错。而Origin的场景就显得很小，基本就是在宇宙飞船里，甚至是几个固定仓里，遭遇外星生物，然后人们互相猜忌而发生的一系列故事。</p><p>外星生物可以寄生人脑，从而控制人。但由于仪器遭到破坏无法检测，所以大家只能互相猜忌求证是否有人被寄生。这是典型的美剧套路，与其说是科幻剧，不如说人性剧。</p><p>剧中，当面对危机情况时，每个人的本性就会暴露出来。以前有信任危机的，遇到情况，最先考虑自己，不相信他人；以前是黑帮的，遇到情况，处理果断，但过于冷血；还有很多以前做错事，无法面对的那帮人，依然在遇到相似抉择时，悔恨不已；但也有一些人，看起来坏坏的，其实内心却很柔软…</p><p>为什么起名字叫《Origin起源》。它彷佛在告诉我们：你想告别过去，重新开始，但实际上，你的过去控制着你，是什么样的人还是什么样的人，改变不了什么。</p><p>但真的不会改变吗？一帮有着各种过去的人，面对突如其来的灾难，紧紧抱在一块，又互相猜忌。文明的本性在黑暗中时不时的亮起。</p><p>这里面有个有意思的设定，被控制的人会渐渐遗忘记忆。而飞船数据库里存储着大家的过去，也就是记忆。那么大家可以通过这些数据来判断谁被寄生了。但貌似所有人都不愿意自己的过去被人知晓，甚至其中一个人不惜毁掉了这些数据（而她并没有被寄生）。</p><p>过去真的这么重要吗？重要。但同时毁掉过去数据，这个设定，或许是想告诉我们，只要内心向往美好，过去也许不是阻碍。</p><h3 id="12-20-2018"><a href="#12-20-2018" class="headerlink" title="12/20/2018"></a>12/20/2018</h3><p>最近打美服王者荣耀，总结了一些经验。队伍里经常互喷的，大概率赢不了；喷的不多的，赢面一半；开始说话积极的，死的特别惨，坑的一笔。还有一些队伍，说话不多的，但有一些提醒没有谩骂的，赢面非常大。但也有啥都不沟通的队伍，一般靠运气。</p><p>非常讨论游戏里面，特别逼逼的那种。尤其是那种还没怎么打，就开始逼逼骂人的，直接带坏了团队的氛围，导致团队配合不畅GG。</p><p>有很多次，遇到无脑骂人的，我都先不说话，用事实来说话。但有的时候真心忍不住，跟他怼了几句。发现，真是越怼越生气，甚至有一次，气的手都哆嗦了。放下手机，一想这不行啊，凭什么让这帮人控制了情绪？不过是个游戏。</p><p>怎么抽离出来？ “不过是个游戏”这样来宽慰自己？我先试着看最高境界的佛陀，他会怎么想。首先他是想着度化众生的心愿，当有人打他，他不还手，骂他不还嘴。但是他沾染了这段游戏中的因果啊，他会怎么做？道歉，该自己承担的责任承担，不该自己的责任，以一个相对客观的方式呈现？…我发现编不下了，佛陀不会打游戏的。他们抽离出来的方式，是从一开始就不跟你产生这样的因果。</p><p>普通人的我，发现这或许是不可避免的，不仅仅是游戏，做很多事情，都会遇到这些事情。试着用礼貌或者理性的方式去回应这些无端的指责谩骂，不过多沉浸在别人中，回归自己，你自己是不是问心无愧。还有非常重要的，你身边的朋友是不是也能够给予你支持，相信你。</p><p>ps:还想说一点，游戏对我们大脑的影响。有时候打游戏中很长时间后，大脑会自动的适应游戏中的环境。比如看见操作很很傻X的人，会很随意的骂一句煞笔。即使放下游戏，还是沉浸在厮杀谩骂的情境中。遇到一点小事，容忍度也降低很多。<br>发现还挺可怕的，尤其是那些暴力血腥的游戏，长期沉浸其中，势必对大脑造成一定的影响。尤其是发育中的孩子们。所以游戏还真不应该多玩，或者玩一些比较好的，甚至帮助孩子培养性格的游戏。</p><h3 id="12-21-2018"><a href="#12-21-2018" class="headerlink" title="12/21/2018"></a>12/21/2018</h3><p>人们往往对自己不了解的事情，充满着偏见。也正是这些偏见，才是构成认知的主要成分。</p><h3 id="12-22-2018"><a href="#12-22-2018" class="headerlink" title="12/22/2018"></a>12/22/2018</h3><p>缘分这个东西，怎么老耍人呢！</p><h3 id="12-24-2018"><a href="#12-24-2018" class="headerlink" title="12/24/2018"></a>12/24/2018</h3><p>Merry Christmas!</p><p>感觉国内的圣诞节气氛都要比我们这高啊。室友说主要是在Malden，都是移民，而且没啥钱，就不怎么装饰，波士顿市区那还是很不错的。原来如此吗。国外的人都是一家子在家聚餐，老外的餐馆基本都关了，不过中餐馆骄还是正常营业的，爆满。</p><h3 id="12-29-2018"><a href="#12-29-2018" class="headerlink" title="12/29/2018"></a>12/29/2018</h3><p>搬家搬到了一个新的地方。</p><h3 id="12-31-2018"><a href="#12-31-2018" class="headerlink" title="12/31/2018"></a>12/31/2018</h3><p>新的一年里，要说的是事情真的很多。有好玩的，也有郁闷的。无聊的可以点开这无聊的<a href="http://jononearth.com/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8/2019%E6%96%B0%E5%B9%B4flag/">2019新年flag</a>文</p><p>主要讲了这些乱七八糟的东西  </p><ul><li>学业</li><li>遇到的人，朋友，过客，缘分</li><li>生活</li><li>写作（博客）</li><li>娱乐</li></ul><p>最后了，附上新年拍的图片。很喜欢这个小松鼠和被扭曲的哥们哈:sweat_smile:。<br><img src="/images/squirrel.jpg" width = 40% height = 40% div align=left /><br><img src="/images/niuqu.png" width = 40% height = 40% div align=right /><br><img src="/images/coffeebar.jpg" width = 40% height = 40% div align=left /><br><img src="/images/isec.jpg" width = 40% height = 40% div align=right />    </p>]]></content>
      
      
      <categories>
          
          <category> 大话西游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活无趣 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019新年flag</title>
      <link href="/2019/01/01/2019%E6%96%B0%E5%B9%B4flag/"/>
      <url>/2019/01/01/2019%E6%96%B0%E5%B9%B4flag/</url>
      
        <content type="html"><![CDATA[<p>凯哥给我的新年祝福短信是：新年里，祝鹏博士找到属于自己的女超人。<br>凯哥是我两年来美国，遇到的最重要的贵人了。虽然一年前他就离开了，但我们一直保持联系。他是北理的教授，来MIT访学，非常幸运的跟他做了室友。一个个性十足，超级努力的人。很奇怪，我就简单地被他这种奇异的特质给吸引了。尽管有时候我会受不了他的跳脱和自负而“嫌弃”他。</p><p>当身边的人是这么个优秀的人的时候，你就会想努力的去接近。这或许就是我想要读博的原因之一吧。<br>同时他有时又像我的家长似的，很是关心我的终身大事。“这么帅的小伙，要不我帮你找个富婆嫁了得了”，我“呵呵，好啊，找啊”。其实呢，他是不会瞎找的。反而，自居很了解我，说“得找这样那样的才适合你。很难找，但别着急，会有女超人出现的”。这不，还送了我个女超人。</p><p>话说两边，新年了，不应该是对过去的总结吗？为什么写above。因为言简意赅的总结了过去的2年。</p><hr><h2 id="2018总结Y"><a href="#2018总结Y" class="headerlink" title="2018总结Y"></a>2018总结Y</h2><p>那么来看看冗长无聊的版本吧。</p><p>美国2年了。来这是因为在国内的时候，过的不是很如意。不喜欢按部就班的上班，而且从底层干起。你说我浮躁也罢，好高骛远也罢，我是不后悔出来的这2年。在此，再次感谢父母这么放任这么任性的我，提供财力上的支持。</p><p>那么2年里，你收获了什么？</p><h3 id="学业-当头喝棒，为时不晚"><a href="#学业-当头喝棒，为时不晚" class="headerlink" title="学业: 当头喝棒，为时不晚"></a>学业: 当头喝棒，为时不晚</h3><p>来美国主要是学习嘛。可刚来那会我不这么想，一心想创业，操着一口不流利的Chinglish就闯进社团，现在想来都佩服当时的勇气。然后想着去认识各种各样牛逼的人，拉他们创业。结果搞了一个学期，发现是这样的：牛逼的人不理你，大部分人只想进大公司找到工作；美国创业社群也很难融进去，沟通很成问题；然后，妈的，学期末差点挂科。</p><p>这一下算打醒了我。牛逼人不鸟你，首先是自己能力不够，那么自己可不可以去做个这个专业的牛逼人呢；而且初来咋到，凭着一腔热血，到处瞎转，根本没有涉及到一些创业的圈子，啥人脉都没有。接下来，我算开始融入到正常的留学圈，不再到处折腾。开始认真学习。慢慢的，绩点提高了上来，而且也知道了将来要在什么专业上有所建树，不再做个无头苍蝇了。</p><p>正因为在专业知识的自我学习下，我才会有了一个机会，接触到了我的博导，并且有机会被他看中做了博士。</p><p>从我这边同学的角度来看，有点不能理解，刚开始来的时候跟他大谈特谈理想的呢？但似乎又能理解，来美2年了吗，总归是认清了现实。</p><p>是啊，可能是认清现实了吧。读博是近些年来做的最重大的决定，但其实并没费什么力气做这个决定，一切都是顺其自然。</p><h3 id="社交：真诚待人，且行且珍惜"><a href="#社交：真诚待人，且行且珍惜" class="headerlink" title="社交：真诚待人，且行且珍惜"></a>社交：真诚待人，且行且珍惜</h3><p>2年，遇到了很多人。除了凯哥是我遇到的一个最重要的人外，还有一些人对我的帮助或影响也很大。如Mingmin同学，也是室友，对我的生活影响蛮大的，带着我从生活的小事到去美国各地游玩。有了他，刚来的我生活变的easy很多，精彩了很多。   </p><p>还有我的房东，一个老移民，室友对他的贬褒不一。我觉得他人很好，他不喜麻烦，我也是。于是我们两相处的应该很愉快，有时他会带着我们买买菜，钓钓鱼啥的。更重要的是，对于孑然一身来到美国的我，他提供了一个类似家的地方，还有个人可以依靠商量下（主要是心里上的）。</p><p>还有一些可爱的同学们，大多数比我小好几岁。跟他们相处，很容易看出来，欠缺历练。很难处到一个聊的很来的朋友。不过幸好，有这么几个。18年8月初去黄石的2个同学就是，一个比我大，一个比我小。大的工作了很多年，在约束性的韩企里工作，向往自由，于是辞职留学了。跟他相处，也收获了不少，比较会做人做事，很会照顾人，会生活，跟他去黄石，他笑着说2个少爷带着一个仆人。但跟他熟了之后，他强硬的一面就露了出来，不过好在，他在得到我“不露声色”的暗示后，能及时收敛住。<br>小的，用大的话说，眼睛很透明，写满了未经世事，天真无邪。他不善于社交，我们在课堂上偶然认识之后，竟然一直保持联系。熟悉了之后，就更依赖我们了。不过有了他，我们去哪都很方便了，因为他有车一族啊。而且我读博士，要不是他拉着我找的现在的老师，估计就不会有接下的事情了。<br>现在发现，我们3个人，还真不能缺少他，2人都没法成局。这不自从他回国之后，我和大的M同学竟很少碰面了。有时我也会烦他，但现在他走了，还真有点想他了。</p><p>还有一些其他的同学，如第一学期就认识的，现在竟然都跟同一个老师读博了，一个美国人，一个中国人。接下来的几年里，相信我们会处的很愉快。</p><p>第一个学期认识一个印度的朋友，一直到现在，都保持联系，有时小聚一下。印度人很喜欢交朋友，很能说会说，这是不是也从一面述说了为什么印度人能称霸硅谷高管阶层。</p><p>还有一些校外认识的朋友，参加了一个华人创投组织，里面能人不少。期待下一年跟多跟他们学习学习。</p><p>还有很多的朋友，我发现写都写不过来了。有鼓励我一起读博的大龄同学；有被我忽悠一起打壁球的小伙；有特别爱组局，能玩会玩的阳光小姐姐；有目的性很强的但很nice的学长；有睡一间房（她睡地上）但啥事都没发生的小胖美女室友；有生活习惯超级马虎，一不留神，就操英语的室友；有强势个性的北京糙汉子室友；有佛系，但熟了之后才看清也有不能侵犯的南方室友；有相处特别好，但没机会相处的同学，有时候觉得很遗憾；有一起聊创业理想的小姐姐；有一起看音乐会，但之后没有联系的一起入学的女同学；有考试，大家聚在一起互抱大腿的同学；有陪我练口语的华裔小姐姐，是个gay，活的很认真，很美国；有帮助我们处理各种学业上问题的老外辅导员，回复超快，想给他送锦旗…</p><p>遇到的大多数，基本都比较nice。这边的环境比较简单，大家基本没有什么利益上纠葛。互相尊重，也不需要溜须拍马，搞阶级斗争啥的，工作还是要靠实力来说话。</p><h3 id="生活：是啥？暂时没有"><a href="#生活：是啥？暂时没有" class="headerlink" title="生活：是啥？暂时没有"></a>生活：是啥？暂时没有</h3><p>没有女朋友，所以很宅。不去学校的时候，就自己刷剧。不过，幸好，我有一颗好奇探索的心。跟着朋友们去过不少地方。Boston周边的大小景点，海边沙滩，海岛，去了不少。纽约华盛顿也去过一次。黄石也自驾去了一次，详看《<a href="http://jononearth.com/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/">盐湖到黄石6天不完全攻略</a>》。不过跟朋友圈里的朋友一比，就相差太多了。贫穷限制了我的行动力，还有一堆due.</p><h3 id="其他：没有八卦"><a href="#其他：没有八卦" class="headerlink" title="其他：没有八卦"></a>其他：没有八卦</h3><p>2018年做的最好的事情，应该说是开了个博客。这个博客主要是写给自己的。对于我的起了个很大的作用，因为把很多的东西都组织整理了起来，知识结构通过整理更加清晰。就像是我大脑的衍生，随调随用。</p><p>找到女朋友的目标没有兑现。家里介绍了一个，聊的不错，人也不错。可惜天不时，尤其是地不利，人和也枉然。对于我们两人来说，终究是一个该错过的缘分。愿各自安好，我都说没有八卦。</p><p>认知这个好难说。反正现在不再浮躁了，随着年龄的增长，见识的人和事多了，认知自然而然都会增长。</p><img src="/images/niuqu.jpg" width = 40% height = 40% div align=center /><hr><h2 id="2019展望A"><a href="#2019展望A" class="headerlink" title="2019展望A"></a>2019展望A</h2><h3 id="学业：这是工作"><a href="#学业：这是工作" class="headerlink" title="学业：这是工作"></a>学业：这是工作</h3><p>应该以一个什么样的身份去读博？学生吗？谢了，这次我想换个身份，以工作的态度去读这个书。工作的态度意味着离开学校，老师的庇护，能够学会独当一面；意味着遇到事情，不再以一个学生业余的方式对待，而是更加专业professional；意味着加倍努力的工作，完成老师的指标，完成工作上的超越。</p><p>狭隘目标，论文一年至少发2篇；数学基础越来越扎实；专业机器学习变得professional。</p><h3 id="社交：牛逼吸引牛逼"><a href="#社交：牛逼吸引牛逼" class="headerlink" title="社交：牛逼吸引牛逼"></a>社交：牛逼吸引牛逼</h3><p>吸引更多牛逼的人，不过首先自己要变得越来越好。<br>跟更多的优秀的博士交流，即使不能够获取啥有用信息，也能近朱者赤。<br>与更多工作的优秀朋友交流，获取不同视角的专业了解。<br>与更多积极进取的小伙伴交流，听听你们的故事。</p><h3 id="生活：三点一线牵"><a href="#生活：三点一线牵" class="headerlink" title="生活：三点一线牵"></a>生活：三点一线牵</h3><p>新年换了个新家，新的开始，离学校更近了。学校，健身房，家。大概大部分的时间都会在此度过了。工作既生活，生活既工作。</p><img src="/images/isec.jpg" width = 40% height = 40% div align=center />    <p>个人生活，家里面的压力已经让我到了不得不重视的程度。期望下学期遇到那个对的她，身边不再有其他人的她。</p><p>有机会，还是要去旅个游啥的。跟朋友去个小岛，小镇，爬个山。也能组织起我们组，出去游玩一次。但去更远的地方，就随缘吧。</p><h3 id="其他：练就易经第一重"><a href="#其他：练就易经第一重" class="headerlink" title="其他：练就易经第一重"></a>其他：练就易经第一重</h3><p>易经第一重：习惯培养，有规律的生活。<br>易经第二重：其他能力的培养。第二重第一章演讲能力….<br>易经第三重：待开发<br>…<br>易经第N重：从心所欲而不逾矩  </p><p>多读书，一定要多读书。接触多元的知识，发现自我的偏见尤为关键。</p><p>博客内容要持续更新，一个月至少更新一篇。随笔的话，1~3天要写一段。对，写作能力也要培养下，不能写完自己都不想回看。</p><p>以罗胖罗振宇的一句话结尾：</p><blockquote><p>做事情最好的时间是10年前，次好的时间就是现在。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习知识点</title>
      <link href="/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>学习中遇到的一些知识点，简单整理一些。持续更新中。。。。</p><h3 id="生成模型（generative-model）-VS-判别模型（discriminative-model）"><a href="#生成模型（generative-model）-VS-判别模型（discriminative-model）" class="headerlink" title="生成模型（generative model） VS 判别模型（discriminative model）"></a>生成模型（generative model） VS 判别模型（discriminative model）</h3><ul><li><p>判别模型是直接学习$P(y/x)$,或者直接从特征空间学习类别标签</p></li><li><p>生成模型事假定数据满足一定的分布特征，需要学习$P(x/y)$  </p></li><li><p>优缺点：1，生成式模型都会对数据的分布做一定的假设, 比如朴素贝叶斯会假设在给定y的情况下各个特征之间是条件独立的，当数据满足这些假设时, 生成式模型通常需要较少的数据就能取得不错的效果, 但是当这些假设不成立时, 判别式模型会得到更好的效果。<br>2，生成式模型最终得到的错误率会比判别式模型高, 但是其需要更少的训练样本就可以使错误率收敛；3，生成式模型更容易拟合, 比如在朴素贝叶斯中只需要计下数就可以, 而判别式模型通常都需要解决凸优化问题；4， 生成式模型可以更好地利用无标签数据(比如DBN), 而判别式模型不可以；5，判别式模型可以对输入数据x进行预处理, 使用ϕ(x)来代替x, 如下图所示, 而生成式模型不是很方便进行替换（？）.<br><a href="https://blog.csdn.net/Fishmemory/article/details/51711114" target="_blank" rel="noopener">https://blog.csdn.net/Fishmemory/article/details/51711114</a><br><a href="http://www.cnblogs.com/kemaswill/p/3427422.html" target="_blank" rel="noopener">http://www.cnblogs.com/kemaswill/p/3427422.html</a></p></li></ul><h3 id="隐马尔可夫模型-Hidden-Markov-Model"><a href="#隐马尔可夫模型-Hidden-Markov-Model" class="headerlink" title="隐马尔可夫模型(Hidden Markov Model)"></a>隐马尔可夫模型(Hidden Markov Model)</h3><h3 id="分类器评价指标（metric）"><a href="#分类器评价指标（metric）" class="headerlink" title="分类器评价指标（metric）"></a>分类器评价指标（metric）</h3><p>对于分类器，主要的评价指标有precision，recall，F-score，Accuracy以及ROC曲线,,AUC</p><p>|   |         |预测的类|||<br>|:—–|:—-|:—-|:—|<br>|         |  |Yes|No|合计|<br>|实际的类|Yes|TP|FN|P|<br>|        |No|FP|TN|N|<br>|||P’|N’|||</p><p>首先来解释一下表格中的术语：</p><ol><li>真正例/真阳性(True Positive, TP)：指被分类器正确分类的正元组。令TP为真正例的个数。</li><li>真负例/真阴性(True Negative, TN)：指被分类器正确分类的负元组。令TN为真负例的个数。</li><li>假正例/假阳性(False Positive, FP)：指被分类器错误标记为正元组的负元组。令FP为假正例的个数。</li><li>假负例/假阴性(False Negative, FN)：指被分类器错误标记为负元组的正元组。令FN为假负例的个数。</li><li>正元组数(Positive, P)：样本中实际的正元组数。</li><li>负元组数(Negative, N)：样本中实际的负元组数。</li><li>P’：被分类器分为正元组的样本数。</li><li>N’：被分类器分为负元组的样本数。</li><li>真正率(True Positive Rate , TPR)【灵敏度(sensitivity)】：$TPR = TP /(TP + FN)$ ，即正样本预测结果数/ 正样本实际数</li><li>假负率(False Negative Rate , FNR) ：$FNR = FN /(TP + FN)$ ，即被预测为负的正样本结果数/正样本实际数</li><li>假正率(False Positive Rate , FPR) ：$FPR = FP /(FP + TN)$ ，即被预测为正的负样本结果数 /负样本实际数</li><li>真负率(True Negative Rate , TNR)【特指度(specificity)】：$TNR = TN /(TN + FP)$ ，即负样本预测结果数 / 负样本实际数</li><li>精确度(Precision): $P = TP/(TP+FP)$ . 精度就是我选择的这一类就是正确的概率是多少。力求 识别出来的的视频中，绝大部分都不含色情视频，“宁放过大部分视频，不错一个视频含sex”</li><li>召回率(Recall): R = TP/(TP+FN)，即真正率。召回率是选择的这一类是正确的，占实际总体的概率是多少。力求 在所有的病人中，都能识别出这些病人，“宁错诊有病，不放过一个真病”。</li><li>F-score：查准率和查全率的调和平均值, 更接近于P, R两个数较小的那个: $F=2* P* R/(P + R)$</li><li>准确率(Aaccuracy): 分类器对整个样本的判定能力,即将正的判定为正，负的判定为负: $A = (TP + TN)/(TP + FN + FP + TN)$</li><li>ROC(Receiver Operating Characteristic):ROC的主要分析工具是一个画在ROC空间的曲线——ROC curve，横坐标为false positive rate(FPR)，纵坐标为true positive rate(TPR)。”In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as <strong>its discrimination threshold</strong> is varied.””</li><li>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。<img src="/images/class_metric2.png" width = 100% height = 100% div align=center /></li><li>为什么使用ROC曲线？<br>既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候,”<a href="https://www.quora.com/Why-do-we-use-ROC-curves" target="_blank" rel="noopener">AUC</a>. It is already computed by varying class priors. Does not get affected much by size of data in a particular class”.。在实际的数据集中经常会出现类不平衡(class imbalance)现象，即负样本比正样本多很多(或者相反)，而且测试数据中的正负样本的分布也可能随着时间变化.  还有一个就是可以随着阈值的变化。</li></ol><img src="/images/class_metric.png" width = 100% height = 100% div align=center /><p>reference:<br><a href="https://www.cnblogs.com/gatherstars/p/6084696.html" target="_blank" rel="noopener">https://www.cnblogs.com/gatherstars/p/6084696.html</a>   <a href="https://blog.csdn.net/pipisorry/article/details/51788927#commentBox" target="_blank" rel="noopener">https://blog.csdn.net/pipisorry/article/details/51788927#commentBox</a><br><a href="http://blog.sina.com.cn/s/blog_629e606f0102v7a0.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_629e606f0102v7a0.html</a>  </p><h3 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h3><p>机器学习中发生过拟合的主要原因有：</p><ol><li>使用过于复杂的模型；</li><li>数据噪声较大；</li><li>训练数据少。</li></ol><p>由此对应的降低过拟合的方法有：</p><ol><li>简化模型假设，或者使用惩罚项限制模型复杂度；</li><li>进行数据清洗，减少噪声；</li><li>收集更多训练数据。</li></ol><h3 id="梯度下降了解多少"><a href="#梯度下降了解多少" class="headerlink" title="梯度下降了解多少"></a>梯度下降了解多少</h3><p>three variants of gradient descent, among which mini-batch gradient descent is the most popular：</p><ol><li>批量梯度下降（batch Grandient descent）. BGD 是梯度下降算法最原始的形式, 其特点是每次更新参数 ω 时, 都使用整个训练集的数据.</li><li>随机梯度下降（stochastic grandient descent）.SGD 每次以一个样本, 而不是整个数据集来计算梯度.</li><li>小批量梯度下降（mini-batch grandient descent）. MBGD 是为解决 BGD 与 SGD 各自缺点而发明的折中算法, 或者说它利用了 BGD 和 SGD 各自优点. 其基本思想是: 每次更新参数时, 使用 n 个样本, 既不是全部, 也不是 1. (SGD 可以看成是 n=1 的 MBGD 的一个特例)</li></ol><p>|梯度下降算法|    优点|    缺点|<br>|:—–|:—-|:—-|:—|<br>|BGD    |全局最优解    |计算量大, 迭代速度慢, 训练速度慢<br>|SGD    |1.训练速度快 2. 支持在线学习    |准确度下降, 有噪声, 非全局最优解|<br>|MBGD |1. 训练速度较快, 取决于小批量的数目  2. 支持在线学习    |准确度不如 BGD, 仍然有噪声, 非全局最优解</p><p><a href="http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants" target="_blank" rel="noopener">Algorithms</a> that are most commonly used for optimizing SGD:</p><ul><li>Momentum.<br>当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程</li><li>Nesterov accelerated gradient.<br>NAG is a way to give our <strong>momentum</strong> term this kind of prescience. 对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项</li><li>Adagrad.<br>It adapts the <strong>learning rate</strong> to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data.</li><li>Adadelta.<br>Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</li><li>RMSprop<br>RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad’s radically diminishing learning rates.主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行<a href="https://blog.csdn.net/u013709270/article/details/78667531" target="_blank" rel="noopener">衰减</a></li><li>Adam<br>Adaptive Moment Estimation (Adam) [15] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.<br>其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体.</li></ul><p>梯度下降需要关注的参数调参：</p><ul><li>学习率 learn rate $a$.</li><li>学习率衰减 decay</li><li>冲量 momentum</li><li>参数初始值（常常random）</li></ul><p>references:<br><a href="http://kissg.me/2017/07/23/gradient-descent/" target="_blank" rel="noopener">http://kissg.me/2017/07/23/gradient-descent/</a><br><a href="http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants</a><br><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/5970503.html</a><br><a href="https://blog.csdn.net/u013709270/article/details/78667531" target="_blank" rel="noopener">https://blog.csdn.net/u013709270/article/details/78667531</a><br><a href="http://scikit-learn.org/stable/modules/sgd.html" target="_blank" rel="noopener">SGD,Sklearn中应用</a></p><h3 id="Lasso-Ridge"><a href="#Lasso-Ridge" class="headerlink" title="Lasso, Ridge"></a>Lasso, Ridge</h3><blockquote><p>然后针对L2范数 $\Phi \left ( w \right ) = \sum_{j=1}^{n}w_j^2 $ ，同样对它求导，得到梯度变化为 $∂Φ(w)/∂wj=2wj$ (一般会用$λ2$来把这个系数2给消掉)。同样的更新之后使得$wj$的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。——<a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html#4.2" target="_blank" rel="noopener">逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降）</a></p></blockquote><blockquote><p>Lasso回归和岭回归最重要的区别是，岭回归中随着惩罚项增加时，所以项都会减小，但是仍然保持非0的状态，然而Lasso回归中，随着惩罚项的增加时，越来越多的参数会直接变为0，正是这个优势使得lasso回归容易用作特征的选择（对应参数非0项），因此lasso回归可以说能很好的保留那些具有重要意义的特征而去掉那些那些意义不大甚至毫无意义的特征（如果是超多维的稀疏矩阵，这难道不是在垃圾中寻找黄金的“掘金术”吗？），而岭回归永远不会认为一个特征是毫无意义的。 —— <a href="https://blog.csdn.net/qq_34531825/article/details/52689654" target="_blank" rel="noopener">Spark2.0机器学习系列之12： 线性回归及L1、L2正则化区别与稀疏解</a></p></blockquote><blockquote><p>正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及outliers的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项 对应后验估计中的 先验信息 ，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。——<a href="http://charleshm.github.io/2016/03/Regularized-Regression/" target="_blank" rel="noopener">Regularized Regression: A Bayesian point of view</a></p></blockquote><p>总结作用：</p><ul><li>解决共线性问题</li><li>解决变量多于数据数的情况</li><li>解决过拟合的问题，不至于使得参数取的过大，过小等问题</li><li>解决选取特征（lasso）</li></ul><p>L1_lasso 优缺点：</p><ul><li>简化模型复杂度。因为引入参数的先验分布，拉普拉斯分布</li><li>善于处理稀疏数据，选取有用特征</li><li>缺点：处理速度快，但可能牺牲了一些准确性</li></ul><p>L2_Ridge 优缺点：</p><ul><li>简化模型复杂度，因为引入参数的先验分布，高斯分布</li><li>解决logistic regression共线性问题。</li><li>解决样本点比较少，而特征比较多，特征个数多于样本个数的情况。</li></ul><p><a href="https://blog.csdn.net/qq_34531825/article/details/52689654" target="_blank" rel="noopener">Spark2.0机器学习系列之12： 线性回归及L1、L2正则化区别与稀疏解</a></p><p><a href="https://blog.csdn.net/kejiaming/article/details/64439664" target="_blank" rel="noopener">逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降）</a></p><p><a href="https://www.zhihu.com/question/23536142/answer/90135994" target="_blank" rel="noopener">贝叶斯角度Regularized Regression: A Bayesian point of view</a></p><p><a href="http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression</a></p><h3 id="什么是共线性问题"><a href="#什么是共线性问题" class="headerlink" title="什么是共线性问题"></a>什么是共线性问题</h3><p>共线性问题对线性回归模型有如下影响：</p><ul><li>参数的方差增大；</li><li>难以区分每个解释变量的单独影响；</li><li>变量的显著性检验失去意义；</li><li>回归模型缺乏稳定性。样本的微小扰动都可能带来参数很大的变化；</li><li>影响模型的泛化误差。</li></ul><p>共线性问题的解决方法:</p><ul><li>增加数据</li><li>对模型施加某些约束条件（L2,L1）</li><li>删除一个或几个共线变量</li><li>将模型适当变形</li><li>主成分回归，降维</li></ul><p><a href="https://www.jianshu.com/p/ef1b27b8aee0?from=timeline" target="_blank" rel="noopener">讲讲共线性问题</a></p><p><a href="https://blog.csdn.net/diyiziran/article/details/17025471" target="_blank" rel="noopener">多重共线性的产生原因、判别、检验、解决方法</a></p><h3 id="哪些算法可以online-learning"><a href="#哪些算法可以online-learning" class="headerlink" title="哪些算法可以online learning"></a>哪些算法可以online learning</h3><h3 id="Gradient-boosting"><a href="#Gradient-boosting" class="headerlink" title="Gradient boosting"></a>Gradient boosting</h3><p><a href="http://www.cnblogs.com/willnote/p/6801496.html" target="_blank" rel="noopener">Gradient boosting</a></p><h3 id="GBDT-vs-Random-forestes"><a href="#GBDT-vs-Random-forestes" class="headerlink" title="GBDT vs Random forestes"></a>GBDT vs Random forestes</h3><p>GBDT是计算每个模型训练后的残差<br><a href="https://blog.csdn.net/login_sonata/article/details/73929426" target="_blank" rel="noopener">随机森林和GBDT的区别</a></p><h3 id="GMM-Gaussian-mixture-model"><a href="#GMM-Gaussian-mixture-model" class="headerlink" title="GMM(Gaussian mixture model)"></a>GMM(Gaussian mixture model)</h3><p>假设模型服从几个高斯分布相叠加结果。计算这个点属于每个模型的概率，哪个最大就是属于哪一类的。</p><p>一个简单的解释：<br><a href="https://zhuanlan.zhihu.com/p/31103654" target="_blank" rel="noopener">一文详解高斯混合模型原理</a></p><p>公式：高斯模型叠加<br>$$p(x)=\sum_{k=1}^{K}p(k)p(x|k)=\sum_{k=1}^{K}\phi_{k}N(x|\mu_{k},\varepsilon_{k})$$</p><p>利用<a href="https://www.cnblogs.com/Gabby/p/5344658.html" target="_blank" rel="noopener">EM算法</a>来更新迭代数据，有三个数据需要更新，$\phi_{k}, \mu_{k}, \varepsilon_{k}$</p><img src="/images/GMM.png" width = 80% height = 80% div align=center /><ul><li>与Kmeans的<a href="https://blog.csdn.net/tingyue_/article/details/70739671" target="_blank" rel="noopener">关系</a>：<br>K-Means算法其实是GMM的EM解法在高斯分量协方差ϵI→0时的一个特例，GMM输出的是数据点属于每个每类的概率，我们用最大似然方法去确定分类。就严谨性来说，用概率进行描述数据点的分类，GMM显然要比K-mean好很多。<br>实际应用中，对于 K-means，我们通常是重复一定次数然后取最好的结果，但由于 GMM 每一次迭代的计算量比 K-means 要大许多，使用GMM时，一个更流行的做法是先用 K-means （已经重复并取最优值了）得到一个粗略的结果，然后将其作为初值（只要将 K-means 所得的 聚类中心传给 GMM即可），再用 GMM 进行细致迭代。</li></ul><p>Reference:<br><a href="https://blog.csdn.net/jinping_shi/article/details/59613054" target="_blank" rel="noopener">高斯混合模型（GMM）及其EM算法的理解</a><br><a href="https://blog.csdn.net/llp1992/article/details/47058109" target="_blank" rel="noopener">https://blog.csdn.net/llp1992/article/details/47058109</a><br><a href="https://blog.csdn.net/tingyue_/article/details/70739671" target="_blank" rel="noopener">https://blog.csdn.net/tingyue_/article/details/70739671</a></p><h3 id="EM-算法"><a href="#EM-算法" class="headerlink" title="EM 算法"></a>EM 算法</h3><p>数学原理：<br>求被选到的概率最大。</p><p><a href="http://blog.51cto.com/9269309/1892833" target="_blank" rel="noopener">http://blog.51cto.com/9269309/1892833</a><br><a href="https://blog.csdn.net/linyanqing21/article/details/50939009" target="_blank" rel="noopener">https://blog.csdn.net/linyanqing21/article/details/50939009</a></p><h3 id="SVM推导，对偶性的作用，核函数有哪些，有什么区别"><a href="#SVM推导，对偶性的作用，核函数有哪些，有什么区别" class="headerlink" title="SVM推导，对偶性的作用，核函数有哪些，有什么区别"></a>SVM推导，对偶性的作用，核函数有哪些，有什么区别</h3><p>确定下界 最大化</p><p><a href="https://www.cnblogs.com/xxrxxr/p/7536131.html" target="_blank" rel="noopener">关于SVM数学细节逻辑的个人理解（二）：从基本形式转化为对偶问题</a></p><p><a href="http://www.hanlongfei.com/convex/2015/11/05/duality/" target="_blank" rel="noopener">http://www.hanlongfei.com/convex/2015/11/05/duality/</a></p><p><a href="https://blog.csdn.net/Sunshine_in_Moon/article/details/51321461" target="_blank" rel="noopener">https://blog.csdn.net/Sunshine_in_Moon/article/details/51321461</a></p><p><a href="https://www.zhihu.com/question/58584814" target="_blank" rel="noopener">https://www.zhihu.com/question/58584814</a></p><h3 id="bp算法介绍，梯度弥散问题。"><a href="#bp算法介绍，梯度弥散问题。" class="headerlink" title="bp算法介绍，梯度弥散问题。"></a>bp算法介绍，梯度弥散问题。</h3><h3 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h3><p>以后如果有机会多学下NLP的话，就专门写一片自然语言教程。<br>先记下一些看过的非常好的教程。<br><a href="http://www.sohu.com/a/221418079_817016" target="_blank" rel="noopener">一文读懂自然语言处理（NLP）入门学习要点</a><br><a href="https://www.jiqizhixin.com/articles/081203?from=synced&keyword=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86" target="_blank" rel="noopener">自然语言处理是如何工作的？一步步教你构建 NLP 流水线</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data structure and algorithm(1)</title>
      <link href="/2018/08/16/Algorithm-1/"/>
      <url>/2018/08/16/Algorithm-1/</url>
      
        <content type="html"><![CDATA[<p>Notes from Udacity Data Science Interview Preparation lesson.</p><ul><li>big O Notation<br><a href="http://bigocheatsheet.com/" target="_blank" rel="noopener">http://bigocheatsheet.com/</a></li></ul><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><h3 id="The-difference-between-Array-and-list"><a href="#The-difference-between-Array-and-list" class="headerlink" title="The difference between Array and list"></a>The difference between Array and list</h3><ul><li>Array has index</li><li>List has order but no indices</li><li>Python list is built as an array</li></ul><h3 id="linked-lists"><a href="#linked-lists" class="headerlink" title="linked lists"></a>linked lists</h3><ul><li>will tell you where is the next list</li><li>the same as array is that store the value information</li><li>the difference with array is that they store the different information. array store the index inforation, and linked list store a reference to the next element in the list.<img src="/images/linked_list.png" width = 80% height = 80% div align=center /></li></ul><p>when coding：<br> it has two parts in one node. one of them is value of linked list(self.value), another is pointer(self.next) which point to next list.<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lass Node():</span><br><span class="line">   __slots__&#x3D;[&#39;_item&#39;,&#39;_next&#39;]    #限定Node实例的属性</span><br><span class="line">   def __init__(self,item):</span><br><span class="line">       self._item&#x3D;item</span><br><span class="line">       self._next&#x3D;None     #Node的指针部分默认指向None</span><br><span class="line">   def getItem(self):</span><br><span class="line">       return self._item</span><br><span class="line">   def getNext(self):</span><br><span class="line">       return self._next</span><br><span class="line">   def setItem(self,newitem):</span><br><span class="line">       self._item&#x3D;newitem</span><br><span class="line">   def setNext(self,newnext):</span><br><span class="line">       self._next&#x3D;newnext</span><br></pre></td></tr></table></figure><br>reference:<br> <a href="http://python.jobbole.com/83953/" target="_blank" rel="noopener">python数据结构——链表的实现</a> <a href="http://www.cnblogs.com/linxiyue/p/3551633.html" target="_blank" rel="noopener">http://www.cnblogs.com/linxiyue/p/3551633.html</a></p><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><ul><li>like stack pancake, list-based data structure. But different from linked_list and Array</li><li>LIFO: last in first out.</li><li>PUSH:insert； POP：remove</li></ul><h3 id="Queues"><a href="#Queues" class="headerlink" title="Queues"></a>Queues</h3><ul><li>FIFO: first in first out; list-based</li><li>dequeues or double-end queues can dequeue and enqueue in both head and tail<img src="/images/queues.png" width = 80% height = 80% div align=center /></li><li>priority queue. dequeue with highest priority(1). if have same priority, dequeue the oldest one.<img src="/images/priority_queue.png" width = 80% height = 80% div align=center /></li></ul><h3 id="sets"><a href="#sets" class="headerlink" title="sets"></a>sets</h3><ul><li>the difference from list is that it don’t have orders, but can’t have repeated elements</li></ul><h3 id="Maps-dictionary"><a href="#Maps-dictionary" class="headerlink" title="Maps(dictionary)"></a>Maps(dictionary)</h3><ul><li>like dictionary. Maps = &lt;key, value&gt;. a group of keys is a set.</li><li>Dictionaries are wonderfully flexible—you can store a wide variety of structures as values. You store another dictionary or a list:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Cities to add:</span><br><span class="line">Bangalore (India, Asia)</span><br><span class="line">Atlanta (USA, North America)</span><br><span class="line">Cairo (Egypt, Africa)</span><br><span class="line">Shanghai (China, Asia)&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">locations &#x3D; &#123;&#39;North America&#39;: &#123;&#39;USA&#39;: [&#39;Mountain View&#39;]&#125;&#125;</span><br><span class="line">locations[&#39;North America&#39;][&#39;USA&#39;].append(&#39;Atlanta&#39;)</span><br><span class="line">locations[&#39;Asia&#39;] &#x3D; &#123;&#39;Inida&#39;:[&#39;Bangalore&#39;]&#125;</span><br><span class="line">locations[&#39;Asia&#39;][&#39;China&#39;]&#x3D; [&#39;Shanghai&#39;]</span><br><span class="line">locations[&#39;Africa&#39;] &#x3D; &#123;&#39;Egypt&#39;: [&#39;Cairo&#39;]&#125;</span><br><span class="line"></span><br><span class="line">for city in sorted(locations[&#39;North America&#39;][&#39;USA&#39;]):</span><br><span class="line">    print city</span><br><span class="line"></span><br><span class="line">m &#x3D; []    </span><br><span class="line">for country, city in locations[&#39;Asia&#39;].iteritems():</span><br><span class="line">    m.append(city[0]+&#39;-&#39;+ country)</span><br><span class="line">for listm in sorted(m):</span><br><span class="line">   print listm</span><br></pre></td></tr></table></figure><h3 id="Hashing"><a href="#Hashing" class="headerlink" title="Hashing"></a>Hashing</h3><p>Using a data structure that employs a hash function allows you to look up in constant time.</p><ul><li>using the reminder as index, store this value at this place  <img src="/images/hashing1.png" width = 80% height = 80% div align=center /></li><li>if have collisions, two ways: 1, change hash function; 2, store all the value into one collection, the array will turn to ‘bucket’.  <img src="/images/hashing2.png" width = 60% height = 60% div align=center /></li><li>in bucket, still can apply hash function  <img src="/images/hashing3.png" width = 60% height = 60% div align=center /></li><li>For the load factor, you should divide the number of values by the number of buckets. $Load Factor = Number of Entries / Number of Buckets$<br><a href="https://classroom.udacity.com/courses/ud944/lessons/7118294395/concepts/79336691870923" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud944/lessons/7118294395/concepts/79336691870923</a><img src="/images/hashing4.png" width = 50% height = 50% div align=center /><img src="/images/hashing5.png" width = 50% height = 50% div align=center /></li></ul><h3 id="Hash-Maps"><a href="#Hash-Maps" class="headerlink" title="Hash Maps"></a>Hash Maps</h3><img src="/images/hash_maps.png" width = 80% height = 80% div align=center />- In interview, you will be asked about creating "Hash table" to test your understand hashing. also need to know the upside and downside of a hash function<h3 id="Hash-for-String-Keys"><a href="#Hash-for-String-Keys" class="headerlink" title="Hash for String Keys"></a>Hash for String Keys</h3><ul><li>string turn into ASCLL, A=65, B=66,C=67</li></ul><img src="/images/string_keys.png" width = 80% height = 80% div align=center />- why use 31, it's more likely convention.  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">example: Hash Value &#x3D; (ASCII Value of First Letter * 100) + ASCII Value of Second Letter</span><br><span class="line">class HashTable(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.table &#x3D; [None]*10000</span><br><span class="line"></span><br><span class="line">    def store(self, string):</span><br><span class="line">        hv &#x3D; self.calculate_hash_value(string)</span><br><span class="line">        if hv !&#x3D; -1:</span><br><span class="line">            if self.table[hv] !&#x3D; None:</span><br><span class="line">                self.table[hv].append(string)</span><br><span class="line">            else:</span><br><span class="line">                self.table[hv] &#x3D; [string]</span><br><span class="line"></span><br><span class="line">    def lookup(self, string):</span><br><span class="line">        hv &#x3D; self.calculate_hash_value(string)</span><br><span class="line">        if hv !&#x3D; -1:</span><br><span class="line">            if self.table[hv] !&#x3D; None:</span><br><span class="line">                if string in self.table[hv]:</span><br><span class="line">                    return hv</span><br><span class="line">        return -1</span><br><span class="line"></span><br><span class="line">    def calculate_hash_value(self, string):</span><br><span class="line">        value &#x3D; ord(string[0])*100 + ord(string[1])</span><br><span class="line">        return value</span><br></pre></td></tr></table></figure><h2 id="Trees-Data-Structure"><a href="#Trees-Data-Structure" class="headerlink" title="Trees_Data Structure"></a>Trees_Data Structure</h2><ul><li>tree is an extension of linked-list</li><li>the following figure, different color has its meaning.  <img src="/images/tree.png" width = 80% height = 80% div align=center /></li></ul><h3 id="Tree-Traversal"><a href="#Tree-Traversal" class="headerlink" title="Tree Traversal"></a>Tree Traversal</h3><ul><li><p>DFS(depth first search):</p><ul><li><p>pre-order  </p><img src="/images/DFS_preorder.png" width = 80% height = 80% div align=center /></li><li><p>in-order  </p><img src="/images/DFS_inorder.png" width = 80% height = 80% div align=center /></li><li><p>post-order  (right first)</p><img src="/images/DFS_postorder.png" width = 80% height = 80% div align=center /></li><li><p>Q&amp;A: D, F, E, B, C, A</p><img src="/images/tree2.png" width = 80% height = 80% div align=center /></li></ul></li><li><p>BFS(Breadth first search): check the same level node first</p></li><li><p>Search and Delete, insert. perfect tree, each node has two child  </p><img src="/images/tree3.png" width = 80% height = 80% div align=center /></li></ul><h3 id="Binary-search-tree-BST"><a href="#Binary-search-tree-BST" class="headerlink" title="Binary search tree(BST)"></a>Binary search tree(BST)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class BinaryTree(object):</span><br><span class="line">    def __init__(self, root):</span><br><span class="line">        self.root &#x3D; Node(root)</span><br><span class="line"></span><br><span class="line">    def search(self, find_val):</span><br><span class="line">        return self.preorder_search(tree.root, find_val)</span><br><span class="line"></span><br><span class="line">    def print_tree(self):</span><br><span class="line">        return self.preorder_print(tree.root, &quot;&quot;)[:-1]</span><br><span class="line"></span><br><span class="line">    def preorder_search(self, start, find_val):</span><br><span class="line">        if start:</span><br><span class="line">            if start.value &#x3D;&#x3D; find_val:</span><br><span class="line">                return True</span><br><span class="line">            else:</span><br><span class="line">                return self.preorder_search(start.left, find_val) or self.preorder_search(start.right, find_val)</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">    def preorder_print(self, start, traversal):</span><br><span class="line">        if start:</span><br><span class="line">            traversal +&#x3D; (str(start.value) + &quot;-&quot;)</span><br><span class="line">            traversal &#x3D; self.preorder_print(start.left, traversal)</span><br><span class="line">            traversal &#x3D; self.preorder_print(start.right, traversal)</span><br><span class="line">        return traversal</span><br></pre></td></tr></table></figure><ul><li><p>rule to BSTS: right of node is larger than left</p><img src="/images/tree4.png" width = 80% height = 80% div align=center /></li><li><p>unbalanced BSTS  </p><img src="/images/tree5.png" width = 80% height = 80% div align=center /></li><li><p>apply the rule to BSTS and pre-order Traversal. Search and insert  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class BST(object):</span><br><span class="line">    def __init__(self, root):</span><br><span class="line">        self.root &#x3D; Node(root)</span><br><span class="line"></span><br><span class="line">    def insert(self, new_val):</span><br><span class="line">        self.insert_helper(self.root, new_val)</span><br><span class="line"></span><br><span class="line">    def insert_helper(self, current, new_val):</span><br><span class="line">        if current.value &lt; new_val:</span><br><span class="line">            if current.right:</span><br><span class="line">                self.insert_helper(current.right, new_val)</span><br><span class="line">            else:</span><br><span class="line">                current.right &#x3D; Node(new_val)</span><br><span class="line">        else:</span><br><span class="line">            if current.left:</span><br><span class="line">                self.insert_helper(current.left, new_val)</span><br><span class="line">            else:</span><br><span class="line">                current.left &#x3D; Node(new_val)</span><br><span class="line"></span><br><span class="line">    def search(self, find_val):</span><br><span class="line">        return self.search_helper(self.root, find_val)</span><br><span class="line"></span><br><span class="line">    def search_helper(self, current, find_val):</span><br><span class="line">        if current:</span><br><span class="line">            if current.value &#x3D;&#x3D; find_val:</span><br><span class="line">                return True</span><br><span class="line">            elif current.value &lt; find_val:</span><br><span class="line">                return self.search_helper(current.right, find_val)</span><br><span class="line">            else:</span><br><span class="line">                return self.search_helper(current.left, find_val)</span><br><span class="line">        return False</span><br></pre></td></tr></table></figure></li></ul><h3 id="Heaps"><a href="#Heaps" class="headerlink" title="Heaps"></a>Heaps</h3><ul><li>also a kind of tree has its own rule</li><li>Max heaps and min heaps. max: parent always big than child; min vice versa. <a href="https://www.youtube.com/watch?time_continue=14&v=M3B0UJWS_ag" target="_blank" rel="noopener">link</a><img src="/images/maxheaps.png" width = 80% height = 80% div align=center /></li><li>heaps can have several trees, not like binary</li><li>heaps binary trees search big 0 time is $O(n)$</li><li>heapify: insert in the child leaf first, then compare it to the parent, if bigger then swap their value. The big O time is $O(logn)$<img src="/images/heapify.png" width = 80% height = 80% div align=center /></li><li>heap implementation: heap often store as sorted array. as shown below, array only store value and index, but tree will store bunch of pointers, which mean array save space.<img src="/images/heap2.png" width = 80% height = 80% div align=center /><img src="/images/heap3.png" width = 80% height = 80% div align=center /></li></ul><h3 id="self-balanced-tree-red-black"><a href="#self-balanced-tree-red-black" class="headerlink" title="self-balanced tree(red-black)"></a>self-balanced tree(red-black)</h3><img src="/images/self_balanceing.png" width = 80% height = 80% div align=center /><ul><li><p>definition: minimize the level of tree</p></li><li><p><strong>Red Black-Tree</strong> an extension of Binary tree. 5 rules</p><ul><li>every node is red or black</li><li>Every leaf (NULL) is black. Every node in your tree doesn’t otherwise have two leaves, must have null children(black)</li><li>If a node is red, then both its children are black.</li><li>the root node is black</li><li>Every simple path from a node to a descendant leaf contains the same number of black nodes.  </li></ul><img src="/images/red_black_tree.png" width = 80% height = 80% div align=center /></li></ul><p>still watching:   <a href="https://classroom.udacity.com/courses/ud944/lessons/7122604912/concepts/78867246220923" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud944/lessons/7122604912/concepts/78867246220923</a><br>  <a href="https://www.youtube.com/watch?v=O5Yl-m0YbVA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=O5Yl-m0YbVA</a></p><h2 id="Graphs-Networks"><a href="#Graphs-Networks" class="headerlink" title="Graphs(Networks)"></a>Graphs(Networks)</h2><img src="/images/graph.png" width = 80% height = 80% div align=center />- tree is a type of graph.- Node, edge  <img src="/images/graph2.png" width = 80% height = 80% div align=center /><h3 id="directions-and-cycles"><a href="#directions-and-cycles" class="headerlink" title="directions and cycles"></a>directions and cycles</h3><ul><li><p>directions like this figure show</p></li><li><p>avoid graphs has cycles, infinite loop  </p><img src="/images/graph3.png" width = 80% height = 80% div align=center /></li><li><p>DAG: directed Acyclic(no cycles) Graph. type show up often</p></li></ul><h3 id="connectivity"><a href="#connectivity" class="headerlink" title="connectivity"></a>connectivity</h3><img src="/images/graph4.png" width = 80% height = 80% div align=center />- right seems has more connection than left one.- remove one connection in left group, whole group will destoryed(disconnected)<h4 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h4><ul><li><p>disconnected:<br>Disconnected graphs are very similar whether the graph’s directed or undirected—there is some vertex or group of vertices that have no connection with the rest of the graph.</p></li><li><p>weakly connected:<br>A directed graph is weakly connected when only replacing all of the directed edges with undirected edges can cause it to be connected. Imagine that your graph has several vertices with one outbound edge, meaning an edge that points from it to some other vertex in the graph. There’s no way to reach all of those vertices from any other vertex in the graph, but if those edges were changed to be undirected all vertices would be easily accessible.</p></li><li><p>connected:<br>Here we only use “connected graph” to refer to undirected graphs. In a connected graph, there is some path between one vertex and every other vertex.</p></li><li><p>Strongly Connected:<br>Strongly connected directed graphs must have a path from every node and every other node. So, there must be a path from A to B AND B to A.</p></li></ul><h4 id="Graph-Representations"><a href="#Graph-Representations" class="headerlink" title="Graph Representations"></a>Graph Representations</h4><p>vertex orbject can have a list of edges it’s connected to and vice versa.<br><img src="/images/graph5.png" width = 80% height = 80% div align=center /></p><p>There are other graph represent ways:</p><ul><li><p>Edge list, could be 2D, and 3D</p><img src="/images/graph6.png" width = 80% height = 80% div align=center /></li><li><p>Adjacency list, store the node that adjacent to index  </p><img src="/images/graph7.png" width = 80% height = 80% div align=center /></li><li><p>Adjacency Matrix, store a matrix, if node 0 connect to node 1, then show 1, otherwise show 0 in matrix.  </p><img src="/images/graph8.png" width = 80% height = 80% div align=center /></li></ul><p>code from udacity <a href="https://classroom.udacity.com/courses/ud944/lessons/7114284829/concepts/79348548570923" target="_blank" rel="noopener">link</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">class Node(object):</span><br><span class="line">    def __init__(self, value):</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line">        self.edges &#x3D; []</span><br><span class="line"></span><br><span class="line">class Edge(object):</span><br><span class="line">    def __init__(self, value, node_from, node_to):</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line">        self.node_from &#x3D; node_from</span><br><span class="line">        self.node_to &#x3D; node_to</span><br><span class="line"></span><br><span class="line">class Graph(object):</span><br><span class="line">    def __init__(self, nodes&#x3D;[], edges&#x3D;[]):</span><br><span class="line">        self.nodes &#x3D; nodes</span><br><span class="line">        self.edges &#x3D; edges</span><br><span class="line"></span><br><span class="line">    def insert_node(self, new_node_val):</span><br><span class="line">        new_node &#x3D; Node(new_node_val)</span><br><span class="line">        self.nodes.append(new_node)</span><br><span class="line"></span><br><span class="line">    def insert_edge(self, new_edge_val, node_from_val, node_to_val):</span><br><span class="line">        from_found &#x3D; None</span><br><span class="line">        to_found &#x3D; None</span><br><span class="line">        for node in self.nodes:</span><br><span class="line">            if node_from_val &#x3D;&#x3D; node.value:</span><br><span class="line">                from_found &#x3D; node</span><br><span class="line">            if node_to_val &#x3D;&#x3D; node.value:</span><br><span class="line">                to_found &#x3D; node</span><br><span class="line">        if from_found &#x3D;&#x3D; None:</span><br><span class="line">            from_found &#x3D; Node(node_from_val)</span><br><span class="line">            self.nodes.append(from_found)</span><br><span class="line">        if to_found &#x3D;&#x3D; None:</span><br><span class="line">            to_found &#x3D; Node(node_to_val)</span><br><span class="line">            self.nodes.append(to_found)</span><br><span class="line">        new_edge &#x3D; Edge(new_edge_val, from_found, to_found)</span><br><span class="line">        from_found.edges.append(new_edge)</span><br><span class="line">        to_found.edges.append(new_edge)</span><br><span class="line">        self.edges.append(new_edge)</span><br><span class="line"></span><br><span class="line">    def get_edge_list(self):</span><br><span class="line">        edge_list &#x3D; []</span><br><span class="line">        for edge_object in self.edges:</span><br><span class="line">            edge &#x3D; (edge_object.value, edge_object.node_from.value, edge_object.node_to.value)</span><br><span class="line">            edge_list.append(edge)</span><br><span class="line">        return edge_list</span><br><span class="line"></span><br><span class="line">    def get_adjacency_list(self):</span><br><span class="line">        max_index &#x3D; self.find_max_index()</span><br><span class="line">        adjacency_list &#x3D; [None] * (max_index + 1)</span><br><span class="line">        for edge_object in self.edges:</span><br><span class="line">            if adjacency_list[edge_object.node_from.value]:</span><br><span class="line">                adjacency_list[edge_object.node_from.value].append((edge_object.node_to.value, edge_object.value))</span><br><span class="line">            else:</span><br><span class="line">                adjacency_list[edge_object.node_from.value] &#x3D; [(edge_object.node_to.value, edge_object.value)]</span><br><span class="line">        return adjacency_list</span><br><span class="line"></span><br><span class="line">    def get_adjacency_matrix(self):</span><br><span class="line">        max_index &#x3D; self.find_max_index()</span><br><span class="line">        adjacency_matrix &#x3D; [[0 for i in range(max_index + 1)] for j in range(max_index + 1)]</span><br><span class="line">        for edge_object in self.edges:</span><br><span class="line">            adjacency_matrix[edge_object.node_from.value][edge_object.node_to.value] &#x3D; edge_object.value</span><br><span class="line">        return adjacency_matrix</span><br><span class="line"></span><br><span class="line">    def find_max_index(self):</span><br><span class="line">        max_index &#x3D; -1</span><br><span class="line">        if len(self.nodes):</span><br><span class="line">            for node in self.nodes:</span><br><span class="line">                if node.value &gt; max_index:</span><br><span class="line">                    max_index &#x3D; node.value</span><br><span class="line">        return max_index</span><br><span class="line"></span><br><span class="line">graph &#x3D; Graph()</span><br><span class="line">graph.insert_edge(100, 1, 2)</span><br><span class="line">graph.insert_edge(101, 1, 3)</span><br><span class="line">graph.insert_edge(102, 1, 4)</span><br><span class="line">graph.insert_edge(103, 3, 4)</span><br><span class="line"># Should be [(100, 1, 2), (101, 1, 3), (102, 1, 4), (103, 3, 4)]</span><br><span class="line">print graph.get_edge_list()</span><br><span class="line"># Should be [None, [(2, 100), (3, 101), (4, 102)], None, [(4, 103)], None]</span><br><span class="line">print graph.get_adjacency_list()</span><br><span class="line"># Should be [[0, 0, 0, 0, 0], [0, 0, 100, 101, 102], [0, 0, 0, 0, 0], [0, 0, 0, 0, 103], [0, 0, 0, 0, 0]]</span><br><span class="line">print graph.get_adjacency_matrix()</span><br></pre></td></tr></table></figure><h4 id="Graph-Traversal"><a href="#Graph-Traversal" class="headerlink" title="Graph Traversal"></a>Graph Traversal</h4><ul><li>just like tree traversal</li><li>DFS(depth first search)</li><li>BFS(breadth first search)</li></ul><h2 id="Algorithm-Searching-and-Sorting"><a href="#Algorithm-Searching-and-Sorting" class="headerlink" title="Algorithm_Searching and Sorting"></a>Algorithm_Searching and Sorting</h2><p>code visualization: <a href="https://visualgo.net/en" target="_blank" rel="noopener">https://visualgo.net/en</a></p><h3 id="Binary-Search"><a href="#Binary-Search" class="headerlink" title="Binary Search"></a>Binary Search</h3><ul><li><p>efficiency: $O(log(n))$</p><img src="/images/binary_search.png" width = 80% height = 80% div align=center /><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def binarySearch(listData, value):</span><br><span class="line">    low &#x3D; 0</span><br><span class="line">    high &#x3D; len(listData)-1</span><br><span class="line">    while (low &lt;&#x3D; high):</span><br><span class="line">       mid &#x3D; (low+high)&#x2F;2</span><br><span class="line">       if (listData[mid] &#x3D; value):</span><br><span class="line">           return mid</span><br><span class="line">       elif(listData[mid] &lt; value):</span><br><span class="line">          low &#x3D; mid+1</span><br><span class="line">       else:</span><br><span class="line">          high &#x3D; mid-1</span><br><span class="line">    return -1</span><br></pre></td></tr></table></figure><p>Resource: <a href="https://www.cs.usfca.edu/~galles/visualization/Search.html" target="_blank" rel="noopener">https://www.cs.usfca.edu/~galles/visualization/Search.html</a></p></li></ul><h3 id="Recursion"><a href="#Recursion" class="headerlink" title="Recursion"></a>Recursion</h3><ul><li>1, call it self; 2, base case(get out the loop); 3, alter the input parament<img src="/images/recursion.png" width = 80% height = 80% div align=center /><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">the Fibonacci Sequence.</span><br><span class="line">def get_fib(position):</span><br><span class="line">    if position &#x3D;&#x3D; 0 or position &#x3D;&#x3D; 1:</span><br><span class="line">        return position</span><br><span class="line">    return get_fib(position - 1) + get_fib(position - 2)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h3><ul><li>in plce or not, in place: lest space; not: lest time. Tradeoff</li></ul><h4 id="bubble-Sort-in-place"><a href="#bubble-Sort-in-place" class="headerlink" title="bubble Sort(in place)"></a>bubble Sort(in place)</h4><ul><li><p>Big O is $(n-1)(n-1)=n^2$, best case is $O(n)$, the array is already sorted.</p><img src="/images/Bubble_sort.png" width = 80% height = 80% div align=center /><p>reference: <a href="https://en.wikipedia.org/wiki/Bubble_sort" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bubble_sort</a></p></li></ul><h4 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h4><ul><li><p>divide the arrray into one by one, then combine every two element except the first element. Then the combined two element compare first, then compare to others just as following images.  </p><img src="/images/merge_sort1.png" width = 80% height = 80% div align=center /><img src="/images/merge_sort2.png" width = 80% height = 80% div align=center /></li><li><p>Merge sort efficiency: $O(n*logn)$  </p><img src="/images/merge_sort3.png" width = 60% height = 60% div align=center /><img src="/images/merge_sort4.png" width = 60% height = 60% div align=center /><img src="/images/merge_sort5.png" width = 60% height = 60% div align=center /></li></ul><h4 id="Quick-Sort"><a href="#Quick-Sort" class="headerlink" title="Quick Sort"></a>Quick Sort</h4><ul><li>random choose one of element, then compare and move as following figure show.  <img src="/images/quick_sort.png" width = 60% height = 60% div align=center /><img src="/images/quick_sort2.png" width = 60% height = 60% div align=center />- efficiency: the worst case is O(n^2), the average and the best case is $O(nlogn)$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for each (unsorted) partition</span><br><span class="line">set first element as pivot</span><br><span class="line">   storeIndex &#x3D; pivotIndex + 1</span><br><span class="line">   for i &#x3D; pivotIndex + 1 to rightmostIndex</span><br><span class="line">    if element[i] &lt; element[pivot]</span><br><span class="line">      swap(i, storeIndex); storeIndex++</span><br><span class="line">  swap(pivot, storeIndex - 1)</span><br></pre></td></tr></table></figure><p>reference: <a href="http://visualgo.net/en/sorting/" target="_blank" rel="noopener">http://visualgo.net/en/sorting/</a></p><h3 id="some-Resources"><a href="#some-Resources" class="headerlink" title="some Resources"></a>some Resources</h3><p><a href="http://python.jobbole.com/87440/" target="_blank" rel="noopener">常用查找数据结构及算法（Python实现)</a></p>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> Data structure </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>盐湖到黄石6天不完全攻略</title>
      <link href="/2018/08/10/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/"/>
      <url>/2018/08/10/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<p>黄石6天游玩不完全攻略（多图，慎点）</p><h3 id="游玩关键词："><a href="#游玩关键词：" class="headerlink" title="游玩关键词："></a>游玩关键词：</h3><p>盐湖城，荒漠，摩门教，大盐湖；黄石公园，喷泉盆地（geyser basin），大棱镜喷泉，瀑布山川河流草地，徒步hiking，野牛，鹿；大提顿，湖，河，熊</p><h3 id="第1天"><a href="#第1天" class="headerlink" title="第1天"></a>第1天</h3><p>盐湖城有三大特征，荒漠，摩门教，大盐湖。我们是从Boston转机到了盐湖城，然后租车，在盐湖城玩了半天，看了最著名的摩门教圣地。盐湖城是摩门教徒所创建的一座城市。走在摩门教公园里，看见不多的宣教人员，人很友好。盐湖城大约有三分之二信仰摩门教，著名的杨百翰大学就是个摩门教会大学（不在盐湖城）。</p><img src="/images/yellowstone/摩门教.jpg" width = 80% height = 80% div align=center /><p>傍晚的时候,我们在盐湖城市区转了转，去了爵士主场场馆看了看。走在他们中心的街道上，竟然看不到几个人。这就是犹他州最大的城市?!<br><img src="/images/yellowstone/saltlake.jpg" width = 80% /> <img src="/images/yellowstone/saltlake2.jpg" width = 80% /></p><p>然后我们开车到了盐湖城和黄石中间的一个小城，logan住宿，一个超漂亮的农场。<br><img src="/images/yellowstone/农场.jpg" width = 80% height = 80% div align=center /></p><h3 id="第2天"><a href="#第2天" class="headerlink" title="第2天"></a>第2天</h3><p>第二天我们开车从黄石西边入口进入，35刀门票，直达第一个间歇喷泉。第一次见到这种火山形成的地貌奇观，很是兴奋。接下来又看了各种不同的间歇性喷泉，在看完大棱镜之后，就再也提不起看喷泉的兴致了。大棱镜实在太美了。<br><img src="/images/yellowstone/gayser basin.jpg" width = 80% height = 80% div align=center /><br><img src="/images/yellowstone/大棱镜.jpg" width = 80% height = 80% div align=center /></p><p>我们的路线是第一天玩北线，在谷歌trip app上把景点标出来，就可以显示在谷歌地图上，然后离线谷歌地图。黄石没有信号，所以离线地图真的非常重要。<br><img src="/images/yellowstone/猛犸温泉.jpg" width = 80% height = 800% div align=center /><br>北线上面有温泉盆地，最著名的是猛犸温泉景区，很漂亮。不过相比较看岩石喷泉盆地，我更喜欢他周边山谷的小溪，可以去hiking，可以钓鱼，可以淌水。</p><img src="/images/yellowstone/小溪.jpg" width = 80% height = 80% div align=center /><p>晚上回家住在Ashton的一个地方，开车1个半小时，想住在公园里面，要么太贵，要么早被订完了。据说很多都是提前一年预订的。还有一种方式，露营。可以租个房车，可以去营地搭个帐篷，有些地方要预订，有些地方是先到先得。我们比较遗憾，看着别人家的camping很是羡慕。不过Airbnb真的也不错，现在乡村或小镇的Airbnb设施完备，比酒店住的还舒服。<br><img src="/images/yellowstone/回家路上.jpg" width = 80% height = 80% div align=center /></p><h3 id="第3天"><a href="#第3天" class="headerlink" title="第3天"></a>第3天</h3><p>接下来一天，我们玩了南线上的景色，南线主要是大棱镜喷泉，必须爬到山上才能俯瞰到她的全貌，之后去old faith 那边看一些小盆地就见怪不怪了。然后主要去yellowstone lake那边玩湖，其实是没啥意思的，中途我们想去深林里找熊🐻，可是刚走不远就被自己吓了出来。hiking深林的话最好带着防熊喷雾，蚊虫喷雾等装备。老老实实的去湖那看风景和一些间歇性喷泉，可是此时我们已经提不起什么精神了，快审美疲劳。所以下面南线的景色很快就转完了。<br><img src="/images/yellowstone/yellow lake.jpg" width = 80% height = 80% div align=center /></p><p>于是我们提前去了东南线上的风景，Hayden 山谷看野牛。然后傍晚去了峡谷村庄(canyon village)看lower fall。登上高处，俯瞰远处黄色，红色的峡谷间倾泻而下的瀑布，实在是太美了，瞬间满血复活。私以为傍晚去看最好，光不那么刺眼，照的峡谷更加鲜艳动人。<br><img src="/images/yellowstone/lowerfall.jpg" width = 80% height = 80% div align=center /></p><h3 id="第4天"><a href="#第4天" class="headerlink" title="第4天"></a>第4天</h3><p>早上还是先去峡谷村庄去看剩下的upper fall，要爬山，下山，好一阵子。早上的光照着瀑布，不那么柔和，总感觉有一层雾气隔着你和瀑布，欲遮还羞。<br><img src="/images/yellowstone/uperfall.jpg" width = 80% height = 80% div align=center /></p><p>接下来我们去玩了东北那条线，那边的风景跟其它的线又有些不同，山和草地比较多，爬上山顶，一览无余。ps：用苹果手机vivid滤镜照出来的风景显得更加鲜艳漂亮，拍照新手推荐。</p><p>景色虽然很美，但看多了也乏，我们找到了一个hiking 的地方，一个沿着山谷河流的悬崖。路其实不算危险，虽然有的路就在悬崖边上。对面是我们之前开车过来的路，之前在那边看到时候，景色很美，但在这边看，风景不仅更壮美，也多了一种刺激感和满足感。<br><img src="/images/yellowstone/hiking.jpg" width = 80% height = 80% div align=center /></p><img src="/images/yellowstone/hiking2.jpg" width = 80% height = 80% div align=center /><p>来这边游客很少，大多数都是有经验的远足者。遇到其他人的时候，都会Hi，hello寒暄下，颇有种知音的感觉。走了大概有3个小时左右，连绵不绝的山脉，估计走上一天也到不了头。</p><img src="/images/yellowstone/hiking3.jpg" width = 80% height = 80% div align=center /><img src="/images/yellowstone/hiking4.jpg" width = 80% height = 80% div align=center /><p>接下来，我们吃完饭，沿着东北方向的线继续前进。因为这条线上，游客实在不多，我们可以放肆的随时停下来拍拍照片，淌淌小溪，看看野牛，好不自在。<br><img src="/images/yellowstone/野牛.jpg" width = 80% height = 80% div align=center /><br><img src="/images/yellowstone/小溪2.jpg" width = 80% height = 80% div align=center /></p><p>这条线上景点虽然不多，但我们一致觉得很美，没有太多游客的嘈杂，让我们随意的深入这样的大自然。不需要做什么攻略，自己探索就行。山路18弯，开起来据说感觉beir(天津话)爽。<br><img src="/images/yellowstone/十八弯.jpg" width = 80% height = 80% div align=center /></p><img src="/images/yellowstone/十八弯2.jpg" width = 80% height = 80% div align=center />晚上住在东线出去的cody，一个安安静静地小城，建筑很美，设施也比较齐全。朋友笑称可以在这里住一辈子。<img src="/images/yellowstone/青山.jpg" width = 80% height = 80% div align=center /><h3 id="第5天"><a href="#第5天" class="headerlink" title="第5天"></a>第5天</h3><p>第五天早上开车穿过黄石前往大提顿国家公园，我们准备玩上一天来着。可是，前有黄石，再看大提顿就没啥意思了。如果先看大提顿，再去看黄石，估计会好很多。在著名的Jackson湖畔hiking，兴许是太累了，或提不起兴致，我们绕到湖的另一边，就直接坐船回来了。<br><img src="/images/yellowstone/大提顿.jpg" width = 80% height = 80% div align=center /></p><p>不过大提顿还是给了我们惊喜。在黄石公园我们没有找到熊，在这里却看到好多只。一个熊妈妈带着好几只小熊仔过马路。看着熊妈妈不大的身材，我们嘲笑自己之前的退缩，还真以为熊都像是熊大熊二那么大吗？<br><img src="/images/yellowstone/bear.jpg" width = 80% height = 80% div align=center /></p><p>这边有一个漂流项目，沿着Snake river，沿途看景应该会不错。<br><img src="/images/yellowstone/DSC02544.jpg" width = 80% height = 80% div align=center /></p><p>晚上就在大提顿边上住了下来。一个叫Victor 的小镇，搜yelp的时候，竟发现小镇啥吃的都有，法餐，意餐，中餐，泰餐，韩餐，当然还有美餐。我们去吃了顿韩餐，不敢去烤肉。在黄石这些天，我们几个嘴唇特别干，各种上火。所以再来的话，防晒，唇膏等装备要备齐。<br><img src="/images/yellowstone/沿途美景.jpg" width = 80% height = 80% div align=center /></p><h3 id="第6天"><a href="#第6天" class="headerlink" title="第6天"></a>第6天</h3><p>一大早开回盐湖城，出发大盐湖。</p><img src="/images/yellowstone/saltlake4.jpg" width = 80% height = 80% div align=center /><p>一望无际的大盐湖，确实也属奇观。脱了鞋子，走在已经干涸的盐湖上，感受百万年来的环境变迁。</p><img src="/images/yellowstone/saltlake3.jpg" width = 80% height = 80% div align=center /><p>我们去玩的小岛名叫羚羊岛(Antelope island)，但其实在该小岛上的土生羚羊已经灭绝了。在小岛上hiking应该是非常好的，但这个季节不太合适，太热了。所以我们开车车绕了小岛一圈就出来了。<br><img src="/images/yellowstone/羚羊岛.jpg" width = 80% height = 80% div align=center /></p><p>然后直奔犹他大学，找停车的地方就找了好久。犹他大学历史比较悠久，里面的摩门教徒非常多，建筑还是蛮新的，但我们可能只看了外面大概，貌似山下，山上的一片都是他们的。犹他大学虽然排名不高，但他们学校的取得的成就却不少。反观我们学校，排名虽然比它高，但历史沉淀不足，距离世界名校可能还有很长的路要走。<br><img src="/images/yellowstone/犹他大学.jpg" width = 80% height = 80% div align=center /></p><p>之后我们还去了美国第一家KFC门店吃了顿自助。来美国还是第一次吃KFC，记得国内我还是蛮喜欢吃的。可是，在炸鸡遍天下的美国，即使是KFC，也真心吃不下多少了，也不知道跟国内口味还一不一样。<br><img src="/images/yellowstone/kfc.jpg" width = 80% height = 80% div align=center /></p><p>旅途结束，洗车，还车，上飞机，回家，躺尸。</p><p>说说这次旅程的我们的几点感受。</p><ol><li>我们发现黑人旅游非常少见，几天下来，见到的黑人不超过2位数。绝大多数是亚洲人和白人。不得不说，种族间的贫富差距还是有的吧。</li><li>旅游还是自助好。跟团基本上都是去看那几个固定的景点，看多了，还是很乏的。好多的美景需要自己去探索。在后面的阶段，我们就没看见旅游团，而恰恰是后面阶段更让我们觉得不虚此行。</li><li>人与自然…<img src="/images/yellowstone/羚羊岛2.jpg" width = 100% height = 100% div align=center /></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6月每日一吐</title>
      <link href="/2018/07/01/6%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/"/>
      <url>/2018/07/01/6%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/</url>
      
        <content type="html"><![CDATA[<p>很早之前，坚持过一段时间的，每日吐槽身边的事和人，以及自己。期望自己能够发现生活中有趣，无趣的一面，关注生活，而不是让时间悄悄溜走。</p><p>有了自己的博客，没什么人关注。目前的状态还是蛮喜欢的。努力记录，努力创造。</p><h3 id="6-30-2018"><a href="#6-30-2018" class="headerlink" title="6/30/2018"></a>6/30/2018</h3><p>热死了，热死了，今天热死了。月末最后一天，总结。</p><ol><li>室内定位项目，从确定怎么做室内定位，到尝试深度学习的简单算法，autoencoder，ensemble，方法的尝试，初步完成了精度的要求。下一步要改进目前的算法，以及把做的整理成文档。</li><li>完成技术博客4篇，主要是ensembel算法，机器学习经典算法总结，深度学习调参总结，以及数据的预处理特征工程等。2篇随笔。</li><li>简单的算法书中的几大算法已经看完；有待进一步整理及练习拓展；</li><li>确定自己职业方向：AI产品经理，修改简历，制作了简历个人网站。下一步，了解更多的关于AI产品经理的东西；继续修改简历。投简历。</li><li>周边游玩4次，2次艺术博物馆，1次海边，一次罗德岛。</li><li>每周坚持2次跆拳道练习，基本动作已经熟悉。</li></ol><h3 id="6-29-2018"><a href="#6-29-2018" class="headerlink" title="6/29/2018"></a>6/29/2018</h3><p>在家“休息了一天”，就没去学校，然后温度就飘升，干到了32度了。热死了，这是要逼我去学校好好学习天天向上啊。明天要修简历，找工作，准备面试，你说下个月这些能准备好吗？<br>话说今天把《太空无垠》美剧第三季追完了。看的好激动，末尾他们开启星门，将去宇宙深处去探索移民了。这是我近年来看的科幻中上佳之作。多希望我们也能进入宇宙大航海时代，那一天离我们还有多远啊。</p><h3 id="6-28-2018"><a href="#6-28-2018" class="headerlink" title="6/28/2018"></a>6/28/2018</h3><p>今天早上会上闲聊，我问一个法国来的小伙，巴黎的生活水平相比波士顿哪个比较贵。我开始觉得巴黎这么高大上的国际大都市应该会比波士顿贵吧。但他却说，波士顿要贵出一倍。就拿房租来说，800刀左右在巴黎能住一套房子，而我们这只能租一个单间。不过在美国其他偏远城市，租房可能就比巴黎便宜了。但巴黎可是国际性的大都市啊，波士顿没有她有名吧（虽然也有名），房租竟然这么贵。我好多同学都是1000多刀住一个破破的房间，没办法，离学校近啊。之前就看到过学校里有传单，美国人民抗议因为学校的原因，使周边房价上涨到住不起的地步。</p><h3 id="6-27-2018"><a href="#6-27-2018" class="headerlink" title="6/27/2018"></a>6/27/2018</h3><p>念头是不是我。</p><h3 id="6-26-2018"><a href="#6-26-2018" class="headerlink" title="6/26/2018"></a>6/26/2018</h3><p>中午跟一个朋友吃饭。问起他国内工作几年了。竟然有7年，但他看起来非常年轻。我很喜欢他，总是挂着笑脸，跟他聊天也蛮舒服的。我们聊起来，身边确实有不少工作几年后，然后出来读书的，而且这绝大多数人真看不出岁数，‘臭不要脸的’混迹在国内刚毕业出来的学生中。这是不是可以说明年轻心态对相貌的积极影响呢？</p><p>而且工作过后出来的，跟学生一比，交流起来也确实显得成熟多了。我觉得这是优势。学生应该增加工作经验，来扩大自己的视野，并且提高自己的情商。</p><h3 id="6-25-2018"><a href="#6-25-2018" class="headerlink" title="6/25/2018"></a>6/25/2018</h3><p>今天真是排了一天的队伍啊。今天和室友准备去拿驾照。没有SSN号，先去一个地方办一个材料，等了足足2个多小时。然后去拿驾照的地方办，这个快， 不过也排了半个小时的队伍。然后一看12点半多了，去吃个饭吧。哎，来到一个餐厅，还要排队，又排了将近半小时。如果不是看在店比较火，面比较好吃的份上，真是要怒了。因为旁边是电影院，我说好久没来看电影了，然后我们一合计吃完直接去看电影了…你晓得电影放映时间不可能我们想看就有的，于是我们也惯性似的等了40分钟。嗨，都排了一天的队了，还在乎这半个小时的等待吗。</p><h3 id="6-24-2018"><a href="#6-24-2018" class="headerlink" title="6/24/2018"></a>6/24/2018</h3><p>今天我们俱乐部聚会，我提前1小时15分钟出发，然后遇到地铁坏掉，转公交，再转地铁。然后地铁死活等在那不走，这是要我迟到啊。看着已经过了约定时间，我心里慌的不行了。迟到那么久，让各位大拿等着你，是不是不太好啊。于是我想来想去给负责人发了条微信：…哥，地铁卡在这…抱歉…去了会很晚，就不过去了吧…<br>然后负责人很快回了个“好”。心理痛斥自己，下次即使提前2个小时也要早点出来，让别人等是很可耻的，很没有时间观念。我在地铁上正做着深刻的检讨，只见微信群里，一个人说正在赶，还需要时间，然后陆陆续续的人，都说没到。看见最晚一个人是距离约定时间快半个小时了，还在找地方停车。<br>我的天呐，我要去的话，其实还不算最晚的。干嘛要给负责人发那条心虚的微信。气人啊。大家都这么没有时间观念吗？</p><h3 id="6-23-2018"><a href="#6-23-2018" class="headerlink" title="6/23/2018"></a>6/23/2018</h3><p>今天听朋友说开车的三个阶段：第一阶段，很虚。对待各种状况总感觉捉襟见肘；第二阶段，进行暴躁阶段。说明你已经开车比较稳了，对待一些路上车辆犯的新手错误越来越不能忍受了，总以为自己现在比这帮傻逼会开车；第三阶段，比较平复了。见惯了新手，老手开车的各种姿势，也就没那么大的感觉起伏了。</p><p>他说的不一定对，但听他这么一讲，我一下对自己开车有底了。知道自己处于什么水平。这是大家都会出现的阶段，所以不是你一个人笨，大家都这样。这让我联想到大学室友提炼的恋爱的几个阶段，这让我搞对象能就知道方向，而不会摸不着头脑。<br>我在想这种普世的分级手段，至少有两个好处：</p><ol><li>好像升级打游戏似的，目标清晰明确，不会让你摸不着头脑；</li><li>这是一个放之大部分人皆准的道理，而不是需要单枪匹马独自面对的状况，这种心理学上我不知道怎么说。但是人是来社交动物，如果就觉的只有你一个人笨笨的，那你的心情肯定是很糟糕的。但当发现所有人都跟你一样笨，你会不会轻松很多。</li></ol><h3 id="6-22-2018"><a href="#6-22-2018" class="headerlink" title="6/22/2018"></a>6/22/2018</h3><p>终于把自己的简历网站给做好了。梳理一下自己做过的事情，感觉调理多了。下一步就可以开始投简历了。</p><h3 id="6-21-2018"><a href="#6-21-2018" class="headerlink" title="6/21/2018"></a>6/21/2018</h3><p>遇到一朋友，问工作找的怎么样了。说还没什么眉目。国外工作这么难找吗？除了计算机专业的，貌似其他专业都不景气，尤其是去年各大公司都招满的情况下。我说那你回国吧，至少工资也能拿1万5了吧。然后他哈哈的对我说，以前工资就达到了那个水平了。这么说，感觉找不到2万以上的工资，都对不起2年国外的时间花费啦。</p><h3 id="6-20-2018"><a href="#6-20-2018" class="headerlink" title="6/20/2018"></a>6/20/2018</h3><p>今天回家比较早，下午4点钟坐地铁竟然都坐满了。你知道为什么吗？他们很多人下班了…对于天朝来的我们，感觉不可思议:astonished:。美国人真是会享受啊，4点钟就下班了。早上就算有人8点上班吧，去除中午吃饭1个小时，7个小时上班时间。想想是不是很幸福。</p><p>今天正好看到一篇硅谷嘲笑中国”996”的工作作息的文章。作者说，如果硅谷实行这种隐形制度，人就会跑光了，中国的这种奇葩作息说到底是被员工给惯的。</p><p>我认为有这几个原因形成了这种隐形制度：</p><ol><li>中国特殊的发展时期。说到底中国还是不够发达，资源有限，竞争压力大，新时代的机遇来临，谁能早日出头就能占据资源置高点。</li><li>中国独特的文化。隐忍，内向，等级思想，从重，不作为等等，无不助长着这种制度的诞生。</li></ol><p>不禁想起卓别林大师的经典电影《摩登时代》，何其相似。<br>相对来说，国外的个人主义比中国更加盛行，他们很早的就组成了工会维护劳动人民个人的权益。好消息是，新时代成长起来的孩子，也越来越有个性，相信随着时间推移，这种制度会成为历史吧。</p><h3 id="6-19-2018"><a href="#6-19-2018" class="headerlink" title="6/19/2018"></a>6/19/2018</h3><p>今天早上的跆拳道课上，老师叫我们组队练习各种动作。我跟一个60多岁的人分到了一组。别看他老，基本上，我们能做的动作，他都能做。甚至比我标准。<br>让我不禁想到在威海遇到的一个晨练大爷，明明80多岁了，看起来跟60多岁似的，竟然翻跟头一点都不费劲。而我们这些年轻人，一个简单的翻跟头都很难实现了吧。</p><h3 id="6-18-2018"><a href="#6-18-2018" class="headerlink" title="6/18/2018"></a>6/18/2018</h3><p>今天再次去罗德岛去参观那里的豪宅。去年7月份的时候去过，那时候办的会员正好用上了。第二次参观豪宅跟第一次有什么不同的感受吗？<br>因为是第二次，没有了第一次的震惊之感。你看，我的几个同学一边看一边“大惊小怪的”说着有钱人的世界果然不然一样，一个厕所都比我家大。<br>探索的欲望变小了，而是期待找到上次见到过的印象深刻的物品，或艺术。下面这个美女就是我找了5个豪宅才终于找回的。上次就对她“一见钟情”。只可惜这个女主人的命运并不是很好。<br>豪宅的后花园是面朝大海的，每次坐在宽广的草坪上，我就会想起来小时候，我们玩的时候经常从前庄跑到后庄，到处都是树木和花草，感觉这个世界好大。而现在开着车从一个省到另外一个省，也没有了这种感觉。不禁想如今的孩子没有广阔的天地够他们“驰骋”了，因为他们爸妈最多买的起100多平的房子。<br><img src="/images/Elizabeth in the Elms.jpg" width = 50% height = 50% div align=center /></p><h3 id="6-17-2018"><a href="#6-17-2018" class="headerlink" title="6/17/2018"></a>6/17/2018</h3><p>说说我在追的几部科幻美剧。<br>大家都在追西部世界，我却越来越提不起兴趣来了，已经旷了几集。都在回忆，可能是要把潜藏的线都给挖掘出来。但对于我来说，不够宏大，太过细节，有点拖拉。4颗星。</p><p>我更喜欢《太空无垠》这部太空科幻opera。已经追到3季了。前2季主题是移民太空的火星人，小行星人以及地球母星之间的三角关系。剧情跌宕，而且是完全有可能发生的。第三季潜藏的主题开始浮现，人类探索太空与外星高科技生物的相遇。每集看的我热血沸腾的。我最怕科幻剧停留在固有场景里面，期待这部剧有更多的场景突破。5颗推荐</p><p>另外还追了《真实人类》（Humans）,该剧展示了AI觉醒后与人类的冲突与相处。4颗星。</p><p>之前还追《黑暗物质》（The dark material）,也是太空opera，展示了宏大的天空场景，各大公司，政治团体相互争夺地盘的事情。不过里面有各种元素，AI, 平行空间，各种科幻概念等等。非常值得一看。 4.5星推荐</p><h3 id="6-16-2018"><a href="#6-16-2018" class="headerlink" title="6/16/2018"></a>6/16/2018</h3><p>今天跟朋友比谁宅，看来是我胜出了。我喜欢独处，也喜欢跟朋友在一块。很喜欢喧嚣后的平静那种感觉。<br>但长时间的独处会让人过多的沉浸在自己的世界之中，而与朋友，家人，社会人一起的时候，那种连接才是存在的基础。<br>所以说，我不很在乎去哪玩，哪吃，我更在乎的是跟谁。</p><h3 id="6-15-2018"><a href="#6-15-2018" class="headerlink" title="6/15/2018"></a>6/15/2018</h3><p>今天跟室友聊起来找对象的事情。她说她爸叫她赶紧毕业回家，不然这边找不到，快成老姑娘了。我也不好说意思说啥了。我自己都成老男孩了，没资格去说。</p><p>身在国外，找对象很难吗？我觉得相对国内来说，要难的多。但分人，受欢迎的美女到哪都是香饽饽吧？会撩妹的小哥，应该也不会觉得难吧。但对于我等一般人来说，几年的国外生活，找个对象真难。但奇怪的是，找对象的需求貌似非常旺盛。尤其是上研究生的女生，哈哈。</p><p>出国在外，大家变得小心翼翼，一是文化，二是钱吧。这里聚会变的少了，或者聚会范围也不像国内那么一桌一桌的，顶多几个人。这就在无形中让大家没有那么多机会去接触其他的人，更何况是搞对象呢。  </p><p>所以我早就想做一个海外，或者就是波士顿地区的交友平台了。肯定能火…</p><p>现在东风已备，只差…</p><h3 id="6-14-2018"><a href="#6-14-2018" class="headerlink" title="6/14/2018"></a>6/14/2018</h3><p>跟优秀的人为伍，真的会让你的血液都开始兴奋。与他们共事，可期，可成。又让我不安的血液躁动了起来。真的还有好多事情要做啊。</p><p>话说，做事情，我是非常控制欲的。每次做什么项目的时候，如果不能掌控主导，就会变的焉了。不过话说自己做的项目还真是自己主导的居多。不管是研究生课题，还是课程设计等等。</p><p>记得刚来美国上了一门创业课，跟2个外国人组队，最后没选我的idea，项目实施我没法掌控，所以那时候有些无所适从的。可能那时候英语也不好，被老外嫌弃的不要不要地…</p><h3 id="6-13-2018"><a href="#6-13-2018" class="headerlink" title="6/13/2018"></a>6/13/2018</h3><p>刚刚收到好友的微信，看的我莫名其妙。这微信像是继续我们刚刚没有说完的事情。可是跟他聊这件事情，不是前几天早上吗？ 看了他的上一条微信视频记录，竟然是今天早上打来的。 一下恍然，对啊。今天早上被他吵醒，聊了1个半小时。怎么大脑会把这个记录默认为好几天前的事情呢？</p><p>朋友在国内，晚上给我打的电话，一觉醒来，仍然念着这件事，于是顺手发了句昨晚没来及说的话。而我一天下来，经历了很多的事情。一下把时间度感给拉长了很多。无怪乎，人都说在同一时间尺度下，唯一能延长你寿命的方式就是增加你的体验感。</p><h3 id="6-12-2018"><a href="#6-12-2018" class="headerlink" title="6/12/2018"></a>6/12/2018</h3><p>刚刚晚上看了一个Youtube视频，讲李敖大师怒骂台湾军队发言的视频<a href="https://www.youtube.com/watch?v=hwg4TQOPVCU" target="_blank" rel="noopener">大陆打过来能抵抗多久</a>,如果能点开的话可以看看。</p><p>看到了一个还算理智的评论，台湾人认同中国的根，但要让他们回归统一，他们是不想的。</p><p>我觉得这个也是正常的，作为你固有的东西，任何人都会想捍卫一下的。如果我是台湾人，我也不想回归。我虽然能看到大陆的好，但同时我更能看清大陆的不好。想要让我回归，那你可能要改掉你所有的毛病才行。</p><p>我回复他的内容如下：<br>看到您比较理智的评论，特地点开看了下。貌似您的回复也是最多的。很喜欢理智的声音。首先声明两岸真是一家亲。我最喜欢的歌手是周杰伦，很欣赏蔡康永。现在大陆和台湾的综艺节目都在互播。单从文化娱乐来看上来说，就是不可分割的。<br>来美国期间，我很深刻的体会到了自由的感觉。这一点毋庸置疑，国内很多东西都是要噤声的，我也觉得这个是不对的。但同时我也看到了美国所谓的民主，贫富差距巨大。给你投票，你解决了民众多少生活问题。有时候，能说几句话就以为民主有多好，可是对于那些吃不起饭，看不起病的人，提供更多的帮助可能更实际些。<br>中国目前是有很多的问题的，但他至少现在是走在解决人民问题的道路上的（拿阴暗面来做反例，任何一个政体国家都是有的）。台湾问题比较复杂，武力统一台湾是绝对不可取的，反对一切的战争行为。中国只有越来越富强了，两岸交流越来越离不开对方了，自然而然就是统一的了。</p><p>补充个昨天就想说的：<br>美国民主貌似很好，很自由，但他问题真的很多；中国问题虽然很多，但是她做的越来越好。全面超越美国那只是时间的问题。</p><h3 id="6-11-2018"><a href="#6-11-2018" class="headerlink" title="6/11/2018"></a>6/11/2018</h3><p>转眼间，来美国已经一年半了。来时认识的一哥们今天见面(来我这边租房)，发现快成了小胖子了都。来的时候，挺帅的小伙子，经过美国1年多的“喂养”，快不复存在了。我记得当时还问他健身的技巧。回来后，我照了照镜子，看着微胖的小脸，下定决心，每周要去健身房3次，嗯，3次….不能少了</p><h3 id="6-10-2018"><a href="#6-10-2018" class="headerlink" title="6/10/2018"></a>6/10/2018</h3><p>实在想吐槽下，老师叫我们用matlab来实现python的代码。你知道用matlab有多麻烦，多落后吗？你知道机器学习用matlab我一窍不通吗？你知道现在大家做机器学习，尤其是深度学习都用python吗？你知道你的好基友Dens，他们组现在都用的python了吗？发现现在搞学术都喜欢用matlab，但那帮学术搞完搞业界的，都已经看到python才是正道啊。</p><p>一切都是最好的安排（心中默念），或许哪天真能用到呢。</p><h3 id="6-9-2018"><a href="#6-9-2018" class="headerlink" title="6/9/2018"></a>6/9/2018</h3><p>今天boston有龙舟，有慈善长跑，还有同性恋大游行。big day! 然而，我竟窝在家里改算法…说好的体验生活呢，说好的去认识更多的人呢？你就是个宅男，鉴定完毕！</p><h3 id="6-8-2018"><a href="#6-8-2018" class="headerlink" title="6/8/2018"></a>6/8/2018</h3><p>今天竟然是第一篇吐槽。没什么可说的，上来撕自己先。我很喜欢早上的时光，因为那时候还有自控力，看了会书，学了会习。吃完午饭，放个电影放松一下，就完蛋了。一下午就看电影睡觉…</p>]]></content>
      
      
      <categories>
          
          <category> 大话西游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活无趣 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ensemble</title>
      <link href="/2018/06/28/Ensemble/"/>
      <url>/2018/06/28/Ensemble/</url>
      
        <content type="html"><![CDATA[<p>Ensembel(集成学习)是一个简单，但非常有效的算法，在各大kaggle竞赛中，获得很高排名的，很多都应用了ensemble方法。这里是对ensemble learning 进行优秀资源的整理，便于以后查看。</p><p>了解集成学习可以从这篇blog开始：<br><a href="https://blog.csdn.net/qq_36330643/article/details/77621232" target="_blank" rel="noopener">集成学习(ensemble learning)原理详解</a></p><p>常见的Ensemble方法有这2种：Bagging and boosting。还有现在越来越多的stacking and blending。</p><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging 算法如下图，通过随机采样训练集，进行训练，采集T个训练集，就训练T个弱学习器。然后通过一定的结合策略，如取平均，或者vote等形式变成一个强学习器。采集训练集时，是有放回的采集。<br><img src="/images/bagging.png" width = 80% height = 80% div align=center /></p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p><p>Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(<a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting" target="_blank" rel="noopener">Gradient Boosting Tree,quora</a>)。AdaBoost和提升树算法的原理在后面的文章中会专门来讲。From: <a href="https://blog.csdn.net/qq_36330643/article/details/77621232" target="_blank" rel="noopener">link</a></p><img src="/images/boosting.png" width = 80% height = 80% div align=center /><p>原理解释：</p><iframe width="754" height="400" src="https://www.youtube.com/embed/fecp5nmetws" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><iframe width="754" height="400" src="https://www.youtube.com/embed/1GxscvKU2Ic" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="Stacking-amp-blending"><a href="#Stacking-amp-blending" class="headerlink" title="Stacking&amp;blending"></a>Stacking&amp;blending</h2><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p>数据比赛大杀器—-模型融合(stacking&amp;blending)<br><a href="https://blog.csdn.net/qq_18916311/article/details/78557722" target="_blank" rel="noopener">大话机器学习之STACKing,一个让诸葛亮都吃瘪的神技</a></p><img src="/images/stacking1.png" width = 80% height = 80% div align=center /><h3 id="错误认知"><a href="#错误认知" class="headerlink" title="错误认知"></a>错误认知</h3><p>研究了stack技能有一阵子，查到的资料和代码基本上都是<a href="https://zhuanlan.zhihu.com/p/25836678" target="_blank" rel="noopener">这样的</a>。里面给的图如下图</p><img src="/images/stacking2.jpg" width = 100% height = 100% div align=center /><p>看他的代码怎么都不能理解。KFold，cross-validation不是应该一个model就要训练了5次吗？为什么图中是用<strong>一个</strong>model来训练<strong>一个</strong>Fold集，而代码中是<strong>每个</strong>model都要训练<strong>每个</strong>fold集。</p><p>所以大部分转载或者写的stack方法都是错误的？不一定，有可能是没注意，有可能他们理解方式不一样。可惜对新手的我们很具有误导性啊。下文的中的作者就遇到跟我一样的情况。</p><h3 id="正确认知"><a href="#正确认知" class="headerlink" title="正确认知"></a>正确认知</h3><p>感谢<a href="https://zhuanlan.zhihu.com/p/26890738" target="_blank" rel="noopener">这篇文章</a>的作者跟我有一样的疑虑,正确的图应该是下面这样的。</p><img src="/images/stacking3.jpg" width = 100% height = 100% div align=center /><p>对于每一轮的 5-fold，Model 1都要做满5次的训练和预测, 之后的得到第一层的预测集P1，<br>model 2做满5次训练和预测，预测的集合是P2，once again, P3, P4, P5。 [P1,P2,P3,P4,P5]作为第二层训练集的input part. output part仍然是整个train set的label.</p><p>testset 在model 1中，每一次的训练就预测一次，总共有[T1,T2,T3,T4,T5],我们这时候后，取 $mean[T1,T2,T3,T4,T5]$ 作为我们下一层的测试集 t1，同样道理，model 2时，得到t2,….第二层的测试集就是[t1,t2,t3,t4,t5]，label仍然时整个test set的label.</p><p>附上该作者的代码解释：<br><img src="/images/stacking4.jpg" width = 100% height = 100% div align=center /></p><h3 id="多维数据的处理办法"><a href="#多维数据的处理办法" class="headerlink" title="多维数据的处理办法"></a>多维数据的处理办法</h3><p>以及我自己项目的部分代码。我的项目label值是一个二维的坐标，所以情况相比一维要复杂的多。在存储第二层的数据时，花了很多力气。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">kf &#x3D; KFold(n_splits&#x3D;5, random_state&#x3D;2018)</span><br><span class="line"></span><br><span class="line">def get_oof(clf, X_train, y_train, X_test):</span><br><span class="line"></span><br><span class="line">    blend_train &#x3D; np.zeros((y_train.shape[0],2))  # 二维label值</span><br><span class="line">    blend_test &#x3D; np.zeros((X_test.shape[0],2))</span><br><span class="line">    blend_test_skf &#x3D; np.zeros((X_test.shape[0],2,5)) # 5是因为有5次训练</span><br><span class="line"></span><br><span class="line">    for i, (train_index, test_index) in enumerate(list(kf.split(X_train))):</span><br><span class="line">        print(&quot;Fold&quot;, i)   </span><br><span class="line"></span><br><span class="line">        kf_X_train &#x3D; X_train[train_index]</span><br><span class="line">        kf_y_train &#x3D; y_train[train_index]</span><br><span class="line">        kf_X_test &#x3D; X_train[test_index]</span><br><span class="line">        kf_y_test &#x3D; y_train[test_index]</span><br><span class="line"></span><br><span class="line">        model &#x3D; clf(kf_X_train,kf_y_train)</span><br><span class="line"></span><br><span class="line">        blend_train[test_index]&#x3D;model.predict(kf_X_test)  #</span><br><span class="line"></span><br><span class="line">        blend_test_skf[:,:,i] &#x3D; model.predict(X_test)   #</span><br><span class="line"></span><br><span class="line">    blend_test[:,:]&#x3D;blend_test_skf.mean(axis&#x3D;2)</span><br><span class="line">    return blend_train, blend_test</span><br></pre></td></tr></table></figure><p>之后需要把P1,P2,P3,P4,P5给整合到表格中，因为P本身是坐标，2维的。就像这个样子。</p><table><thead><tr><th align="left">P1</th><th align="left">P2</th><th align="left">P3</th><th align="left">P4</th><th align="left">P5</th></tr></thead><tbody><tr><td align="left">(34,45)</td><td align="left">(22,44)</td><td align="left">(27,56)</td><td align="left">(43,56)</td><td align="left">(12,34)</td></tr><tr><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td></tr></tbody></table><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>跟我们处理的一维数据很不一样。首先怎么把P1,P2…组合起来就是个问题。用np.column_stack? 不行，那个函数只能表示一维。我们可以设置新的3D矩阵</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train &#x3D; np.zeros((len(p1),2,5)),</span><br><span class="line">train[:,:,0] &#x3D; P1</span><br><span class="line">train[:,:,1] &#x3D; P2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>同样的道理，得到的 test的三维数据。</p><p>怎么来训练呢。一般的算法都是二维数据，当然也有一些算法是支持多维的，如KNN, decisiontree， extratree(看到有这样说的，但我还没尝试过，KNN试过可以)。</p><h4 id="方法二（reshape"><a href="#方法二（reshape" class="headerlink" title="方法二（reshape)"></a>方法二（reshape)</h4><p>利用reshape函数，把二维变1维, x_coordinate 对应 X_label, y_coordinate 对顶Y_label。转化成1维的情况，就很方便了。利用<code>np.column_stack</code>组合5个model得到的数据集。然后进行有监督训练，之后也可以再处理转化成二维的数组（x_coordinate，y_coordinate）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def reshaped(predict):</span><br><span class="line">    size &#x3D; predict.shape[0]</span><br><span class="line">    j &#x3D; predict.reshape((2*size, 1))</span><br><span class="line">    return j</span><br></pre></td></tr></table></figure><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a>Resources:</h2><p><a href="https://zhuanlan.zhihu.com/p/26890738" target="_blank" rel="noopener">Kaggle机器学习之模型融合（stacking）心得</a></p><p><a href="https://blog.csdn.net/shine19930820/article/details/75209021" target="_blank" rel="noopener">Ensemble Learning-模型融合-Python实现</a></p><p><a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="noopener">KAGGLE ENSEMBLING GUIDE</a></p><p><a href="https://blog.csdn.net/u014114990/article/details/50819948" target="_blank" rel="noopener">总结Kaggle-Ensemble-Guide</a></p><p><a href="https://github.com/emanuele/kaggle_pbr/blob/master/blend.py#L68" target="_blank" rel="noopener">github上一个stacking代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> ensmeble </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine learning with python(4)_classical algorithm(持续跟新)</title>
      <link href="/2018/06/21/Machine-learning-with-python-4-classical-algorithm/"/>
      <url>/2018/06/21/Machine-learning-with-python-4-classical-algorithm/</url>
      
        <content type="html"><![CDATA[<p>系列教程4，主要是几个classical的机器学习算法。</p><p>参考的资源有：<br>吴恩达机器学习课程笔记（需要的可以留下邮箱）<br>Udacity 机器学习工程师<br>以及文中各种blog链接  </p><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>讲的很好的一个<br><a href="https://www.jianshu.com/p/c7e642877b0e" target="_blank" rel="noopener">深入浅出–梯度下降法及其实现</a></p><p>随机梯度和batch梯度的区别：随机梯度更新一个数据，有时不易收敛，收敛结果波动。而batch一次需要样本多，耗时，尽管收敛效果更好。</p><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p><a href="https://www.jianshu.com/p/f1d3906e4a3e" target="_blank" rel="noopener">深入浅出最大似然估计</a></p><p>最大似然估计是利用已知的样本的结果，在使用某个模型的基础上，反推最有可能导致这样结果的模型参数值。</p><p>当这个正态分布的期望为多少时，产生这个采样数据的概率为最大？</p><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>简单来说就是用 y=ax+b 去拟合得到最好的a和b。怎么得到最好的a,b呢？用cost function 去寻找这个最佳的参数。<br>cost function 公式：mean quare error.</p><p>然后求这个cost function 的梯度值，下降到最小的梯度值时，那么就得到了最佳参数。</p><img src="/images/linear regression1.png" width = 50% height = 50% div align=center /><p><a href="https://blog.csdn.net/qq_19645269/article/details/78127785" target="_blank" rel="noopener">公式推导，西瓜书</a><br>这里注意。在推导过程中，很容易遇到矩阵不是满秩矩阵的情况，这个时候需要加上正则化项，L1,L2。具体参考<a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/#Lasso-Ridge">note</a></p><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>事实上，逻辑回归是一个分类算法，之所以这样弄，是因为逻辑回顾应用了线性回归类似的方法，只是增加了一个sigmod层。<br>事实上，logical regression是仅含有一个神经元的单层的神经网络。<br><img src="/images/logical regression.png" width = 90% height = 90% div align=center /></p><p>因为加入sigmod函数，使得结果在0~1之间，把它当成概率，$\phi (z)$ 可以视为类1的后验估计，所以可以写成：<br>$$P(y=1\mid x;w) = \phi(w^{T}x+b)=\phi(z)$$<br>$$P(y=0\mid x;w) =1- \phi(z)$$</p><p>一般形式： $$P(y\mid x;w)=\phi(z)^y(1-\phi(z))^{(1-y)}$$</p><p>极大似然：$$L(w)=\prod_{i=1}^{n}p(y^{i}\mid x^{i};w)=\phi(z)^y(1-\phi(z))^{(1-y)}$$</p><p>然后log取导数，sigmod函数有一个很好的性质$$\phi{(z)}=\phi(z)(1-\phi(z))$$</p><p>求导具体过程参考以下。最后得出更新公式。</p><p>参考：<a href="https://blog.csdn.net/zjuPeco/article/details/77165974" target="_blank" rel="noopener">逻辑回归(logistic regression)的本质——极大似然估计</a><br><a href="https://blog.csdn.net/kejiaming/article/details/64439664" target="_blank" rel="noopener">逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降)</a></p><p>Softmax regression其实是多维的Logistic regression，它其实可以看做是单层多个神经元的神经网络！<br><img src="/images/softmax regression.png" width = 60% height = 60% div align=center /></p><p>参考：<a href="https://blog.csdn.net/tz_zs/article/details/79069499" target="_blank" rel="noopener">逻辑回归和神经网络之间有什么关系？</a></p><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><p>知道先验概率，知道条件概率，然后根据贝叶斯公式，知道后验概率是多少。多用于分类。全概率公式展开 $$[p(y=c_{k}|x)=\frac{\prod_{i=1}^{M}p(x^{i}|y=c_{k})p(y=c_{k})}{\sum_{k}p(y=c_{k})\prod_{i=1}^{M}p(x^{i}|y=c_{k})}]$$</p><p>例子：<a href="https://blog.csdn.net/u013634684/article/details/49669081" target="_blank" rel="noopener">朴素贝叶斯算法之过滤垃圾邮件</a><br>非常好多解释资源，可以帮助你理解贝叶斯，及文本分类。</p><p>有三个延申的贝叶斯：<br>[G]aussian Naive Bayes Classifie](<a href="https://chrisalbon.com/machine_learning/naive_bayes/gaussian_naive_bayes_classifier/" target="_blank" rel="noopener">https://chrisalbon.com/machine_learning/naive_bayes/gaussian_naive_bayes_classifier/</a>), 条件概率的部分变成是高斯分布PDF分布形式。  </p><p><a href="https://chrisalbon.com/machine_learning/naive_bayes/multinomial_naive_bayes_classifier/" target="_blank" rel="noopener">Multinomial Naive Bayes Classifier</a>,当我们数据是离散的数据时，用这个</p><p><a href="https://chrisalbon.com/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier/" target="_blank" rel="noopener">Bernoulli Naive Bayes Classifier</a> The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded).</p><h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><p>一堆数据给你，你怎么分类呢。计算这堆数据的entropy(熵)，计算公式是这样的：$$[entropy = - \sum_{i=1}^{n} p_i log_{2} p_i]$$<br><img src="/images/decision tree.png" width = 100% height = 100% div align=center /></p><p>可从图上看出，画第1条线的时候，我们就计算画什么位置，两边的entropy最小。<br>然后用 Entropy(前)-Entropy(后)=信息增益。信息增益越大，说明分的越正确。</p><p>一个例子：</p><iframe width="754" height="400" src="https://www.youtube.com/embed/3FgJOpKfdY8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>但是用信息增益的算法（ID3算法），缺点是信息增益偏向取值较多的特征。</p><p>C4.5决策树的提出完全是为了解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱，不能够对新样本进行有效的预测。</p><p>具体公式查看下面链接。但是C4.5的缺点是偏向取值较少的特征。</p><p>还有一个基尼指数,CART算法。Cart提出了根据基尼系数划分，同时，它的树限定为二叉树，更容易解释，还能处理连续值。<a href="https://chrisalbon.com/machine_learning/trees_and_forests/decision_tree_classifier/" target="_blank" rel="noopener">参见</a></p><p>Further reading:<br><a href="https://www.cnblogs.com/muzixi/p/6566803.html" target="_blank" rel="noopener">决策树–信息增益，信息增益比，Geni指数的理解</a><br><a href="https://www.cnblogs.com/coder2012/p/4508602.html" target="_blank" rel="noopener">决策树</a></p><p>Random forests: 用决策树容易overfitting, 几次随机选取几列进行训练，最后voting得到最佳的结果。</p><iframe width="754" height="400" src="https://www.youtube.com/embed/n5DhXhcYKcw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>SVM 是为了使分类边界的距离（Margin）最大化, $[M = \frac{2}{\left | x \right |}]$，并且分类的错误率最低。它的loss function 包括两个部分，$classification error + Margin error$. 其中 $margin error = 1/2*||w||^2$。所以这个loss function形式很像是一般的error 加上了一个 L2(Ridge) punishment。两个error是一个tradeoff的过程，想要更好准确率，加大C，如果想要更大的margin，减小C。</p><p>$$[min\frac{1}{2}\left | w \right |^{2}+C\sum_{i=1}^{R}\varepsilon_{i} , s.t.,y_{i}(w^{T}x_{i}+b)\geq 1-\varepsilon_{i},\varepsilon_{i}\geq 0]<br>$$</p><iframe width="754" height="400" src="https://www.youtube.com/embed/nWGVAGXwvGE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><iframe width="754" height="400" src="https://www.youtube.com/embed/dSac8Gfgbok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><iframe width="754" height="400" src="https://www.youtube.com/embed/A1wbrcSYc1c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h3 id="kernal"><a href="#kernal" class="headerlink" title="kernal"></a>kernal</h3><p>在二维的情况下，用一条直线把两边数据分类。如果不可分的时候，通过核函数（Kernel）转换，找到一个超平面（hyper-plane）来分类。<br><img src="/images/SVM_4.webp" width = 70% height = 70% div align=center /><br><img src="/images/SVM_8.webp" width = 40% height = 40% div align=center /><img src="/images/SVM_9.webp" width = 40% height = 40% div align=center /></p><ul><li><p>polynomial kernal: (x,y) —-&gt;(x,y,xy,x^2,y^2)</p></li><li><p>RBF kernal: 移动每一个point到山range, 要想很好的区分出二类，得找到合适mountain weights. 怎么找呢？  </p><img src="/images/RBF1.png" width = 80% height = 80% div align=center /><iframe width="754" height="400" src="https://www.youtube.com/embed/xdkIulxXWfQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>找到之后，把点利用高斯分布 lift 到高维。</p><p>参数$\gamma $，越大越窄；越小越宽：   </p><<iframe width="754" height="400" src="https://www.youtube.com/embed/DctkE8kaWPY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></li></ul><p>讲svm比较好的资源如下：<br>第一版本：Margin方式：<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html" target="_blank" rel="noopener">支持向量机基础</a><br>第二版本：cost function方式：<a href="https://blog.csdn.net/ybdesire/article/details/53915093" target="_blank" rel="noopener">SVM理解与参数选择（kernel和C）</a><br>英文版：<a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" target="_blank" rel="noopener">Understanding Support Vector Machine algorithm from examples (along with code)</a><br>SVM参数选择：<a href="https://blog.csdn.net/bryan__/article/details/51506801" target="_blank" rel="noopener">SVM参数详解</a></p><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>传统KNN很简单，就是通过欧式距离或者曼哈顿距离，计算要分类点a和周边K个点的距离，然后看这K个点里，最多的数据点属于哪个类别，就把该要分类的点a归到那一类去。<a href="https://blog.csdn.net/qq_36330643/article/details/77532161" target="_blank" rel="noopener">最近邻算法原理详解</a><br><img src="/images/KNN.jpg" width = 40% height = 40% div align=center /></p><p>它还有几个延申：  </p><ul><li><a href="https://chrisalbon.com/machine_learning/nearest_neighbors/radius_based_nearest_neighbor_classifier/" target="_blank" rel="noopener">Radius-based Nearest Neighbor classifier</a>, 不是设K值，而是设一个圆周距离，在该圆里，看哪个类别最多。</li><li><a href="https://www.cnblogs.com/bigmonkey/p/7387943.html" target="_blank" rel="noopener">加权KNN</a> 有时候各个特征对于分类的贡献是不相同的。用$1/d$距离的倒数来做权重，越远的对分类越弱。</li></ul><p>为什么K越小，bias越小，variance越大；而K变大，high bias, low variance?</p><h2 id="Kmeans-无监督学习"><a href="#Kmeans-无监督学习" class="headerlink" title="Kmeans 无监督学习"></a>Kmeans 无监督学习</h2><p>Kmeans是无监督学习聚类算法。</p><ul><li>第一步，随机选几个点作为聚类中心C。</li><li>第二步，计算数据点跟聚类中心C的距离，离哪个点近就分到哪个类。</li><li>第三步，重新计算聚类中心点C，C = 整个簇的平均值。</li><li>第四步，重复第二步和第三步，直到没有数据会变化。</li></ul><p>Kmeans很简单，但很容易陷入局部最优化，跟初始点的选取很有关系。所以产生了Kmeans++算法。<br>假如有要分K个类，第一个点跟Kmeans一样也是随机选取，但N+1的点选取确实根据$D(x)$来选取的。<br>相比Kmeans,Kmeans++多了几步：</p><ol><li>从数据随机选取一个样本作为初始聚类中心$C_1$</li><li>首先计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离），用D(x)表示；接着计算每个样本被选为下一个聚类中心的概率$[\frac{D(x)^2}{\sum_x D(x)^2}]$. 最后，按照轮盘法选出下一个聚类中心。</li><li>重复第2步知道选出K个聚类中心</li><li>然后按照经典的K-means算法的第二步到第四步</li></ol><p>参考：<a href="https://www.cnblogs.com/wang2825/articles/8696830.html" target="_blank" rel="noopener">K-means与K-means++</a></p><h2 id="各个算法的优缺点"><a href="#各个算法的优缺点" class="headerlink" title="各个算法的优缺点"></a>各个算法的优缺点</h2><p>算法的优劣主要看它的误差，一般误差有方差和偏差组成。一个模型越复杂，拟合的越好，偏差会变小，但容易造成过拟合。对小数据，选取的高偏差，低方差的模型比选取高方差，低偏差的模型要好，因为后者会发生过拟合（overfiting）。然而，随着你训练集的增长，模型对于原数据的预测能力就越好，偏差就会降低，此时低偏差/高方差的分类器就会渐渐的表现其优势（因为它们有较低的渐近误差），而高偏差分类器这时已经不足以提供准确的模型了。</p><p><a href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/" target="_blank" rel="noopener">Choosing a Machine Learning Classifier</a></p><p><a href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/index.html" target="_blank" rel="noopener">机器学习算法比较</a></p><p><a href="https://blog.csdn.net/wuzqChom/article/details/75091612" target="_blank" rel="noopener">偏差和方差</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 经典算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep learning(3)_调参总结</title>
      <link href="/2018/06/15/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/"/>
      <url>/2018/06/15/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>调参的一些blog</p><p>本文blog主要整理自：（主要用于自己学习，不用做其它）<br><a href="https://blog.csdn.net/dugudaibo/article/details/77366245" target="_blank" rel="noopener">https://blog.csdn.net/dugudaibo/article/details/77366245</a></p><p><a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/7366925.html" target="_blank" rel="noopener">Deep learning网络调参技巧</a></p><p>首先，调参调的是超参数（hyperparameters）。超参数和参数的区别是什么呢？</p><p>参数：就是模型可以根据数据可以自动学习出的变量，应该就是参数。比如，深度学习的权重，偏差等</p><p>超参数：就是用来确定模型的一些参数，超参数不同，模型是不同的(这个模型不同的意思就是有微小的区别，比如假设都是CNN模型，如果层数不同，模型不一样，虽然都是CNN模型哈。)，超参数一般就是根据经验确定的变量。在深度学习中，超参数有：学习速率，迭代次数，层数，每层神经元的个数等等。   from:  <a href="https://blog.csdn.net/UESTC_C2_403/article/details/77428736" target="_blank" rel="noopener">link</a>  </p><h2 id="超参数主要包括："><a href="#超参数主要包括：" class="headerlink" title="超参数主要包括："></a>超参数主要包括：</h2><ol><li>学习率 $\eta$</li></ol><p>1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。    </p><p>不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。</p><ol start="2"><li>正则化参数 $\lambda$</li></ol><p>开始从1.0尝试，超过10的很少见</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regularzation_penalty &#x3D; 0.02</span><br><span class="line">model.add(Dense(256, activation&#x3D;&#39;relu&#39;, kernel_regularizer&#x3D;regularizers.l2(regularzation_penalty)))</span><br></pre></td></tr></table></figure><ol start="3"><li>神经网络的层数 $L$</li></ol><p>先从1层开始试</p><ol start="4"><li>每一个隐层中神经元的个数 $j$</li></ol><p>16 32 128，超过1000的情况比较少见。超过1W的从来没有见过</p><ol start="5"><li><p>学习的回合数Epoch</p></li><li><p>小批量数据 minibatch 的大小</p></li></ol><p>128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。</p><ol start="7"><li><p>输出神经元的编码方式(?不太明白这里)</p></li><li><p>代价函数的选择<br>代价函数有很多，例如, <a href="https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications" target="_blank" rel="noopener">更多cost function</a></p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; &#39;categorical_crossentropy&#39; #cross-entropy,</span><br><span class="line">loss &#x3D; mean_squared_error</span><br><span class="line">loss&#x3D;&#39;binary_crossentropy&#39;</span><br></pre></td></tr></table></figure><ol start="9"><li>权重初始化的方法</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">initilization_method &#x3D; &#39;he_normal&#39; #&#39;random_uniform&#39; ,&#39;random_normal&#39;,&#39;TruncatedNormal&#39; ,&#39;glorot_uniform&#39;, &#39;glorot_nomral&#39;, &#39;he_normal&#39;, &#39;he_uniform&#39;</span><br><span class="line"></span><br><span class="line">model.add(Dense(256, input_dim&#x3D;256, activation&#x3D;&#39;relu&#39;, kernel_initializer&#x3D;initilization_method)</span><br></pre></td></tr></table></figure><ol start="10"><li><p>神经元激活函数的种类<br>指的activation function，具体可以参考<a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Deep-Learning-1/#Train-Optimization">deep learning(1)</a></p></li><li><p>参加训练模型数据的规模</p></li></ol><h2 id="实践经验"><a href="#实践经验" class="headerlink" title="实践经验"></a>实践经验</h2><p>实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。</p><ol><li>建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。</li><li>如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。</li><li>如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。</li></ol><h2 id="自动调参"><a href="#自动调参" class="headerlink" title="自动调参"></a>自动调参</h2><ol><li>Grid Search</li><li>Random Search</li><li>Bayesian Optimization</li></ol><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>按吴恩达老师所说的，梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum）</p><p>这部分数学部分不做陈述，主要是代码应用。<br><a href="https://keras.io/optimizers/#adam" target="_blank" rel="noopener">keras optimizers</a></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><a href="https://www.jianshu.com/p/aebcaf8af76e" target="_blank" rel="noopener">简单认识Adam优化器</a></p><p>随机梯度涉及到这几个参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adam(lr&#x3D;0.001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08,</span><br><span class="line">decay&#x3D;0.0,amsgrad&#x3D;False)</span><br></pre></td></tr></table></figure><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a>Resources:</h2><p><a href="https://blog.csdn.net/roslei/article/details/61916038" target="_blank" rel="noopener">18个技巧实战深度学习，资深研究员的血泪教训</a></p><p><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">optimizer总结</a><br><a href="http://ruder.io/deep-learning-optimization-2017/" target="_blank" rel="noopener">optimization for deep learning highlights in 2017</a></p><p><a href="https://blog.csdn.net/qq_20259459/article/details/70316511" target="_blank" rel="noopener">深度学习 14. 深度学习调参，CNN参数调参，各个参数理解和说明以及调整的要领。underfitting和overfitting的理解，过拟合的解释</a></p><p><a href="https://blog.csdn.net/xiaocong1990/article/details/72585696" target="_blank" rel="noopener">深度学习调参策略1</a><br><a href="https://blog.csdn.net/xiaocong1990/article/details/72585735" target="_blank" rel="noopener">深度学习调参策略2</a><br><a href="https://blog.csdn.net/chenzhi1992/article/details/52905569" target="_blank" rel="noopener">深度学习训练的小技巧，调参经验。总结与记录</a></p><p><a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/7366925.html" target="_blank" rel="noopener">Deep learning网络调参技巧</a></p><p><a href="https://zhuanlan.zhihu.com/easyml" target="_blank" rel="noopener">知乎专栏：炼丹心得</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> 机器学习 </tag>
            
            <tag> 调参 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine learning with python(3)</title>
      <link href="/2018/06/04/machine-learning-with-python-3/"/>
      <url>/2018/06/04/machine-learning-with-python-3/</url>
      
        <content type="html"><![CDATA[<p>系列1主要讲了python环境，软件安装，机器学习比较重要的库，还有python, numpy, pandas, mattplotlib的crash course. <a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-1/">machine learning with python 1</a></p><p>系列2主要讲了数据来了，怎么理解数据。主要是一些统计手段，可视化。<a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-2/">machine learning with python 2</a></p><p>本系列3，主要讲prepare data for machine leanring. 数据塞进机器学习算法前，对data 还需要处理什么。</p><h2 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h2><h3 id="scale优点"><a href="#scale优点" class="headerlink" title="scale优点"></a>scale优点</h3><p>关于data为什么要scale(归一化）。有以下几个原因：</p><ol><li>归一化可以加快梯度下降求最优解的速度；</li><li>归一化有可能提高精度（如KNN等算法）</li></ol><h3 id="why"><a href="#why" class="headerlink" title="why?"></a>why?</h3><ul><li>对于说可以加快速度，可以看这Boston 房屋价格变动的这个例子；（在吴恩达的课程中有很经典的案例，需要达人整理的吴恩达笔记可以留邮箱给我，发给你）<br>下图举了一个boston房价的例子，如果刻度不同，得到的loss function的等高线图，<br><img src="/images/machine-learning-with-python-3-1.png" alt="Boston price cost function"><br>我们看到左边的等高线是狭长的，右边是偏圆形的。在左图中，一个scale是0<del>2000，一个scale是1</del>5。这对于Gredient descent下降速度是很有影响的。</li><li>提高精度。是因为一些算法需要计算距离的时候，如果某一个特征值很大，那么算法就会取决于这个特征值，影响算法精度。scale可以避免这种异常值的出现。</li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ol><li>概率模型不需要归一化，因为模型不关心变量的取值，只关心变量的分布和变量的条件概率；</li><li>SVM、线性回归之类的最优化问题需要归一化，是否归一化主要在于是否关心变量取值；</li><li>神经网络需要标准化处理，一般变量的取值在-1到1之间，这样做是为了弱化某些变量的值较大而对模型产生影响。</li><li>在K近邻算法中，如果不对解释变量进行标准化，那么具有小数量级的解释变量的影响就会微乎其微。</li></ol><ul><li>From:<a href="https://blog.csdn.net/zenghaitao0128/article/details/78361038" target="_blank" rel="noopener">https://blog.csdn.net/zenghaitao0128/article/details/78361038</a></li></ul><h3 id="几种scale的方法"><a href="#几种scale的方法" class="headerlink" title="几种scale的方法"></a>几种scale的方法</h3><ul><li><p>MinMaxScaler<br>$$[x^{‘}= \frac{x-min(x)}{max(x)-min(x)}]$$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; MinMaxScaler(feature_range&#x3D;(0, 1))</span><br><span class="line">rescaledX &#x3D; scaler.fit_transform(X)</span><br><span class="line"># summarize transformed, 输出位数小数点后3位</span><br><span class="line">data set_printoptions(precision&#x3D;3)</span><br></pre></td></tr></table></figure></li><li><p>StandardScaler<br>$$[x^{‘}= \frac{x-u}{\sigma }]$$<br>$[u]$是平均值，$[\sigma]$是标准差，经过处理后，数据符合标准正态分布。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; StandardScaler().fit(X)</span><br><span class="line">rescaledX &#x3D; scaler.transform(X)</span><br></pre></td></tr></table></figure></li><li><p>Normalizer（范化）<br>主要有两种范化 L1, L2， sklearn中默认是L2<br>$$x^{‘}=\frac{x}{\left | x \right |}$$<br>说一下做范化的好处：适合在稀疏的数据中应用，比如1000维，只有几维是是非0的。这时候可以应用范化。 这里分母是||x||=square(x1^2+x2^2+…)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; Normalizer().fit(X)</span><br><span class="line">normalizedX &#x3D; scaler.transform(X)</span><br></pre></td></tr></table></figure></li><li><p>Binarize<br>有一个阈值（threshold)，如果有值超过这个值赋1，小于该值为0.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">binarizer &#x3D; Binarizer(threshold&#x3D;0.0).fit(X)</span><br><span class="line">binaryX &#x3D; binarizer.transform(X)</span><br></pre></td></tr></table></figure></li></ul><p>Mark下: 这几种scale方法，前两个比较好理解，但范化不是很理解，为什么能应用到稀疏的数据中，怎么应用，有待进一步查资料。</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>这个地方<strong>mark</strong>一下 ，数学公式比较多。短期内还没法一下全部搞懂。我先把大概给弄上来。</p><blockquote><p>数据决定了机器学习的上限，而算法只是尽可能逼近这个上限</p></blockquote><p>这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括<strong>特征构建、特征提取、特征选择</strong>三个部分。</p><p>特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p><p>reference: <a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a></p><p>这张图特别好，来自：<a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibao/p/6252280.html</a></p><p><img src="/images/machine-learning-with-python-3-2.png" alt="一图概览特征工程"></p><p>补充：关于维度灾难的补充：<a href="https://www.jianshu.com/p/d7aec8b41356" target="_blank" rel="noopener">https://www.jianshu.com/p/d7aec8b41356</a><br>有时候维度太多，会造成维度灾难，具体看链接。所以进行降维很有必要。</p><h3 id="特征构建"><a href="#特征构建" class="headerlink" title="特征构建"></a>特征构建</h3><p>特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。除此之外，属性分割和结合是特征构建时常使用的方法。结构性的表格数据，可以尝试组合二个、三个不同的属性构造新的特征，如果存在时间相关属性，可以划出不同的时间窗口，得到同一属性在不同时间下的特征值，也可以把一个属性分解或切分，例如将数据中的日期字段按照季度和周期后者一天的上午、下午和晚上去构建特征。总之特征构建是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。From:<a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a></p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p><ul><li>特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li><li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li></ul><p>根据特征选择的形式又可以将特征选择方法分为3种：</p><ol><li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li><li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li></ol><p>我们使用sklearn中的feature_selection库来进行特征选择</p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><h5 id="皮尔森相关系数法-Pearson-Correlation"><a href="#皮尔森相关系数法-Pearson-Correlation" class="headerlink" title="皮尔森相关系数法(Pearson Correlation)"></a>皮尔森相关系数法(Pearson Correlation)</h5><h5 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h5><h5 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h5><h5 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h5><h4 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h4><h5 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h5><h4 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h4><h5 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h5><h5 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h5><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><h4 id="主成分分析法（PCA）"><a href="#主成分分析法（PCA）" class="headerlink" title="主成分分析法（PCA）"></a>主成分分析法（PCA）</h4><p>分析PCA很好的文章：<a href="https://www.cnblogs.com/hadoop2015/p/7419087.html" target="_blank" rel="noopener">https://www.cnblogs.com/hadoop2015/p/7419087.html</a><br>PCA是从特征的角度协方差角度： 求出协方差矩阵的特征值和特征向量，然后将特征向量按特征值的大小排序取出前K行组成矩阵P（这个P就是我们对角化协方差矩阵的时所使用的P, 具体的可以看看矩阵对角化的过程）， 这个P就是一组正交变化基， 然后将原始的矩阵X，左乘P，也就是将X变换到P组成的正交基中，然后PX＝Y就是降维后的矩阵。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; array[:,0:8]</span><br><span class="line">Y &#x3D; array[:,8]</span><br><span class="line"># feature extraction</span><br><span class="line">pca &#x3D; PCA(n_components&#x3D;3)</span><br><span class="line">fit &#x3D; pca.fit(X)</span><br><span class="line">x_new &#x3D; fit.transform(x)</span><br></pre></td></tr></table></figure><h4 id="线性判别分析法（LDA）"><a href="#线性判别分析法（LDA）" class="headerlink" title="线性判别分析法（LDA）"></a>线性判别分析法（LDA）</h4><p>LDA则是在已知样本的类标注， 希望投影到新的基后使得不同的类别之间的数据点的距离更大，同一类别的数据点更紧凑。</p><blockquote><p>特征工程更多参考：<br><a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibao/p/6252280.html</a><br><a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">https://www.cnblogs.com/wxquare/p/5484636.html</a><br><a href="http://dataunion.org/14072.html" target="_blank" rel="noopener">http://dataunion.org/14072.html</a><br><a href="https://blog.csdn.net/Dream_angel_Z/article/details/49388733" target="_blank" rel="noopener">https://blog.csdn.net/Dream_angel_Z/article/details/49388733</a></p></blockquote><h2 id="数据集的重新划分"><a href="#数据集的重新划分" class="headerlink" title="数据集的重新划分"></a>数据集的重新划分</h2><h3 id="KFold"><a href="#KFold" class="headerlink" title="KFold"></a>KFold</h3><p><a href="https://blog.csdn.net/FontThrone/article/details/79220127" target="_blank" rel="noopener">Sklearn中的CV与KFold详解</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#Stratified k-fold:实现了分层交叉切分*</span><br><span class="line">from sklearn.model_selection import StratifiedKFold</span><br><span class="line">X &#x3D; np.array([[1, 2, 3, 4],</span><br><span class="line">              [11, 12, 13, 14],</span><br><span class="line">              [21, 22, 23, 24],</span><br><span class="line">              [31, 32, 33, 34],</span><br><span class="line">              [41, 42, 43, 44],</span><br><span class="line">              [51, 52, 53, 54],</span><br><span class="line">              [61, 62, 63, 64],</span><br><span class="line">              [71, 72, 73, 74]])</span><br><span class="line"></span><br><span class="line">y &#x3D; np.array([1, 1, 0, 0, 1, 1, 0, 0])</span><br><span class="line"></span><br><span class="line">stratified_folder &#x3D; StratifiedKFold(n_splits&#x3D;4, random_state&#x3D;0, shuffle&#x3D;False)</span><br><span class="line">for train_index, test_index in stratified_folder.split(X, y):</span><br><span class="line">    print(&quot;Stratified Train Index:&quot;, train_index)</span><br><span class="line">    print(&quot;Stratified Test Index:&quot;, test_index)</span><br><span class="line">    print(&quot;Stratified y_train:&quot;, y[train_index])</span><br><span class="line">    print(&quot;Stratified y_test:&quot;, y[test_index],&#39;\n&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#结果：</span><br><span class="line">Stratified Train Index: [1 3 4 5 6 7]</span><br><span class="line">Stratified Test Index: [0 2]</span><br><span class="line">Stratified y_train: [1 0 1 1 0 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 2 4 5 6 7]</span><br><span class="line">Stratified Test Index: [1 3]</span><br><span class="line">Stratified y_train: [1 0 1 1 0 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 1 2 3 5 7]</span><br><span class="line">Stratified Test Index: [4 6]</span><br><span class="line">Stratified y_train: [1 1 0 0 1 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 1 2 3 4 6]</span><br><span class="line">Stratified Test Index: [5 7]</span><br><span class="line">Stratified y_train: [1 1 0 0 1 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning(2)</title>
      <link href="/2018/05/29/Deep-Learning-2/"/>
      <url>/2018/05/29/Deep-Learning-2/</url>
      
        <content type="html"><![CDATA[<p>First Introduces the Keras, and then give a summary of different architecture of Deep learning, such as CNN.</p><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><p>why use keras? easy, support tensorflow, theano, CNTK.</p><p>How to build a Neural Network in Keras? There are some core concepts need know.</p><h3 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h3><p>The <a href="https://keras.io/models/sequential/" target="_blank" rel="noopener">keras.models.Sequential</a> class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like compile(), fit(), and evaluate() that are used to train and run the model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">#Create the Sequential model</span><br><span class="line">model &#x3D; Sequential()</span><br></pre></td></tr></table></figure><h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><p>The Keras Layer class provides a common interface for a variety of standard neural network layers. There are fully connected layers, max pool layers, activation layers, and more. You can add a layer to a model using the model’s add() method. For example, a simple model with a single hidden layer might look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers.core import Dense, Activation</span><br><span class="line"></span><br><span class="line"># X has shape (num_rows, num_cols), where the training data are stored</span><br><span class="line"># as row vectors</span><br><span class="line">X &#x3D; np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line"># y must have an output vector for each input vector</span><br><span class="line">y &#x3D; np.array([[0], [0], [0], [1]], dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line"># Create the Sequential model</span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line"></span><br><span class="line"># 1st Layer - Add an input layer of 32 nodes with the same input shape as</span><br><span class="line"># the training samples in X</span><br><span class="line">model.add(Dense(32, input_dim&#x3D;X.shape[1]))</span><br><span class="line"></span><br><span class="line"># Add a softmax activation layer</span><br><span class="line">model.add(Activation(&#39;softmax&#39;))</span><br><span class="line"></span><br><span class="line"># 2nd Layer - Add a fully connected output layer</span><br><span class="line">model.add(Dense(1))</span><br><span class="line"></span><br><span class="line"># Add a sigmoid activation layer</span><br><span class="line">model.add(Activation(&#39;sigmoid&#39;))</span><br></pre></td></tr></table></figure><p>More layers decription we can check <a href="https://keras.io/layers/core/" target="_blank" rel="noopener">Keras Layers</a></p><h4 id="Dense"><a href="#Dense" class="headerlink" title="Dense"></a>Dense</h4><p>A dense layer represents a matrix vector multiplication, is used to change the dimensions of your vector.<br>Further Reading: <a href="https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer" target="_blank" rel="noopener">https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer</a></p><h4 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h4><p>Commonly, used in CNN. It transforms your tridmenesional tensor into a monodimensional tensor.<br>Furthere Reading: <a href="https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network" target="_blank" rel="noopener">https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network</a></p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout is a regularization technique, which aims to reduce the complexity of the model with the goal to prevent overfitting.<br>Further Reading: <a href="https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer" target="_blank" rel="noopener">https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer</a></p><h3 id="compile-model"><a href="#compile-model" class="headerlink" title="compile model"></a>compile model</h3><p>Once we have our model built, we need to compile it before it can be run. Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss&#x3D;&quot;categorical_crossentropy&quot;, optimizer&#x3D;&quot;adam&quot;, metrics &#x3D; [&quot;accuracy&quot;])</span><br></pre></td></tr></table></figure><p>We can see the resulting model architecture with the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h3 id="Fit-Model"><a href="#Fit-Model" class="headerlink" title="Fit Model"></a>Fit Model</h3><p>We have deﬁned our model and compiled it ready for eﬃcient computation. Now it is time to execute the model on some data. We can train or ﬁt our model on our loaded data by calling the fit() function on the model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Fit the model</span><br><span class="line">model.fit(X, Y, epochs&#x3D;150, batch_size&#x3D;10)</span><br></pre></td></tr></table></figure><h3 id="Evaluate-model"><a href="#Evaluate-model" class="headerlink" title="Evaluate model"></a>Evaluate model</h3><p>We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same dataset. We use the evaluate function. It returns the loss value &amp; metrics values for the model in test mode.<br>The attribute <em>model.metrics_names</em> will give you the display labels for the scalar outputs.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># evaluate the model</span><br><span class="line">scores &#x3D; model.evaluate(X, Y)</span><br><span class="line">print(&quot;\n%s: %.2f%%&quot; % (model.metrics_names[1], scores[1]*100))</span><br></pre></td></tr></table></figure><h3 id="Predict-Model"><a href="#Predict-Model" class="headerlink" title="Predict Model"></a>Predict Model</h3><p><a href="https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/" target="_blank" rel="noopener">How to Make Predictions with Keras</a><br>This tutorial is great for different kind of prediction, no matter class predict, or regression predict.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(self, x, batch_size&#x3D;None, verbose&#x3D;0, steps&#x3D;None)</span><br></pre></td></tr></table></figure><p>Generates output predictions for the input samples.</p><p>Further Reading: <a href="https://keras.io/models/sequential/" target="_blank" rel="noopener">Keras Documentation</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_classes(x)</span><br></pre></td></tr></table></figure><h2 id="MLP-Multi-Layer-Perceptron"><a href="#MLP-Multi-Layer-Perceptron" class="headerlink" title="MLP(Multi Layer Perceptron)"></a>MLP(Multi Layer Perceptron)</h2><p>The MLP is full connected layers.<br>Todo</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>Todo: descirption</p><h3 id="CNN-in-keras"><a href="#CNN-in-keras" class="headerlink" title="CNN in keras"></a>CNN in keras</h3><p>There are three types of layers in a Convolutional Neural Network:</p><ol><li><p>Convolutional Layers.</p><ul><li>Padding<img src="/images/deeplearning_padding.jpg" width = 70% height = 70% div align=center />[Deep learning.ai--Padding](https://www.coursera.org/lecture/convolutional-neural-networks/padding-o7CWi)</li></ul></li></ol><ol start="2"><li><p>Pooling Layers.</p><iframe width="700" height="380" src="https://www.youtube.com/embed/OkkIZNs7Cyc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></li><li><p>Fully-Connected Layers.</p></li></ol><p>output_size =1+ (input_size+2*padding-kernel_size)/stride</p><p>Further Reading: <a href="https://keras.io/layers/convolutional/#conv2d" target="_blank" rel="noopener">Keras Documentation</a></p><h3 id="1D-2D-3D-CNN-difference"><a href="#1D-2D-3D-CNN-difference" class="headerlink" title="1D,2D,3D CNN difference"></a>1D,2D,3D CNN difference</h3><p>Further Reading: <a href="https://www.cnblogs.com/szxspark/p/8445406.html" target="_blank" rel="noopener">Chinese version</a>,<br><a href="https://stackoverflow.com/questions/42883547/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn" target="_blank" rel="noopener">English Version</a></p><h2 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h2><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><blockquote><p>Announcement: Most of the content I summaried is from Keras documentation, the blog <a href="https://machinelearningmastery.com/deep-learning-with-python/" target="_blank" rel="noopener">Deep learning</a> and Udacity machine learning course.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning(1)</title>
      <link href="/2018/05/27/Deep-Learning-1/"/>
      <url>/2018/05/27/Deep-Learning-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>neural networks are a class of machine learning algorithms to model complex patterns in datasets using multiple hidden layers and non-linear activation functions.</p><p>It contains an input, passes it through multiple layers of hidden neurons(deep learning contains much more layers), and outputs a prediction representing the combined input of all the neurons.</p><p>![1](/images/neural network1.png)</p><p>Neural networks are trained iteratively using optimization techniques like <strong>gradient descent</strong>. After every training, an error metric is calculated based on the difference between prediction and target.</p><p>the derivatives of this error metric are calculated and propagated back through the network using a technique called <strong>backpropagation</strong>. Each neuron’s weights are the adjusted relative to how much they contributed to the total error. This process is repeated iteratively until the network error drops below an acceptable threshold.</p><h3 id="Perceptrons"><a href="#Perceptrons" class="headerlink" title="Perceptrons"></a>Perceptrons</h3><p>A Perceptron takes a group of weighted inputs, if the date points is in right side, then go through the activation function, output the classification.</p><p>![4](/images/neural network4.png)</p><p>![5](/images/neural network5.png)</p><h3 id="Neural-network-and-Linear-regression"><a href="#Neural-network-and-Linear-regression" class="headerlink" title="Neural network and Linear regression"></a>Neural network and Linear regression</h3><p>It is very simple to classify the data in the figure 2 below. we just need to find the line that can best classify them. Maybe, if the data points are more chaos, we adjust need more lines to help classify. However, this is not the difference between NN and LR. NN can directly classify the Non-linear regions, how we do this, we use the activation function. Discuss below.<br>![2](/images/neural network2.png)<br>![3](/images/neural network3.png)</p><h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><p>What is activation functions?<br>Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer. Popular activation functions include <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" target="_blank" rel="noopener">relu</a> and <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" target="_blank" rel="noopener">sigmoid</a>.<br>Sigmoid almost give up by most deep learning architecture. relu is very popular now.<br><a href="">softmax</a> is very important when output the multiple classes.</p><p>Why we use activation function?</p><ul><li><p><strong>Non-linear</strong> as we say above, for non-linear dataset(e.g. x^2, sin, log), if we use multiple lines to classify, it is linear regression, not neural network. Activation functions model the dataset relationships that we need a non-linear prediction equation.</p></li><li><p><strong>Continuously differentiable</strong> – do you remember how we adjust the weights of the networks we talked above. We use the gradient descent, gradient descent need the output to have a nice slope so we can compute error derivatives with respect to weights. if the output is discrete values, we can’t proceed.</p></li><li><p><strong>Fixed range</strong> – activation function typically squash the input data into a narrow range that makes training the model more stable and efficient.</p></li></ul><p>Why we don’t use sigmoid function nowadays? That’s because after multiple layers, the gradient is vanishing. We will never get the lowest error. So the relu function and its <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu" target="_blank" rel="noopener">variants</a> become popular.</p><p>reference: <a href="https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f" target="_blank" rel="noopener">1</a>, <a href="http://www.sohu.com/a/145367458_468740" target="_blank" rel="noopener">2</a>, <a href="http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#activation-functions" target="_blank" rel="noopener">3</a></p><p>Found great resources: <a href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" target="_blank" rel="noopener">Choosing the right Activation Function</a></p><p><em>Add 12/21/2018</em></p><p>sigmoid function<br>$$ $$</p><p>tanh function</p><p>relu function</p><p>when we want to the dW = dJ/dW</p><h3 id="Error-function-loss-function"><a href="#Error-function-loss-function" class="headerlink" title="Error function(loss function)"></a>Error function(loss function)</h3><p>A loss function, or cost function, is a wrapper around our model’s predict function that tells us “how good” the model is at making predictions for a given set of parameters. The loss function has its own curve and its own derivatives. The slope of this curve tells us how to change our parameters to make the model more accurate! We use the model to make predictions. We use the cost function to update our parameters. Our cost function can take a variety of forms as there are many different cost functions available. Popular loss functions include: MSE (L2) and Cross-entropy Loss. <a href="http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#loss-functions" target="_blank" rel="noopener">from here</a></p><h4 id="Cross-entropy"><a href="#Cross-entropy" class="headerlink" title="Cross-entropy"></a>Cross-entropy</h4><p>First, we need take a look at maximum likelihood. 4 points in the figure. Take two classify lines as an example. Multiply each data point’s probability, and find which lines can get the maximum likelihood.<br>![6](/images/neural network6.png)  </p><p>However, the probability is so small, if we have lots of data, the probability will be near 0. So how can we change it to make more easy to distinguish. we will use the log function.   $-log(p)$<br>For binary classification, the cross-entropy can be calculated as:<br>$$[-(ylog(p)+(1-y)log(1-p))]$$<br>For multiple classification, we calculate a separate loss for each class label per observation and sum the result<br>$$-sum_{i=1}^{n} \sum_{j=1}^{m}y_{ij}ln(p_{ij})$$</p><p>Last problem, how we maximize the probabilities, which means minimize the cross entropy.<br>In order to minimize the error function, we need to take some derivatives. $[\frac{\partial E}{\partial w_j}]$, $[\frac{\partial E}{\partial b}]$</p><p><strong>Gradient Descent Step</strong> Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:<br>$${w_j}’\leftarrow w_j - a\left [ \frac{\partial E}{\partial w_j}\right ]$$<br>similarly, updates the bias:<br>$${b}’\leftarrow b - a\left [ \frac{\partial E}{\partial b}\right ]$$<br>a is the learning rate, is a constant.</p><h3 id="Nerual-Network-Architecture"><a href="#Nerual-Network-Architecture" class="headerlink" title="Nerual Network Architecture"></a>Nerual Network Architecture</h3><p>How we combine several perceptrons into a third, more complicated one?<br>As figure 3, two lines help to classify the dataset. Simply as this and add the function of activation to change this linear to nonlinear classification.<br>![7](/images/neural network7.png)</p><p>More detail:   </p><iframe width="854" height="480" src="https://www.youtube.com/embed/Boy3zHVrWB4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>For deep learning, it contains more layers and more nodes than we discuss above.<br>![8](/images/neural network8.png)![9](/images/neural network9.png)</p><h3 id="Feedforward-and-Backpropagation"><a href="#Feedforward-and-Backpropagation" class="headerlink" title="Feedforward and Backpropagation"></a>Feedforward and Backpropagation</h3><h3 id="Train-Optimization"><a href="#Train-Optimization" class="headerlink" title="Train Optimization"></a>Train Optimization</h3><p>Why we use Optimization?<br>there are many things that can fail when train out model. For example, our architecture can be poorly chosen, our data can be noisy and our model could maybe taking years to run.</p><h4 id="Epoch-and-Batch"><a href="#Epoch-and-Batch" class="headerlink" title="Epoch and Batch"></a>Epoch and Batch</h4><p>One epoch means all data go one iterations, batch means divide the dataset into several parts.<br>For example, if I have 1000 data points, the batach_size is 100, then I need 10 times iterations to accomplish 1 times epoch.<br>Why we will have batch size, because train all dataset one time maybe time consuming and lay burden on computer. But it not means increasing batch_size is a good idea. Choose a appropriate paraments is a very important part in deep learning.<br>The same with choose epoch parament.</p><h4 id="Overfitting-and-Underfitting"><a href="#Overfitting-and-Underfitting" class="headerlink" title="Overfitting and Underfitting"></a>Overfitting and Underfitting</h4><p>overfitting is you train too much, gain high variance. Underfitting verse is you train less, gain high bias.<br>![10](/images/neural network10.png)</p><p>Overfitting and Underfitting is a tradeoff, we can draw the error by model complexity graph.<br>![11](/images/neural network11.png)<br>In this graph, we should choose appropriate number of epochs, in case our test error decrease at first then go up again.</p><h4 id="Earlystoppoing"><a href="#Earlystoppoing" class="headerlink" title="Earlystoppoing"></a>Earlystoppoing</h4><p>We can see from last figure, there is a lowest point of val_loss, we should stop at that point. You can see the documentation in <a href="https://keras.io/callback/" target="_blank" rel="noopener">keras document</a>, they use val_loss as monitor to see if the loss is increasing or decreasing. Patience is the number of epochs with no improvement after which training will be stopped.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#in keras</span><br><span class="line">earlyStopping&#x3D;keras.callbacks.EarlyStopping(monitor&#x3D;&#39;val_loss&#39;, patience&#x3D;60, verbose&#x3D;0, mode&#x3D;&#39;auto&#39;)</span><br></pre></td></tr></table></figure><h4 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h4><p>from the figure 17, we see that if the step keep the same, it’s hard to get the lowest point. So, he rule of thumb, if you model is not working, decrease the learning rate. The rule: if steep, take long steps; if plain, small step.<br>![17](/images/neural network17.png)</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>Think of the question?  split the two points.<br>![12](/images/neural network12.png)<br>As we talked above, the probability of the points will say that solution 2 will have smaller error.<br>![13](/images/neural network13.png)<br>However, the smaller error is not good for gradient descent(too sure), just as BertrAIND Russell said:</p><blockquote><p>The whole problem with artificial intelligence is that bad models are so certain of themselves, and good models so full of doubts.</p></blockquote><p>![14](/images/neural network14.png)</p><p><em>Large coefficients will led to overfitting, so how we deal with it?</em> We will <strong>punish the big coefficients</strong>. This method is called <strong>Regularization</strong>.<br>There are two ways: L1, L2.<br>L1 tends to end up with sparse vectors. That means small weights will tend to go to zero. So if we want to reduce the number of weights and end up with a small set. L1 is also good for feature selection, when there are hundreds of features, L1 can help us select which ones are important.<br>L2 on the other hand, it tries to maintain all the weights homogeneously small. This one normally better for training models.<br>![15](/images/neural network15.png)</p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>This is method that in the process of training, no all the nodes are joined at one time.<br>This is a method can prevent overfitting.</p><h4 id="Local-Minima-Momentum"><a href="#Local-Minima-Momentum" class="headerlink" title="Local Minima(Momentum)"></a>Local Minima(Momentum)</h4><p>When we do gradient descent, it sometimes will stuck in local minima. How to solve this problem.</p><p>One is to randomly start. It starts from a few different random places and do gradient descend form all of them. This will increse the probability that we will get to the minimum, at least good local minimum.</p><p>Another is called momentum, just as shown below, the step happened long before will matter less than the ones that happened recently, it will gets us over the hump.<br>![16](/images/neural network16.png)</p><p>There are lots of optimizers, it is very important to help us find the smallest error and get a good model.<br>Check this <a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">blog</a>, it introduces lots of optimizers.<br>And the optimizers in <a href="https://keras.io/optimizers/" target="_blank" rel="noopener">keras</a>.</p><p>To be continue…</p><blockquote><p>Announcment: Most of the content I summary are from Udacity NanoDegree Class–Machine Learning and this cheatsheet website,<a href="http://ml-cheatsheet.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Machine Learning Cheatsheet</a>.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine learning with python(2)</title>
      <link href="/2018/05/18/machine-learning-with-python-2/"/>
      <url>/2018/05/18/machine-learning-with-python-2/</url>
      
        <content type="html"><![CDATA[<p>系列整理第2篇，主要包括load data的三种方式，几种理解数据的统计手段，可视化数据。</p><h2 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h2><p>一般机器学习处理的数据是csv的格式，一般包括file header, comments(用#表示)，delimiter(即逗号)，quotes(数值有引用时，用双引号)</p><h3 id="Load-csv-with-python"><a href="#Load-csv-with-python" class="headerlink" title="Load csv with python"></a>Load csv with python</h3><p>load出来的数据先变成list,再转成array，输出给sklearn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Load CSV Using Python Standard Library</span><br><span class="line">import csv</span><br><span class="line">import numpy</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">raw_data &#x3D; open(filename, &#39;rt&#39;)</span><br><span class="line">reader &#x3D; csv.reader(raw_data, delimiter&#x3D;&#39;,&#39;, quoting&#x3D;csv.QUOTE_NONE)</span><br><span class="line">x &#x3D; list(reader)</span><br><span class="line">data &#x3D; numpy.array(x).astype(&#39;float&#39;)</span><br><span class="line">print(data.shape)</span><br><span class="line"># out</span><br><span class="line">(768, 9)</span><br></pre></td></tr></table></figure><h3 id="Load-csv-with-numpy"><a href="#Load-csv-with-numpy" class="headerlink" title="Load csv with numpy"></a>Load csv with numpy</h3><p>用numpy.loadtxt(),出来的直接是array，但load出来的数据是没有header row</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Load CSV using NumPy</span><br><span class="line">from numpy import loadtxt</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">raw_data &#x3D; open(filename, &#39;rt&#39;)</span><br><span class="line">data &#x3D; loadtxt(raw_data, delimiter&#x3D;&quot;,&quot;)</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Load CSV from URL using NumPy</span><br><span class="line">from numpy import loadtxt</span><br><span class="line">from urllib.request import urlopen</span><br><span class="line">url &#x3D; &#39;https:&#x2F;&#x2F;goo.gl&#x2F;bDdBiA&#39;</span><br><span class="line">raw_data &#x3D; urlopen(url)</span><br><span class="line">dataset &#x3D; loadtxt(raw_data, delimiter&#x3D;&quot;,&quot;)</span><br><span class="line">print(dataset.shape)</span><br></pre></td></tr></table></figure><h3 id="Load-csv-with-pandas"><a href="#Load-csv-with-pandas" class="headerlink" title="Load csv with pandas"></a>Load csv with pandas</h3><p>终于说到最常用的load data的方式，1是因为简单，2是因为方便后续的数据理解可视化。直接用 pandas.read_csv()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line"># Load CSV using Pandas</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names) # 把names&#x3D;col_label赋值给数据</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure><h2 id="理解你的数据with统计描述"><a href="#理解你的数据with统计描述" class="headerlink" title="理解你的数据with统计描述"></a>理解你的数据with统计描述</h2><h3 id="数据一瞥"><a href="#数据一瞥" class="headerlink" title="数据一瞥"></a>数据一瞥</h3><p>利用data.head(), default 是5行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># View first 20 rows</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">peek &#x3D; data.head(20) #20行</span><br><span class="line">print(peek)</span><br></pre></td></tr></table></figure><h3 id="数据维度"><a href="#数据维度" class="headerlink" title="数据维度"></a>数据维度</h3><p>用data.shape</p><h3 id="每个属性的数据类型"><a href="#每个属性的数据类型" class="headerlink" title="每个属性的数据类型"></a>每个属性的数据类型</h3><p>用 date.dtype</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Data Types for Each Attribute</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names) types &#x3D; data.dtypes print(types)</span><br><span class="line"># out</span><br><span class="line">preg int64</span><br><span class="line">plas int64</span><br><span class="line">pres int64</span><br><span class="line">skin int64</span><br><span class="line">test int64</span><br><span class="line">mass float64</span><br><span class="line">pedi float64</span><br><span class="line">age int64</span><br><span class="line">class int64</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><h3 id="统计描述"><a href="#统计描述" class="headerlink" title="统计描述"></a>统计描述</h3><p>用data.describe()<br>描述了8个统计属性：</p><ul><li>count(计数),</li><li>mean(平均数)，</li><li>Standard Deviation(标准差),</li><li>Minimum Value(最小值),</li><li>25th Percentile(第25百分位),</li><li>50th Percentile (Median中位数).,</li><li>75th Percentile(第75百分位),</li><li>Maximum Value(最大数)</li></ul><h3 id="Class-Distribution-类分布"><a href="#Class-Distribution-类分布" class="headerlink" title="Class Distribution(类分布)"></a>Class Distribution(类分布)</h3><p>有一些属性的值可以明显的分为几类（例如0，1），可以通过如下的方法快速的知道属性的class分布情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Class Distribution</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">class_counts &#x3D; data.groupby(&#39;class&#39;).size()</span><br><span class="line">print(class_counts)</span><br><span class="line"># out 看出class 那一列可以分为0，1类别，各有500和268行</span><br><span class="line">class</span><br><span class="line">0 500</span><br><span class="line">1 268</span><br></pre></td></tr></table></figure><h3 id="属性之间相关性-correlation"><a href="#属性之间相关性-correlation" class="headerlink" title="属性之间相关性(correlation)"></a>属性之间相关性(correlation)</h3><p>相关性是指两个变量之间的相关性。<br>一般使用 <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" target="_blank" rel="noopener">Pearson’s Correlation coefficient</a>, 假设属性是正太分布的（这里不是很懂，待查）。<br>0 表示没有相关性，1 和 -1 表示完全相关和负相关。<br>在机器学习中，弱相关性对于机器学习训练结果好，而强相关性很不利于线性和逻辑回归等算法。有可能去掉其中其中属性列再训练。<br>利用Pandas DataFrame中data.corr() 可以直接计算相关系数矩阵。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Pairwise Pearson correlations</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from pandas import set_option</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">set_option(&#39;display.width&#39;, 100)  #屏幕横向最多显示100个字符</span><br><span class="line">set_option(&#39;precision&#39;, 3) #小数点后显示3位</span><br><span class="line">correlations &#x3D; data.corr(method&#x3D;&#39;pearson&#39;)</span><br><span class="line">print(correlations)</span><br></pre></td></tr></table></figure><p><a href="https://www.cnblogs.com/yesuuu/p/6100714.html" target="_blank" rel="noopener">set_option 函数可以参考</a></p><p><img src="/images/%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9F%A9%E9%98%B5.png" alt="得到的相关性矩阵"></p><h3 id="Skew-of-Univariate-Distribution-偏态分布"><a href="#Skew-of-Univariate-Distribution-偏态分布" class="headerlink" title="Skew of Univariate Distribution(偏态分布)"></a>Skew of Univariate Distribution(偏态分布)</h3><p>很多机器学习算法假设数据变量是高斯分布的。但有一些变量分布倾斜或者squash在一个方向等(中文叫<a href="https://baike.baidu.com/item/%E5%81%8F%E6%80%81%E7%B3%BB%E6%95%B0/10793795?fr=aladdin" target="_blank" rel="noopener">偏态分布</a>)，所以可以纠正它的倾斜性从而提高模型准确度，一般尝试取对数，缩小差异。</p><p>0表示不偏，越远离0表示越偏斜 中位数和平均数之差的三次方 除以 标准差三次方</p><p>通过Pandas dataframe中函数 skew()知道分布</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Skew for each attribute</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">skew &#x3D; data.skew() print(skew)</span><br><span class="line"># out</span><br><span class="line">preg 0.901674</span><br><span class="line">plas 0.173754</span><br><span class="line">pres -1.843608</span><br><span class="line">skin 0.109372</span><br><span class="line">test 2.272251</span><br><span class="line">mass -0.428982</span><br><span class="line">pedi 1.919911</span><br><span class="line">age 1.129597</span><br><span class="line">class 0.635017</span><br></pre></td></tr></table></figure><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><ul><li>查看数据。统计描述并不是全部，停下来看看数据，想一想。</li><li>ask why. 问一些和主题相关的问题</li><li>写下想到ideas，以及后面可以尝试的想法</li></ul><h2 id="更懂你的数据-可视化"><a href="#更懂你的数据-可视化" class="headerlink" title="更懂你的数据-可视化"></a>更懂你的数据-可视化</h2><h3 id="Univariate-plots-单变量画图"><a href="#Univariate-plots-单变量画图" class="headerlink" title="Univariate plots(单变量画图)"></a>Univariate plots(单变量画图)</h3><h4 id="Histograms-直方图"><a href="#Histograms-直方图" class="headerlink" title="Histograms(直方图)"></a>Histograms(直方图)</h4><p>最快的方式可以让你看到每个变量的分布情况。y轴是count。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.hist()</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>![Histograms of each attribute](/images/Histograms of each attribute.png)</p><p>可以看出大概他们的分布，例如age,pedi,preg,test他们大概是指数分布，而mass,pres，plas可能是高斯分布。class 一下可以分为两类</p><h4 id="Density-Plots-密度图"><a href="#Density-Plots-密度图" class="headerlink" title="Density Plots(密度图)"></a>Density Plots(密度图)</h4><p>密度图比直方图更能表示出数据分布的特点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.plot(kind&#x3D;&#39;density&#39;, subplots&#x3D;True, layout&#x3D;(3,3), sharex&#x3D;False) pyplot.show()</span><br></pre></td></tr></table></figure><p>![Density plots of each attribute](/images/Density plots of each attribute.png)</p><h4 id="Box-and-Whisker-Plots-箱线图"><a href="#Box-and-Whisker-Plots-箱线图" class="headerlink" title="Box and Whisker Plots(箱线图)"></a>Box and Whisker Plots(箱线图)</h4><p><a href="http://wiki.mbalib.com/wiki/%E7%AE%B1%E7%BA%BF%E5%9B%BE" target="_blank" rel="noopener">箱线图</a> 画出Q1（25分位），Q2（中位数），Q3（75分位），下边界（Q1-1.5<em>(Q3-Q1)）,上边界（Q3+1.5</em>(Q3-Q1)）</p><p>为什么要画箱线图：</p><ul><li>1直观识别异常值。超过上下边界的认为是异常值；</li><li>2看出数据分布的skew和尾重</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">data.plot(kind&#x3D;&#39;box&#39;, subplots&#x3D;True, layout&#x3D;(3,3), sharex&#x3D;False, sharey&#x3D;False)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>![Box and whisker plots of each attribute](/images/Box and whisker plots of each attribute.png)<br>一下可以看出age,test，skin的数据向smaller values偏斜</p><h3 id="Multivariate-Plots-多变量画图"><a href="#Multivariate-Plots-多变量画图" class="headerlink" title="Multivariate Plots(多变量画图)"></a>Multivariate Plots(多变量画图)</h3><p>主要是反应是不同属性间的interaction关系</p><h4 id="Correlation-Matrix-Plot"><a href="#Correlation-Matrix-Plot" class="headerlink" title="Correlation Matrix Plot"></a>Correlation Matrix Plot</h4><p>可以参考上面关于相关性的解释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Correlation Matrix Plot</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line">from pandas import read_csv</span><br><span class="line">import numpy</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">correlations &#x3D; data.corr() # plot correlation matrix</span><br><span class="line">fig &#x3D; pyplot.figure()  # init一个画布</span><br><span class="line">ax &#x3D; fig.add_subplot(111) # 将画布分割成1行1列，图像画在第一块</span><br><span class="line">cax &#x3D; ax.matshow(correlations, vmin&#x3D;-1, vmax&#x3D;1) # matshow是画出矩阵图</span><br><span class="line">fig.colorbar(cax) #给出colorbar</span><br><span class="line">ticks &#x3D; numpy.arange(0,9,1) # tick是刻度的意思</span><br><span class="line">ax.set_xticks(ticks) #设置x刻度范围，步长</span><br><span class="line">ax.set_yticks(ticks)</span><br><span class="line">ax.set_xticklabels(names) #x刻度用names值来表示</span><br><span class="line">ax.set_yticklabels(names)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>跟多关于绘画的<a href="https://matplotlib.org/index.html" target="_blank" rel="noopener">参考</a></p><p>也可以不需要set_xticks，去除ax.set_xticks一下内容，之后图片显示没有label了</p><p><img src="/images/Correlation matrix plot1.png" width="50%" height="50%"><img src="/images/Correlation matrix plot2.png" width="50%" height="50%"></p><p>其实现在还有一种更好看的画图库<a href="http://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heat#seaborn.heatmap" target="_blank" rel="noopener">seaborn</a>来表示热点图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line">cr &#x3D; data.corr()</span><br><span class="line"># plot a heatmap of correlation</span><br><span class="line">ax1 &#x3D; sns.heatmap(cr, center &#x3D; 0)</span><br><span class="line">display(cr)</span><br></pre></td></tr></table></figure><p>![seaborn example](/images/seaborn example.png)</p><h4 id="Scatter-Plot-Matrix-散点图"><a href="#Scatter-Plot-Matrix-散点图" class="headerlink" title="Scatter Plot Matrix(散点图)"></a>Scatter Plot Matrix(散点图)</h4><p>pd.scatter_matrix()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib import pyplot</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from pandas.plotting import scatter_matrix</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">scatter_matrix(data) # if设置diagonal &#x3D;&#39;kde&#39;,对角线会显示密度图</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>![Scatter plot matrix of the data](/images/Scatter plot matrix of the data.png)</p><h3 id="More-links"><a href="#More-links" class="headerlink" title="More links"></a>More links</h3><p><a href="https://blog.csdn.net/qq_34264472/article/details/53814653" target="_blank" rel="noopener">seaborn tutorial</a><br><a href="https://blog.csdn.net/ali197294332/article/details/51694141" target="_blank" rel="noopener">matplotlib tutorial</a><br><a href="https://github.com/JonOnEarth/ML-NANO/blob/master/customer_segments/customer_segments.ipynb" target="_blank" rel="noopener">我的github项目</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine learning with python(1)</title>
      <link href="/2018/05/15/machine-learning-with-python-1/"/>
      <url>/2018/05/15/machine-learning-with-python-1/</url>
      
        <content type="html"><![CDATA[<p>想来机器学习学习也快1年了。一年来，囫囵吞枣，上完了Udacity的机器学习课程，拿到了证书，做了几个项目。但总感觉自己学的不够扎实，学了后面忘了前面，非常有必要来复习整理一下。尤其现在跟Professor做了一个机器学习应用，更加凸显了自己数学方面的薄弱。</p><p>想了好几种方式来扎实自己的机器学习。开始想用Udacity复习的，发现再翻看视频很费时间，暂且作为补充方式。首选这位大神的书，用他的书来复习，能事半功倍。强势安利：<a href="https://machinelearningmastery.com/products/" target="_blank" rel="noopener">machinelearningmastery.com</a>。以下基本都是书中的笔记。</p><p>这次机器学习的几个重点：<br>1, python怎么处理数据<br>2, 算法总结</p><h2 id="Python-Ecosystem-for-Machine-Learning"><a href="#Python-Ecosystem-for-Machine-Learning" class="headerlink" title="Python Ecosystem for Machine Learning"></a>Python Ecosystem for Machine Learning</h2><h3 id="两个Python软件："><a href="#两个Python软件：" class="headerlink" title="两个Python软件："></a>两个Python软件：</h3><p>推荐新手安装这两个python软件(我是win10)：  </p><ul><li>Anaconda 读“安娜康德”，大蟒蛇的意思。数据处理的很多包都已经集成，安装省事，不需要你配置各种环境。</li><li>winPython 绿色版本，不用安装，下载即用。集成了各种软件，各种库。</li><li>新手开始最好用软件里的jupyter notebook，别提多方便。附上jupyter的一个<a href="https://www.datacamp.com/community/blog/jupyter-notebook-cheat-sheet" target="_blank" rel="noopener">小抄</a></li></ul><h3 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h3><ul><li>Numpy：让数据变成arrays格式，使数据适用于机器学习算法</li><li>Pandas: load, organize and anlyze 数据, 更好的理解数据</li><li>Matpoltlib: 可视化数据</li></ul><h3 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h3><p>需要Scipy的环境支持。有很多的机器学习算法，分类，回归，聚类等。</p><h2 id="Crash-course"><a href="#Crash-course" class="headerlink" title="Crash course"></a>Crash course</h2><h3 id="Crash-course-of-python"><a href="#Crash-course-of-python" class="headerlink" title="Crash course of python"></a>Crash course of python</h3><p>参考<a href="https://jononearth.com/%E5%AD%A6%E7%BC%96%E7%A8%8B/python-cheatsheet1/">python小抄</a>  </p><h3 id="Numpy-Crash-Course"><a href="#Numpy-Crash-Course" class="headerlink" title="Numpy Crash Course"></a>Numpy Crash Course</h3><p>参考<a href="https://jononearth.com/%E5%AD%A6%E7%BC%96%E7%A8%8B/Numpy-%E5%B0%8F%E6%8A%84/">numpy小抄</a></p><h4 id="create-array"><a href="#create-array" class="headerlink" title="create array"></a>create array</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># define an array</span><br><span class="line">import numpy</span><br><span class="line">mylist &#x3D; [1, 2, 3]</span><br><span class="line">myarray &#x3D; numpy.array(mylist)</span><br><span class="line">print(myarray)</span><br><span class="line">print(myarray.shape)</span><br><span class="line"># out</span><br><span class="line">[1 2 3]</span><br><span class="line">(3,)</span><br></pre></td></tr></table></figure><h4 id="获取-Data"><a href="#获取-Data" class="headerlink" title="获取 Data"></a>获取 Data</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># access values</span><br><span class="line">import numpy</span><br><span class="line">mylist &#x3D; [[1, 2, 3], [3, 4, 5]]</span><br><span class="line">myarray &#x3D; numpy.array(mylist)</span><br><span class="line">print(myarray)</span><br><span class="line">print(myarray.shape)</span><br><span class="line">print(&quot;First row: %s&quot; % myarray[0])</span><br><span class="line">print(&quot;Last row: %s&quot; % myarray[-1])</span><br><span class="line">print(&quot;Specific row and col: %s&quot; % myarray[0, 2])</span><br><span class="line">print(&quot;Whole col: %s&quot; % myarray[:, 2])</span><br><span class="line"># out</span><br><span class="line">[[1 2 3] [3 4 5]]</span><br><span class="line">(2, 3)</span><br><span class="line">First row: [1 2 3]</span><br><span class="line">Last row: [3 4 5]</span><br><span class="line">Specific row and col: 3</span><br><span class="line">Whole col: [3 5]</span><br></pre></td></tr></table></figure><h4 id="Arithmetic"><a href="#Arithmetic" class="headerlink" title="Arithmetic"></a>Arithmetic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># arithmetic</span><br><span class="line">import numpy</span><br><span class="line">myarray1 &#x3D; numpy.array([2, 2, 2])</span><br><span class="line">myarray2 &#x3D; numpy.array([3, 3, 3])</span><br><span class="line">print(&quot;Addition: %s&quot; % (myarray1 + myarray2))</span><br><span class="line">print(&quot;Multiplication: %s&quot; % (myarray1 * myarray2))</span><br><span class="line"># out</span><br><span class="line">Addition: [5 5 5]</span><br><span class="line">Multiplication: [6 6 6]</span><br></pre></td></tr></table></figure><h3 id="Matplotlib-Crash-Course"><a href="#Matplotlib-Crash-Course" class="headerlink" title="Matplotlib Crash Course"></a>Matplotlib Crash Course</h3><ul><li>call the function with data (such as .plot)</li><li>call many functions to setup the properties of the plot(labels and colors)</li><li>make the plot visible(e.g. .show)</li></ul><h4 id="直线图"><a href="#直线图" class="headerlink" title="直线图"></a>直线图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># basic line plot 一维</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy</span><br><span class="line">myarray &#x3D; numpy.array([1, 2, 3])</span><br><span class="line">plt.plot(myarray)</span><br><span class="line">plt.xlabel(&#39;some x axis&#39;)</span><br><span class="line">plt.ylabel(&#39;some y axis&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># basic scatter plot</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy</span><br><span class="line">x &#x3D; numpy.array([1, 2, 3])</span><br><span class="line">y &#x3D; numpy.array([2, 4, 6])</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.xlabel(&#39;some x axis&#39;)</span><br><span class="line">plt.ylabel(&#39;some y axis&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="subplots"><a href="#subplots" class="headerlink" title="subplots"></a>subplots</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, axes &#x3D; plt.subplots(2, 2, subplot_kw&#x3D;dict(polar&#x3D;True))# fig还是指一个整体图</span><br><span class="line">axes[0, 0].plot(x, y) # axes表示每个图的集合</span><br><span class="line">axes[1, 1].scatter(x, y)</span><br></pre></td></tr></table></figure><h4 id="python-画图"><a href="#python-画图" class="headerlink" title="python 画图"></a>python 画图</h4><ul><li>怎么画更好看：<a href="https://matplotlib.org/3.2.0/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py" target="_blank" rel="noopener">style sheets</a>, <a href="https://matplotlib.org/3.2.0/gallery/index.html#style-sheets" target="_blank" rel="noopener">other style sheets</a></li><li>plotly: </li></ul><h3 id="Pandas-Crash-Course"><a href="#Pandas-Crash-Course" class="headerlink" title="Pandas Crash Course"></a>Pandas Crash Course</h3><p>参考<a href="https://jononearth.com/%E5%AD%A6%E7%BC%96%E7%A8%8B/Pands%E5%B0%8F%E6%8A%84/">pandas小抄</a></p><h4 id="series"><a href="#series" class="headerlink" title="series"></a>series</h4><p>一个series是1维的array + index，每个row上面加了label</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># series</span><br><span class="line">import numpy</span><br><span class="line">import pandas</span><br><span class="line">myarray &#x3D; numpy.array([1, 2, 3])</span><br><span class="line">rownames &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]</span><br><span class="line">myseries &#x3D; pandas.Series(myarray, index&#x3D;rownames)</span><br><span class="line">print(myseries)</span><br><span class="line"># out</span><br><span class="line">a 1</span><br><span class="line">b 2</span><br><span class="line">c 3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(myseries[0])</span><br><span class="line">print(myseries[&#39;a&#39;])</span><br><span class="line">1</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>Data frame是多维array + row_label + col_label</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># dataframe</span><br><span class="line">import numpy</span><br><span class="line">import pandas</span><br><span class="line">myarray &#x3D; numpy.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">rownames &#x3D; [&#39;a&#39;, &#39;b&#39;]</span><br><span class="line">colnames &#x3D; [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span><br><span class="line">mydataframe &#x3D; pandas.DataFrame(myarray, index&#x3D;rownames, columns&#x3D;colnames)</span><br><span class="line">print(mydataframe)</span><br><span class="line"># out</span><br><span class="line">  one two three</span><br><span class="line">a 1   2    3</span><br><span class="line">b 4   5    6</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;method 1:&quot;)</span><br><span class="line">print(&quot;one column:\n%s&quot; % mydataframe[&#39;one&#39;])</span><br><span class="line">print(&quot;method 2:&quot;)</span><br><span class="line">print(&quot;one column:\n%s&quot; % mydataframe.one)</span><br><span class="line"># out</span><br><span class="line">method 1:</span><br><span class="line">one column:</span><br><span class="line">a 1</span><br><span class="line">b 4</span><br><span class="line">method 2:</span><br><span class="line">one column:</span><br><span class="line">a 1</span><br><span class="line">b 4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas小抄</title>
      <link href="/2018/05/10/Pands%E5%B0%8F%E6%8A%84/"/>
      <url>/2018/05/10/Pands%E5%B0%8F%E6%8A%84/</url>
      
        <content type="html"><![CDATA[<p>Pandas 在数据中学中是常用的库。 整理了一些好用方便的小抄，随时可以查看。</p><h2 id="Pandas小抄"><a href="#Pandas小抄" class="headerlink" title="Pandas小抄"></a>Pandas小抄</h2><ul><li><p><a href="https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/pandas/pandas.ipynb" target="_blank" rel="noopener">来源1 notebook版</a><br><img src="/images/pandas%E5%B0%8F%E6%8A%841.jpg" alt=""></p></li><li><p><a href="https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.S4P4T=U" target="_blank" rel="noopener">来源2</a><br><img src="/images/pandas%E5%B0%8F%E6%8A%842.jpg" alt=""></p></li><li><p><a href="http://datasciencefree.com/pandas.pdf" target="_blank" rel="noopener">来源3</a><br><img src="/images/pandas%E5%B0%8F%E6%8A%843.jpg" alt=""></p></li></ul><h2 id="my-note"><a href="#my-note" class="headerlink" title="my note"></a>my note</h2><h3 id="重要概念1：Series"><a href="#重要概念1：Series" class="headerlink" title="重要概念1：Series"></a>重要概念1：Series</h3><ul><li>series 有 index<br>表示方式：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ser_1 &#x3D; Series([1,5,3])</span><br><span class="line">ser_1</span><br><span class="line">&gt;&gt;&gt; 0   1</span><br><span class="line">&gt;&gt;&gt; 1   5</span><br><span class="line">&gt;&gt;&gt; 2   3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ser_1.values</span><br><span class="line">&gt;&gt;&gt;array([1,5,3])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ser_2 &#x3D; Series([1,1,2,,-3], index &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]</span><br><span class="line">ser_2</span><br><span class="line">&gt;&gt;&gt;a   1</span><br><span class="line">&gt;&gt;&gt;b   1</span><br><span class="line">&gt;&gt;&gt;c   2</span><br><span class="line">&gt;&gt;&gt;d   -3</span><br></pre></td></tr></table></figure></li><li>create a series by passing in a dict:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dict_1 &#x3D; &#123;&#39;foo&#39; : 100, &#39;bar&#39; : 200, &#39;baz&#39; : 300&#125;</span><br><span class="line">ser_3 &#x3D; Series(dict_1)</span><br><span class="line">ser3</span><br><span class="line">&gt;&gt;&gt;bar  200</span><br><span class="line">&gt;&gt;&gt;baz  300</span><br><span class="line">&gt;&gt;&gt;foo  100</span><br><span class="line">&gt;&gt;&gt;dtype: int64</span><br></pre></td></tr></table></figure></li></ul><h3 id="重要概念2：DataFrame"><a href="#重要概念2：DataFrame" class="headerlink" title="重要概念2：DataFrame"></a>重要概念2：DataFrame</h3><ul><li>Dict to DataFrame   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dict_1 &#x3D; &#123;&#39;state&#39;:[&#39;VA&#39;,&#39;VA&#39;,&#39;VA&#39;,&#39;MA&#39;,&#39;MA&#39;],</span><br><span class="line">          &#39;year&#39;:[2012,2013,2014,2014,2015],</span><br><span class="line">          &#39;pop&#39;:[5,5,5,3,2]&#125;</span><br><span class="line">df_1 &#x3D; DataFrame(dict_1)</span><br></pre></td></tr></table></figure></li></ul><table><thead><tr><th align="left"></th><th align="left">pop</th><th align="left">state</th><th align="left">year</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">5</td><td align="left">VA</td><td align="left">2012</td></tr><tr><td align="left">1</td><td align="left">5</td><td align="left">VA</td><td align="left">2013</td></tr><tr><td align="left">2</td><td align="left">5</td><td align="left">VA</td><td align="left">2014</td></tr><tr><td align="left">3</td><td align="left">3</td><td align="left">MA</td><td align="left">2014</td></tr><tr><td align="left">4</td><td align="left">2</td><td align="left">MA</td><td align="left">2015</td></tr></tbody></table><ul><li>返回Series值  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df_1[&#39;state&#39;]</span><br><span class="line">&gt;&gt;&gt;0 VA</span><br><span class="line">&gt;&gt;&gt;1 VA</span><br><span class="line">&gt;&gt;&gt;2 VA</span><br><span class="line">&gt;&gt;&gt;3 MA</span><br><span class="line">&gt;&gt;&gt;4 MA</span><br><span class="line">&gt;&gt;&gt;Name: state, dtype: object</span><br></pre></td></tr></table></figure><ul><li>取一行值</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_1.ix[0]</span><br><span class="line">&gt;&gt;&gt;pop 5</span><br><span class="line">&gt;&gt;&gt;state VA</span><br><span class="line">&gt;&gt;&gt;year 2012</span><br></pre></td></tr></table></figure><h3 id="去掉NaN值（todo）"><a href="#去掉NaN值（todo）" class="headerlink" title="去掉NaN值（todo）"></a>去掉NaN值（todo）</h3><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><ul><li>get dataset csv</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_1 &#x3D; pd.read_csv(&quot;..&#x2F;data&#x2F;jon.csv&quot;)</span><br></pre></td></tr></table></figure><ul><li>list the first 5 rows of the DataFrame</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_1.head()</span><br></pre></td></tr></table></figure><ul><li>create a copy of the CSV file, encoded in UTF-8 and hiding the index and header labels:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_1.to_csv(&#39;..&#x2F;data&#x2F;jon.csv,</span><br><span class="line">            encoding&#x3D;&#39;utf-8&#39;,</span><br><span class="line">            index&#x3D;False,</span><br><span class="line">            header&#x3D;False)</span><br></pre></td></tr></table></figure><h3 id="df里的数据与数比大小后-输出"><a href="#df里的数据与数比大小后-输出" class="headerlink" title="df里的数据与数比大小后 输出"></a>df里的数据与数比大小后 输出</h3><p>df_ &gt; 5表示 df里面大于5的数输出true，df_[true]输出，<strong>小于5的输出NaN</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_[df_ &gt; 5]</span><br></pre></td></tr></table></figure><h3 id="合并数据"><a href="#合并数据" class="headerlink" title="合并数据"></a>合并数据</h3><p>pd.concat([],axis=0, ignore_index=true,…)</p><h3 id="重复值的处理"><a href="#重复值的处理" class="headerlink" title="重复值的处理"></a>重复值的处理</h3><p>Python数据分析-数据处理-重复值处理<br><a href="https://blog.csdn.net/maxwell315/article/details/75639190" target="_blank" rel="noopener">https://blog.csdn.net/maxwell315/article/details/75639190</a></p><p>duplicated<br>drop_duplicated</p><h3 id="读取mat格式文件"><a href="#读取mat格式文件" class="headerlink" title="读取mat格式文件"></a>读取mat格式文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mport pandas as pd</span><br><span class="line">import scipy</span><br><span class="line">from scipy import io</span><br><span class="line">features_struct &#x3D; scipy.io.loadmat(&#39;E:&#x2F;workspacelxr&#x2F;contem&#x2F;data.mat&#39;)</span><br><span class="line">features &#x3D; features_struct[&#39;data&#39;]</span><br><span class="line">dfdata &#x3D; pd.DataFrame(features)</span><br><span class="line">datapath1 &#x3D; &#39;E:&#x2F;workspacelxr&#x2F;contem&#x2F;data.txt&#39;</span><br><span class="line">dfdata.to_csv(datapath1, index&#x3D;False)</span><br><span class="line">---------------------</span><br><span class="line">作者：Luisa_M</span><br><span class="line">来源：CSDN</span><br><span class="line">原文：https:&#x2F;&#x2F;blog.csdn.net&#x2F;zebralxr&#x2F;article&#x2F;details&#x2F;78254192</span><br><span class="line">版权声明：本文为博主原创文章，转载请附上博文链接！</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pandas </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numpy 小抄</title>
      <link href="/2018/05/09/Numpy-%E5%B0%8F%E6%8A%84/"/>
      <url>/2018/05/09/Numpy-%E5%B0%8F%E6%8A%84/</url>
      
        <content type="html"><![CDATA[<p>NumPy is the library that gives Python its ability to work with data at speed. Originally, launched in 1995 as ‘Numeric,’ NumPy is the foundation on which many important Python data science libraries are built, including Pandas, SciPy and scikit-learn.</p><p>先给出几个很好的numpy 小抄，以后再整理遇到的一些numpy相关问题：</p><h2 id="Numpy小抄"><a href="#Numpy小抄" class="headerlink" title="Numpy小抄"></a>Numpy小抄</h2><ul><li><p><img src="/images/numpy_%E5%B0%8F%E6%8A%842.jpg" alt="[来源1](https://www.dataquest.io/blog/numpy-cheat-sheet/)"></p></li><li><p><img src="/images/numpy_notebook.png" alt="[来源2：Notebook版](https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/numpy/numpy.ipynb)"></p></li></ul><h2 id="numpy重要性质"><a href="#numpy重要性质" class="headerlink" title="numpy重要性质"></a>numpy重要性质</h2><h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h3><p>就是说矩阵运算的时候， a.shape = (3,2), b.shape=(3,1), a+b 时，b会自动复制成shape = (3,2),进行运算。</p><p>参考： <a href="https://www.coursera.org/learn/neural-networks-deep-learning/notebook/Zh0CU/python-basics-with-numpy-optional" target="_blank" rel="noopener">coursera deeplearning.ai</a>,</p><h2 id="My-note"><a href="#My-note" class="headerlink" title="My note"></a>My note</h2><table><thead><tr><th align="left">函数</th><th align="left">解释</th><th align="left">例子</th></tr></thead><tbody><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.around.html#numpy.around" target="_blank" rel="noopener">numpy.round(a, decimals=0, out=None)</a> 与around函数相似</td><td align="left">Round an array to the given number of [decimals].</td><td align="left">np.around([0.37, 1.64], decimals=1)  结果：array([ 0.4,  1.6])</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.floor.html" target="_blank" rel="noopener">np.floor()</a></td><td align="left">返回标量的最大整数</td><td align="left">&gt;&gt;&gt; a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])  (&gt;&gt;&gt; np.floor(a) 结果：array([-2., -2., -1.,  0.,  1.,  1.,  2.]))</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html" target="_blank" rel="noopener">np.column_stack((x, y))</a></td><td align="left">把两个1D的array变成一个2D array</td><td align="left">&gt;&gt;&gt; a = np.array((1,2,3)) &gt;&gt;&gt; b = np.array((2,3,4)) &gt;&gt;&gt; np.column_stack((a,b)) 结果：array([[1, 2],[2, 3],[3, 4]])</td></tr><tr><td align="left">numpy.arange(0,9,1)</td><td align="left">表示0到9之间的数，step是1</td><td align="left">输出是0，1，2…到8</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linspace.html" target="_blank" rel="noopener">np.linspace(start, stop, num,endpoint)</a></td><td align="left">start to stop之间有num个数</td><td align="left">&gt;&gt;&gt; np.linspace(2.0, 3.0, num=5, endpoint=False) out: array([ 2. ,  2.2,  2.4,  2.6,  2.8])</td></tr><tr><td align="left">np.float_()</td><td align="left">list变array, float</td><td align="left"></td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html" target="_blank" rel="noopener">np.unique()</a></td><td align="left">算array里有多少个count的时候可以用一下</td><td align="left"></td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html" target="_blank" rel="noopener">numpy.random.rand(a,b)</a></td><td align="left">转化成a行b列0,1之间的任意数</td><td align="left">np.random.rand(3,2) out: array([[ 0.14022471,  0.96360618], [ 0.37601032,  0.25528411], [ 0.49313049,  0.94909878]]) #random</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html" target="_blank" rel="noopener">np.sum()</a></td><td align="left">axis=1,一行向量自己里面相加，axis=0,一列里面自己相加</td><td align="left">&gt;&gt;&gt; np.sum([[0, 1], [0, 5]]), np.sum([[0, 1], [0, 5]], axis=0) np.sum([[0, 1], [0, 5]], axis=1) output: 6, array([0,6]), array([1, 5])</td></tr><tr><td align="left"><a href="https://blog.csdn.net/hqh131360239/article/details/79061535" target="_blank" rel="noopener">np.linalg.norm()</a></td><td align="left">求范数,默认2范数，axis=1,一行向量自己范2，axis=0,一列列列向量范2</td><td align="left"></td></tr><tr><td align="left">np.row_stack()</td><td align="left">矩阵增加行</td><td align="left">a = np.array([[4, 4,], [5, 5]])  c = np.row_stack((a, [8,9]))  output: array([[4,4],[5,5],[8,9]])</td></tr><tr><td align="left">np.column_stack()</td><td align="left">矩阵增加行</td><td align="left">a = np.array([[4, 4,], [5, 5]])  c = np.column_stack((a, [8,9]))  output: array([[4,4,8],[5,5,9])</td></tr><tr><td align="left"><a href="https://jingyan.baidu.com/article/9113f81b2291802b3214c706.html" target="_blank" rel="noopener">np.all()</a></td><td align="left">假如我们想要知道矩阵a和矩阵b中所有对应元素是否相等，我们需要使用all方法</td><td align="left"></td></tr><tr><td align="left">np.any()</td><td align="left">假如我们想要知道矩阵a和矩阵b中对应元素是否有一个相等，我们需要使用any方法</td><td align="left"></td></tr><tr><td align="left"><a href="https://blog.csdn.net/yangyuwen_yang/article/details/79193770" target="_blank" rel="noopener">np.unique()</a>，<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html" target="_blank" rel="noopener">link2</a></td><td align="left">重复的值，取唯一值并且从小到大排列</td><td align="left">a, s= np.unique(A, return_index=True)</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.delete.html" target="_blank" rel="noopener">np.delete(arr,1,axis=0)</a></td><td align="left">axis = 0 mean delete vertically</td><td align="left"></td></tr><tr><td align="left"><a href="https://www.programiz.com/python-programming/assert-statement" target="_blank" rel="noopener">assert(a.shape == (1,5))</a></td><td align="left">Assertions are simply boolean expressions that checks if the conditions return true or not. If it is true, the program does nothing and move to the next line of code. However, if it’s false, the program stops and throws an error.</td><td align="left"></td></tr><tr><td align="left">numpy.sum(a, axis=None, dtype=None, out=None, keepdims=False)</td><td align="left"><a href="https://stackoverflow.com/questions/39441517/in-numpy-sum-there-is-parameter-called-keepdims-what-does-it-do" target="_blank" rel="noopener">keepdims</a>的重要性</td><td align="left"></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 小抄</title>
      <link href="/2018/05/08/python-cheatsheet1/"/>
      <url>/2018/05/08/python-cheatsheet1/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th align="left">列表</th><th align="left">元组</th><th align="left">集合</th><th align="left">字典</th></tr></thead><tbody><tr><td align="left">英文</td><td align="left">list</td><td align="left">tuple</td><td align="left">set</td></tr><tr><td align="left">可否读写</td><td align="left">读写</td><td align="left">只读</td><td align="left">读写</td></tr><tr><td align="left">可否重复</td><td align="left">是</td><td align="left">是</td><td align="left">否</td></tr><tr><td align="left">存储方式</td><td align="left">值</td><td align="left">值</td><td align="left">键(不能重复)</td></tr><tr><td align="left">是否有序</td><td align="left">有序</td><td align="left">有序</td><td align="left">无序</td></tr><tr><td align="left">初始化</td><td align="left">[1,’a’]</td><td align="left">(‘a’, 1)</td><td align="left">set([1,2]) 或 {1,2}</td></tr><tr><td align="left">添加</td><td align="left">append</td><td align="left">只读</td><td align="left">add</td></tr><tr><td align="left">读元素</td><td align="left">l[2:]</td><td align="left">t[0]</td><td align="left">无</td></tr></tbody></table><table><thead><tr><th align="left">字符串’string’</th><th align="left">列表[lists]</th><th align="left">元组(Tuples)</th><th align="left">字典{dictionaries}</th></tr></thead><tbody><tr><td align="left">字符串连接：firstname=’jon’, lastname=’earth’, fullname = firstname + lastname</td><td align="left">创建一个列表：planets = [‘Earth’, ‘Mars’, ‘Jupiter’]</td><td align="left">元组类似lists,但其中元素不能被操作：dimensions=(1920, 1080)</td><td align="left">一个简单的字典 alien = {‘color’:’green’,’planet’:’Mars’}</td></tr><tr><td align="left"></td><td align="left">获取列表元素：home = planets[0]</td><td align="left">dimensions[0]=1920</td><td align="left">取得字典中的keyvalue,如 alien[‘color’]  输出 green</td></tr><tr><td align="left"></td><td align="left">列表切片 planets.append(‘Saturn’)</td><td align="left"></td><td align="left">加入一个新的键值(key为x_positon, value为0)  alien[‘x_position’] = 0</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">遍历所有的键值(key-value)  for name, number in fav_numbers.items():</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">遍历所有的“键”(key)  for name in fav_numbers.keys()：</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">遍历所有的“值”(value)  for number in fav_numbers.values()：</td></tr></tbody></table><h2 id="一些很有用的参考资料"><a href="#一些很有用的参考资料" class="headerlink" title="一些很有用的参考资料"></a>一些很有用的参考资料</h2><ul><li><a href="http://pythontutor.com/" target="_blank" rel="noopener">VISUALIZE CODE AND GET LIVE HELP</a><br>可视化你的code, 帮助你理解</li><li><a href="https://www.cnblogs.com/soaringEveryday/p/5044007.html" target="_blank" rel="noopener">python中 list, tuple，dict, set的用法</a></li><li><a href="https://blog.csdn.net/lilongsy/article/details/70895753" target="_blank" rel="noopener">Python列表、元组、集合、字典的区别和相互转换</a></li><li><a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PythonForDataScience.pdf" target="_blank" rel="noopener">Lists小抄和Numpy</a></li></ul><hr><h2 id="set"><a href="#set" class="headerlink" title="set"></a>set</h2><p>set就像是把Dict中的key抽出来了一样，类似于一个List，但是内容又不能重复，通过调用set()方法创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set([3,6,3]) or</span><br><span class="line">   &#123;3,6,3&#125;</span><br></pre></td></tr></table></figure><ul><li>嵌套tuple<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">s &#x3D; set([(&#39;Adam&#39;, 95), (&#39;Lisa&#39;, 85), (&#39;Bart&#39;, 59)])</span><br><span class="line">#tuple</span><br><span class="line">for x in s:</span><br><span class="line">    print x[0],&#39;:&#39;,x[1]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">Lisa : 85</span><br><span class="line">Adam : 95</span><br><span class="line">Bart : 59</span><br></pre></td></tr></table></figure></li><li>通过add和remove来添加、删除元素（保持不重复），添加元素时，用set的add()方法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; set([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; s.add(4)</span><br><span class="line">&gt;&gt;&gt; print s</span><br><span class="line">set([1, 2, 3, 4])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; set([1, 2, 3, 4])</span><br><span class="line">&gt;&gt;&gt; s.remove(4)</span><br><span class="line">&gt;&gt;&gt; print s</span><br><span class="line">set([1, 2, 3])</span><br></pre></td></tr></table></figure></li></ul><h2 id="列表lists"><a href="#列表lists" class="headerlink" title="列表lists"></a>列表lists</h2><ul><li>lists中嵌套lists<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_list2 &#x3D; [[4,5,6,7], [3,4,5,6]]</span><br><span class="line">my_list2[1][:2]</span><br></pre></td></tr></table></figure></li></ul><h3 id="Numpy-与-list-简单对比"><a href="#Numpy-与-list-简单对比" class="headerlink" title="Numpy 与 list 简单对比"></a>Numpy 与 list 简单对比</h3><ul><li>Numpy Arrarys<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_list &#x3D; [1, 2, 3, 4]</span><br><span class="line">my_array &#x3D; np.array(my_list)</span><br><span class="line">my_2darray &#x3D; np.array([[1,2,3],[4,5,6]])</span><br></pre></td></tr></table></figure></li><li>取其中的元素<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Subset</span><br><span class="line">my_array[1]  # select item at index 1</span><br><span class="line">&gt;&gt;&gt; 2</span><br><span class="line"></span><br><span class="line"># slice</span><br><span class="line">my_array[0:2]</span><br><span class="line">&gt;&gt;&gt;array([1, 2])  # selcet items at index 0 and 1</span><br><span class="line"></span><br><span class="line"># Subset 2D Numpy arrays</span><br><span class="line">my_2darray[:,0]</span><br><span class="line">&gt;&gt;&gt;arrary([1,4])  # my_2darray[rows, columns]</span><br></pre></td></tr></table></figure></li></ul><h2 id="字典-Dictionaries"><a href="#字典-Dictionaries" class="headerlink" title="字典 Dictionaries"></a>字典 Dictionaries</h2><ul><li>使用key来访问value 2种方法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alien_0 &#x3D; &#123;&#39;color&#39;: &#39;green&#39;, &#39;points&#39;:5&#125;</span><br><span class="line">print(alien_0[&#39;color&#39;])</span><br><span class="line">print(alien_0[&#39;points&#39;])</span><br><span class="line"></span><br><span class="line">alien_color &#x3D; alien_0.get(&#39;color&#39;)</span><br></pre></td></tr></table></figure></li><li>增加一个键值对<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alien_0 &#x3D; &#123;&#39;color&#39;: &#39;green&#39;, &#39;points&#39;: 5&#125;</span><br><span class="line">alien_0[&#39;x&#39;] &#x3D; 0</span><br><span class="line">alien_0[&#39;y&#39;] &#x3D; 25</span><br><span class="line">alien_0[&#39;speed&#39;] &#x3D; 1.5</span><br></pre></td></tr></table></figure></li><li>删除一个键值对<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del alien_0[&#39;points&#39;]</span><br></pre></td></tr></table></figure></li><li>Dict的合并<ul><li>方法1：可以用dict函数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d1 &#x3D; &#123;&#39;mike&#39;:12, &#39;jack&#39;:19&#125;</span><br><span class="line">d2 &#x3D; &#123;&#39;jone&#39;:22, &#39;ivy&#39;:17&#125;</span><br><span class="line">dMerge &#x3D; dict(d1.items() + d2.items())</span><br><span class="line">print dMerge</span><br><span class="line">&gt;&gt;&gt;&#123;&#39;mike&#39;: 12, &#39;jack&#39;: 19, &#39;jone&#39;: 22, &#39;ivy&#39;: 17&#125;</span><br></pre></td></tr></table></figure></li><li>方法2：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dMerge2 &#x3D; dict(d1, **d2)</span><br><span class="line">print dMerge2</span><br><span class="line">&gt;&gt;&gt;&#123;&#39;mike&#39;: 12, &#39;jack&#39;: 19, &#39;jone&#39;: 22, &#39;ivy&#39;: 17&#125;</span><br></pre></td></tr></table></figure></li><li>方法2比方法1速度快很多，方法2等同于：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dMerge3 &#x3D; dict(d1)</span><br><span class="line">dMerge3.update(d2)</span><br><span class="line">print dMerge</span><br><span class="line">&gt;&gt;&gt;&#123;&#39;mike&#39;: 12, &#39;jack&#39;: 19, &#39;jone&#39;: 22, &#39;ivy&#39;: 17&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="嵌套"><a href="#嵌套" class="headerlink" title="嵌套"></a>嵌套</h2><ol><li><p>字典嵌套在列表中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Start with anempty list.</span><br><span class="line">users &#x3D; []</span><br><span class="line"># Make a new user, and add them to the list.</span><br><span class="line">new_user &#x3D; &#123;    </span><br><span class="line">  &#39;last&#39;: &#39;fermi&#39;,   </span><br><span class="line">  &#39;first&#39;: &#39;enrico&#39;,  </span><br><span class="line">  &#39;username&#39;: &#39;efermi&#39;,</span><br><span class="line">   &#125;</span><br><span class="line">users.append(new_user)</span><br><span class="line"></span><br><span class="line"> # Make another new user, and add them as well.</span><br><span class="line">new_user &#x3D; &#123;   </span><br><span class="line">   &#39;last&#39;: &#39;curie&#39;,  </span><br><span class="line">  &#39;first&#39;: &#39;marie&#39;,   </span><br><span class="line">  &#39;username&#39;: &#39;mcurie&#39;,    &#125;</span><br><span class="line">users.append(new_user)</span><br><span class="line"></span><br><span class="line"> # Show all information about each user.</span><br><span class="line">for user_dict in users:    </span><br><span class="line">    for k, v in user_dict.items():</span><br><span class="line">        print(k + &quot;: &quot; + v)   </span><br><span class="line">    print(&quot;\n&quot;)</span><br></pre></td></tr></table></figure></li><li><p>列表嵌套在字典中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fav_languages &#x3D; &#123;</span><br><span class="line">      &#39;jen&#39;: [&#39;python&#39;, &#39;ruby&#39;],</span><br><span class="line">      &#39;sarah&#39;: [&#39;c&#39;],  </span><br><span class="line">      &#39;edward&#39;: [&#39;ruby&#39;, &#39;go&#39;],</span><br><span class="line">      &#39;phil&#39;: [&#39;python&#39;, &#39;haskell&#39;],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # Show all responses for each person.</span><br><span class="line">    for name, langs in fav_languages.items():</span><br><span class="line">         print(name + &quot;: &quot;)</span><br><span class="line">         for lang in langs:</span><br><span class="line">             print(&quot;-&quot; + lang)</span><br></pre></td></tr></table></figure></li><li><p>字典嵌套在字典中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">users &#x3D; &#123;</span><br><span class="line">  &#39;aeinstein&#39;: &#123;</span><br><span class="line">        &#39;first&#39;: &#39;albert&#39;,</span><br><span class="line">        &#39;last&#39;: &#39;einstein&#39;,</span><br><span class="line">        &#39;location&#39;: &#39;princeton&#39;,&#125;,</span><br><span class="line">  &#39;mcurie&#39;: &#123;</span><br><span class="line">        &#39;first&#39;: &#39;marie&#39;，</span><br><span class="line">        &#39;last&#39;: &#39;curie&#39;,  </span><br><span class="line">        &#39;location&#39;: &#39;paris&#39;,&#125;,</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"> for username, user_dict in users.items():</span><br><span class="line">     print(&quot;\nUsername: &quot; + username)</span><br><span class="line">     full_name &#x3D; user_dict[&#39;first&#39;] + &quot; &quot;</span><br><span class="line">     full_name +&#x3D; user_dict[&#39;last&#39;]</span><br><span class="line">     location &#x3D; user_dict[&#39;location&#39;]</span><br><span class="line">     print(&quot;\tFull name: &quot; + full_name.title())</span><br><span class="line">     print(&quot;\tLocation: &quot; + location.title())</span><br></pre></td></tr></table></figure></li><li><p>元组tuple嵌套在列表list中</p></li><li><p>list嵌套在tuple中</p></li></ol><h2 id="类class"><a href="#类class" class="headerlink" title="类class"></a>类class</h2><p>todo</p><h2 id="文件读取和写入"><a href="#文件读取和写入" class="headerlink" title="文件读取和写入"></a>文件读取和写入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f_path &#x3D; &quot;C:\Users\ehmatthes\books\alice.txt&quot;</span><br><span class="line"></span><br><span class="line">with open(f_path) as f_obj:</span><br><span class="line">  lines &#x3D; f_obj.readlines()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">filename &#x3D; &#39;programming.txt&#39;</span><br><span class="line"># overwirte文件</span><br><span class="line">with open(filename, &#39;w&#39;) as f:</span><br><span class="line">   f.write(&quot;I love programming!&quot;)</span><br><span class="line"># 添加到文件末尾</span><br><span class="line">with open(filename, &#39;w&#39;) as f:</span><br><span class="line">f.write(&quot;I love programming!&quot;)</span><br></pre></td></tr></table></figure><h2 id="python前面加星号和双星号"><a href="#python前面加星号和双星号" class="headerlink" title="python前面加星号和双星号"></a>python前面加星号和双星号</h2><h2 id="python-除法"><a href="#python-除法" class="headerlink" title="python 除法"></a>python 除法</h2><p><a href="https://blog.csdn.net/GarfieldEr007/article/details/51304068" target="_blank" rel="noopener">Python中的除法 整除 非整除</a></p><h2 id="self-in-python"><a href="#self-in-python" class="headerlink" title="self in python"></a>self in python</h2><p>首先明确的是self只有在类的方法中才会有，独立的函数或方法是不必带有self的。self在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。</p><p>self名称不是必须的，在python中self不是关键词，你可以定义成a或b或其它名字都可以,但是约定成俗（为了和其他编程语言统一，减少理解难度），不要搞另类，大家会不明白的。</p><p>下例中将self改为myname一样没有错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    def _init_(myname,name):</span><br><span class="line">        myname.name&#x3D;name</span><br><span class="line">    def sayhello(myname):</span><br><span class="line">        print &#39;My name is:&#39;,myname.name</span><br><span class="line">p&#x3D;Person(&#39;Bill&#39;)</span><br><span class="line">print p</span><br></pre></td></tr></table></figure><p>self指的是类实例对象本身(注意：不是类本身)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    def _init_(self,name):</span><br><span class="line">        self.name&#x3D;name</span><br><span class="line">    def sayhello(self):</span><br><span class="line">        print &#39;My name is:&#39;,self.name</span><br><span class="line">p1&#x3D;Person(&#39;Bill&#39;)</span><br><span class="line">p2 &#x3D; Person(&#39;Apple&#39;)</span><br><span class="line">print p1</span><br></pre></td></tr></table></figure><p>如果self指向类本身，那么当有多个实例对象时，self指向哪一个呢？</p><p>总结:</p><ul><li>self在定义时需要定义，但是在调用时会自动传入。</li><li>self的名字并不是规定死的，但是最好还是按照约定是用self</li><li>self总是指调用时的类的实例。</li></ul><p>转载自：<a href="https://www.cnblogs.com/chownjy/p/8663024.html" target="_blank" rel="noopener">Python类中的self到底是干啥的</a></p><h2 id="python-数据类型转换-dtype"><a href="#python-数据类型转换-dtype" class="headerlink" title="python 数据类型转换 dtype"></a>python 数据类型转换 dtype</h2><p>b = a.astype(int)<br><a href="https://www.cnblogs.com/hhh5460/p/5129032.html" target="_blank" rel="noopener">https://www.cnblogs.com/hhh5460/p/5129032.html</a></p><h2 id="python一些函数"><a href="#python一些函数" class="headerlink" title="python一些函数"></a>python一些函数</h2><table><thead><tr><th align="left">函数</th><th align="left">解释</th><th align="left">例子</th></tr></thead><tbody><tr><td align="left"><a href="http://www.runoob.com/python/python-func-zip.html" target="_blank" rel="noopener">zip(a,b)</a></td><td align="left">把两个list中对应的元素打包成元组，返回list</td><td align="left">a = [1,2,3],b = [4,5,6], zipped = zip(a,b)     # 打包为元组的列表 out: [(1, 4), (2, 5), (3, 6)]</td></tr><tr><td align="left"><a href="https://www.yiibai.com/python/number_randrange.html" target="_blank" rel="noopener">randrange(1,100,2)</a></td><td align="left">randomly select  an odd number between 0~100</td><td align="left">randrange ([start,] stop [,step])</td></tr><tr><td align="left"><a href="http://www.revotu.com/matrix-transpose-in-python.html" target="_blank" rel="noopener">Python矩阵转置方法</a></td><td align="left">先用zip并行迭代每一个列表元素，然后再用map将结果中的元组转成列表。</td><td align="left">list(map(list, zip(*matrix)))</td></tr><tr><td align="left">list.append()</td><td align="left"></td><td align="left">list1 = [‘C++’, ‘Java’, ‘Python’], list1.append(‘C#’), print (“updated list : “, list1) output: pdated list :  [‘C++’, ‘Java’, ‘Python’, ‘C#’]</td></tr><tr><td align="left"><a href="https://blog.csdn.net/weixin_35955795/article/details/52448345" target="_blank" rel="noopener">matrix.count(3)</a></td><td align="left">count() 方法用于统计某个元素在列表中出现的次数</td><td align="left"></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
