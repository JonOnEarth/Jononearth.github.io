<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2,3月每日一吐</title>
      <link href="/2020/05/02/2-3%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/"/>
      <url>/2020/05/02/2-3%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/</url>
      
        <content type="html"><![CDATA[<h2 id="202003"><a href="#202003" class="headerlink" title="202003"></a>202003</h2><h3 id="20200310"><a href="#20200310" class="headerlink" title="20200310"></a>20200310</h3><p>老外跟我室友竟然认为，反正最终大家都会被传染，破罐破摔的感觉。这帮人，心真大啊。</p><h3 id="20200317"><a href="#20200317" class="headerlink" title="20200317"></a>20200317</h3><p>短短几天，疫情已经很不容乐观了。真没想到，这么快就在美国大流行起来了。现在基本能关的都关了。除了餐厅还在支持take out 和 delivery服务。没有国内的严格控制，还是有很多人在外面工作，尤其是快递员可能已经忙的不可开交。<br>马路上再也看不到多少人了。不过奇怪，有时还能看见有人跑步，没带口罩，心是挺大的。<br>国内老姐给寄的口罩现在被扣下了，打客服电话几十分钟没人接…</p><h3 id="20200318"><a href="#20200318" class="headerlink" title="20200318"></a>20200318</h3><p>实验室的西班牙姑娘心真大。欧洲航线基本关闭的情况下，硬是坐上飞机，飞回西班牙了。不知道她的决定是正确还是鲁莽，这会美国的疫情确实也非常严重。  她说宁愿得病，也想回家跟家人在一起…</p><h3 id="20200319"><a href="#20200319" class="headerlink" title="20200319"></a>20200319</h3><p>在家工作有段时间了，效率有时很高，有时低的吓人，加上拖延症，干脆不工作休息了。不得已，拾起了番茄工作法，保证1小时内，工作集中。慢慢的，注意力越来越集中，工作也变得走向正轨。<br>不过一天下来，平均也就完成6，7个番茄。从早上9点起床到晚上12点，之间15个小时，竟然只工作7个小时。剩下的8小时呢。来，算下，9点到1点工作3小时（刨去吃喝拉撒时间），中午吃饭休息（刷剧）到4点，然后4点到10点，工作3到4个小时，刨去（吃喝拉撒娱乐）。然后刷剧到12点睡觉…</p><p>不得不说，在家工作无人打扰，番茄工作法效率还是蛮高的。但你看看玩的时间竟然超过了工作时间，这一点不可原谅。至少要工作8个番茄吧。</p><h3 id="20200325"><a href="#20200325" class="headerlink" title="20200325"></a>20200325</h3><p>最近一直在循环陶喆的歌。还是他们很喜欢他们的歌，不禁很想吐槽现在的歌坛，上榜的都是什么“神曲”。时代变的更好，但00到10年的歌坛黄金10年也却不可复制了。如今，歌曲已经沦为视频的附庸了，一般很火的歌曲，不是神曲，就是以视频取胜…</p><p>话说杰伦你啥时候出新专辑啊！</p><h2 id="202002"><a href="#202002" class="headerlink" title="202002"></a>202002</h2><p>dailynote 10<br>读书笔记：2（很多篇待更新）<br>技术博客：0</p><h3 id="20200208"><a href="#20200208" class="headerlink" title="20200208"></a>20200208</h3><p>博客终于重写上线了。博客不在的日子，感觉自己失去了整理自己想法和学习的地方，很乱糟糟的，不好不好。</p><p>室友搬走有一周了。每次对于生活的变化，总是伴随着落寞和期待，以及改变到最终的恢复。<br>落寞可以是朋友的离去，期待是对新的生活的憧憬。改变可以是好是坏，而这次的改变貌似有点脱缰；恢复，今天开始恢复写博客，就预示着正常规律生活的即将回归。</p><p>生活有变化，会对生活习惯产生一点影响。对俗人的影响更深。凯哥（好朋友）曾经说过，他喜欢跟人交往，因为不论什么时候，他都在做自己，很自在。所以生活的变化对他来说，不会产生重大影响。而对于我这样的俗人，跟人社交，有时候就会压抑自己，做自己太难了，只有在家一个人的时候，才会放下一切做自己吧。可惜这个时候，做的自己有点过了头，有点放纵。就像橡皮筋，拉力太大，反弹力也会变大。</p><p>“自私”一点，时刻想着去做自己，做一个更好的自己，而不是一个委屈的自己。</p><p>可惜我们太多人，晓得要做更好的自己，不做委屈的自己，却不知道该怎么做。</p><h3 id="20200208-1"><a href="#20200208-1" class="headerlink" title="20200208"></a>20200208</h3><p>今天听到老爸老妈他们说我们那的小县城竟然有6例了…吓的我一激灵，赶紧给他们打了个电话，好在他们表现的还比较乐观，对国家也比较有信心。跟他们聊了特效药很快就会面世，或许很快国内就能恢复了。</p><p>我现在身处国外，跟国内不能感同身受，一颗心但也总揪着，看到国内大家众志成城抗击病毒，也想做点什么。捐钱或许是最简单的方式，也想买口罩寄回去，可是需要花费的金钱和精力可能远远超出了现在的能力。有句老话：穷则独善其身，达则兼济天下。前一句不苟同，但却觉得下一句很有道理。穷人可是有穷人的做法，捐一点钱，做做义工等等。但只有你真的有影响力，实力变强的时候，你才能去做，能做兼济天下的事情。</p><p>修小我，才能成大我。小我都做不好的人，根本没有精力，更没有能力去做大我的事情。</p><h3 id="20200209"><a href="#20200209" class="headerlink" title="20200209"></a>20200209</h3><p>我们需要斗争精神。尤其是跟自己斗争的精神。每天回来，紧绷的精神一下子松懈了下来，开始放松自己，逐渐不受控制走向放纵，看剧，玩游戏，一直到很晚才睡去。我们脑子里面都有一个及时行乐猴，属于动物的本能。当我们甘愿把自己下放给动物本能时，属于人的主动思辨就会越来越弱。</p><p>我们需要跟自己的动物本能做不懈的斗争。不想去干什么必须要做的事情，很有可能就是本能脑畏惧困难。意识到本能脑在运作，就是我们一种高级脑的抗争。<br>也许有人会说，一种用我们的大脑，会很累的，需要用本能脑来调节休息一下。我之前也一直认同，其实并不然。有一点冥想经历的人都知道，我们会有感知我们的感知。换一句说，就好比自己长了另一只眼睛，用来观察自己当下的所思所想，不加任何判断和情绪。你如果试一下，这并不会很累，还可以让你注意力越来越集中。反而注意力的涣散更多的消耗精力。</p><h3 id="20200211"><a href="#20200211" class="headerlink" title="20200211"></a>20200211</h3><p>恢复了小组会议，以为自己准备充分，会收到良好的反馈，可惜不断的被问各种问题，最后打击的不想多讲话了。2个人开会，大家的频率很快就能在一条线上了。但人一多，而且小组里还有一个特别有质疑精神的美国人。 很讨厌被质疑啊，更讨厌被别人牵着鼻子走。开会到最后，然后他们像分配任务式的，叫我试试这个那个。我去了，最终小组会议他们像达成一致，而我这个项目负责人成了执行者。</p><p>从小组会议中，我会得出一个粗浅的结论：多提问，多质疑，即使自己一开始一无所知。然后自己的积极性越来越高涨，从而对会议话题产生更多的主导性。<br>怎么破局。  </p><ol><li>最简单的也是最不可能的办法，撇开老美进行会议。他在里面产生的正向作用比逆向作用更大还是相反，我其实是不确定的。但目前看来，对于我产生逆向作用更多些。对于一个掌控欲很强的人来说，很难接受别人的指挥。</li><li>自己化解。现在产生这种心理，很怕是自己的原因：自己没有好好准备；自己没有意识到他们并没有你想象的那样，“项目基本知识”都会；自己的心态也不对，不应该对他们的质疑产生逆反的心理。</li></ol><p>既然无法撇开老美开会，那么只能调整自己，跟他们“斗”下去了。1，每次开会确定目标，尽量去想更多会议中会碰到的事情，从而对事情发生有所预期；2，好好准备，从框架，到支撑框架的基础也要了解；3，对于被质疑要充分了解，不可避免。话术上要学会怎么应对，心理上要淡化质疑而要强调是他们的反馈。4，整理他们的反馈，跟自己的目标是否达成一致，如果达成一致很好，如果没有，那继续死磕。</p><h3 id="20200214-一万个早起的方法"><a href="#20200214-一万个早起的方法" class="headerlink" title="20200214 一万个早起的方法"></a>20200214 一万个早起的方法</h3><p>今天跟朋友们聊到自己被逼的4点半起床，分享了几个个起床小技巧。一下子引起了共鸣。他们也分享了很多技巧，尽管对他们大多数时候不奏效。  </p><ol><li>闹铃设置。设置2个闹铃，一个轻柔放在枕边，唤醒你就可以。一个闹腾放在很远的地方，最好能被他人听见，这样逼迫你赶紧爬起来关掉它，不至于影响他人。闹铃间隔时间5分钟为宜。</li><li>闹铃音乐设置。看了科学报道，节奏感强的音乐更容易唤醒你，而且唤醒之后更有精力。所以不要那闹腾的音乐，一下子被震醒的，会很痛苦。  </li><li>喝杯温水或者咖啡。这个也是有科学依据和经历的，很有效。就是不要倒在喝热水的路上为好。</li><li>设置4，到5个闹铃，一直到洗手间。我不太喜欢这个方法，太闹心，太繁琐。</li><li>在闹铃前，热好咖啡。买一个定时煮咖啡的机器，闹铃一响，直接过去喝咖啡，形成一个不需要太多动作的惯性。喝完一个很难睡着了吧。</li><li>把最喜欢做的事情，放到早起来做。比如说什么阻止了你睡觉，王者荣耀？那你要不试试早上睁眼玩它。</li><li>制定一个简单明确的计划。早起干什么，如果没有目的，很难起。如果目标不明确，一样也困难。早上大脑刚启动，本能脑暂居主要地位，那种早起的激情很大可能是激发不起来的。如果有一个简单明确的可重复的计划，比如说写日记，对于某些人来说甚至不用调动主动性就可以早起实施。</li><li>最重要的早起方法，应该是早睡。</li><li>通过简单的动作逐渐从本能脑中恢复。如拿手机玩你最喜欢玩的游戏，或者找人聊天，回复短信，邮件等等。</li><li>希望大家可以告诉我更多的早起方法…</li></ol><h3 id="20200220-读书：4小时工作法"><a href="#20200220-读书：4小时工作法" class="headerlink" title="20200220 读书：4小时工作法"></a>20200220 读书：4小时工作法</h3><p>我们都特别羡慕别人的高效行动与结果，一到自己就不行了。书中作者提出了怎么做一个高效的新贵。需要很强的时间管理能力。他主要提出了2点准则。</p><ol><li>二八法则。专注做那些能产生高产出的事情。拒绝和放弃那些低效的活动，如看肥皂剧，无效的社交等等。甚至工作中，一些低产出的客户可以放弃掉</li><li>帕金森法则。文中举了个有帕金森病的老奶奶为例，寄一封信，花了大量的时间挑选信封，邮票等等。而年轻人可能5分钟就搞定了。作者这里强调的是老奶奶因为“时间充裕”，简单的事情被各种情况分神。生活中，如果我们有一个deadline,怎么着你都会想办法在deadline之前完成工作。所以我们需要人为设定deadline，这样才能保证高效率。</li></ol><p>作者还说了一些其他的技巧，怎么保证你工作时间很短，却能产出很大。如外包自己的工作，只需要告诉他们需要做什么。如信任自己的手下，放权给他们去做。如利用一些信息差，实现高效的产出等等。</p><h3 id="20200223"><a href="#20200223" class="headerlink" title="20200223"></a>20200223</h3><p>在美国，流浪汉真的是非常常见。其中大部分是正常人，只有少部分是有精神问题的。对于我们国家的人，有时候就很难理解发达的美国这种奇怪的现象。有点类似他们不理解我们，为啥拐卖儿童竟然大量存在中国。只能说国情不一样，家家有本难念的经。</p><p>对于流浪汉大多数的人是避之不及。正常，确实存在一定的风险。但他们以此为生，如果他们没有乞讨到钱，就很难活下去。有人说，有其他人会给他们钱的。这就像你要去捐款，有人告诫你不用捐款了，那点钱不起什么作用。那如果人人都是这么想的，哪还有人捐款。</p><p>现在会刻意的在身上准备1刀2刀，机会合适的时候，给他们。希望未来有钱了，能够做到更好。之前一个朋友，跟我说，她会给一些流浪汉买吃的，但从不给钱，因为很多人是去买大麻。。。</p><p>看了个关于流浪汉为什么不找工作的视频。说出了很多流浪汉的心声，有道理，但在外人看来，是比较容易破局的。但事不关己，高高挂起，没法设身处地感受。人真的是很难跳脱出环境的限制。就像谈对象的两人，在一起做了很多傻事，身在其中的时候，根本都没有察觉。一旦分开了，才意识那些傻。</p><h3 id="20200227"><a href="#20200227" class="headerlink" title="20200227"></a>20200227</h3><p>健康很重要，是你做其他事情的基础。作息要正常，保持健身习惯，不吃辛辣食物，尽量清淡饮食（口味目前还是有点重），要读书，要冥想，要早起。。。<br>还有不要拖延，每次的压力山大基本都是拖延导致的，压力一大，身体就出现问题，老了，你不能年轻那么折腾了吧 :cry:</p><h3 id="20200229"><a href="#20200229" class="headerlink" title="20200229"></a>20200229</h3><p>大多数人对那些没有对自己产生影响的事情，总是表现的很漠视。现在新冠病毒在美国也非常危险了。但是大多数的人却总是抱着侥幸心理，其实也包括了我。但我还属于比较注意的，我的好几个朋友，却不听劝告，该干嘛干嘛，一点都不上心。我的室友竟然抛出了这个病其实容易治好的理由，不会致死。<br>我的天呐！之前以为他只是不拘小节，现在连自己的生命都”不拘小节“。或许真到对他产生更大影响的时候，才能意识到？</p><h3 id="20200229-读书：麦肯锡教我的思考武器"><a href="#20200229-读书：麦肯锡教我的思考武器" class="headerlink" title="20200229 读书：麦肯锡教我的思考武器"></a>20200229 读书：麦肯锡教我的思考武器</h3><p>听来的书总归是浅的。姑且学习到能学到的东西吧。本书给我的感觉，就是做事一定要多思考，埋头苦干是逃避的，轻松的，低效的。 </p><p>哪些东西需要思考。首先做事情之前，思考这个事情（议题）：值不值得，议题的质量更重要，不值得在不值得的议题上花费大量的时间，当然判断议题的质量需要你调动你能调动的知识，眼界以及各种资源去思考。所以经过深思熟虑之后，做事情其实已经不是难事了。<br>有一些建议，比如说学会做减法，并不是越多越好。宁愿做1件120分的事情，也不做10件60分的事情。</p><p>思考好议题后，就该做事了，但做事也要思考，尤其是效率的问题。1思考做事的routing，理清楚，重点是啥；2敢于舍弃；3勿陶醉很努力的假象,警惕假装思考人</p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>疫情下每日1吐4月</title>
      <link href="/2020/05/02/%E7%96%AB%E6%83%85%E4%B8%8B%E7%9A%84%E6%AF%8F%E6%97%A51%E5%90%90202004/"/>
      <url>/2020/05/02/%E7%96%AB%E6%83%85%E4%B8%8B%E7%9A%84%E6%AF%8F%E6%97%A51%E5%90%90202004/</url>
      
        <content type="html"><![CDATA[<h2 id="202004"><a href="#202004" class="headerlink" title="202004"></a>202004</h2><h3 id="20200402"><a href="#20200402" class="headerlink" title="20200402"></a>20200402</h3><p>国内姐姐给我邮寄的口罩终于到了，现在美国物资极端紧缺，而缓过来的中国，口罩也变得不那么供不应求了。</p><p>我之前一个月前从亚马逊订的口罩也在同一天到达，而我姐过我邮过来，只花了一周…很想吐槽。打开一看，亚马逊买的口罩质量跟我姐邮过来的要差劲好多，但是价格却贵多了啊…</p><p>亚马逊疫情期间，很多价格都乱了，以前很便宜的东西，因为资源短缺，涨到离谱。比如我想买的午餐肉罐头…资本主义，根据需求来定价，经济规律果不其然。</p><h3 id="20200412"><a href="#20200412" class="headerlink" title="20200412"></a>20200412</h3><p>美国的疫情日渐严重。线下业务除了快递行业，其他差不多都陷入了萧条。Amazon以及一个叫instcart的公司，业务暴涨。尤其是后者，之前根本没人知道，现在基本上人人都要用。这是一款帮助人去超市采购东西，然后送过的APP，简单易用，不用出门。简直不能再火了，很多时候，需求太大，都没有派送窗口。一星期前订，一星期后派送都属于正常。</p><p>他们都在扩招采购员，仍然供不应求。跟我接触的快递员，外国人基本没有戴口罩的，华裔基本都戴着口罩，这你或许就能解释一二为什么亚裔在美国感染率那么低了。</p><p>自从我有了一些口罩，老外快递员给我送货的时候，我就会给他们一些。他们不戴一是因为没有习惯，二是没有。所以不管怎么样，除了给了他们比平时多一点的小费，也把口罩给了他们点，希望他们进货的时候，对自己也对他人多负点责任。</p><h3 id="20200413"><a href="#20200413" class="headerlink" title="20200413"></a>20200413</h3><p>麻州已经突破了2万多人，全美快60万了吧。家里人十分担心，老爸每天一条短信确保我是否安全。更有亲朋还有，询问情况。每次只能说我比较安全，不出门，就呆家里，很安分，所以基本不会感染。<br>之前室友不是很在意，现在他可能也被家人反复的告诫和严重的疫情情况，给反应过来，不在朝外面跑了。有时候还真是像吐槽下这个新室友，但想想没必要，无非就是跟大学生活是一样的，同一屋檐下，两个性情，生活习惯不一样的人一起生活，是真需要很长时间去磨合的。<br>而我也是很不喜欢去说别人，更不喜欢别人说我，想来想去，下次就别找室友，你自己住吧，孤独终老适合你。</p><h3 id="2020415"><a href="#2020415" class="headerlink" title="2020415"></a>2020415</h3><p>国内的一文青好友yan找我“约稿”。<br>“国内的方方日记火了”，他说，“尤其是在国外，现在国外要出版这本书，借此再次污名化中国…”<br>他说现在美国疫情严重，叫我也写写在美国抗疫的事情，方方也是这样写的，不过到后面有失偏颇，道听途说了。</p><p>我说好啊，正好有写日记的习惯，尽管拉下好多没写了。<br>“等你写好了，咱来国内出版，没人出版，我帮你出”。<br>”我对美国抗疫的事情很好奇，国内的新闻也不知道真假，就是现在美国生活在水深火热之中“。<br>”美国人还不带口罩吗？“<br>…</p><p>他以为我在美国的主旋律是抗疫，其实主旋律还是工作学习，每天忙碌和愁的也总是工作的事情。抗疫就是呆在家里不出门。<br>”这样会不会没什么可写的“，我跟他这样说。</p><p>”没事，就写写见闻，方方不好’道听途说‘写了那么多, 文笔也没关系“。</p><p>”好的，那我就写了“</p><h3 id="2020416"><a href="#2020416" class="headerlink" title="2020416"></a>2020416</h3><p>卫生纸快用完了。现在真变成硬通货了都。买了好几次，送过来都说缺货，缺货，缺货…<br>天呐，卫生纸从2月底就开始抢购，大家还没有抢购完吗？到现在还是是缺货状态。</p><p>美国人一到特殊情况，就开始囤厕纸，貌似都快成了传统。虽然大家都说笑着，没必要，但你不抢的代价就是，当你需要的时候，总是缺货的尴尬。</p><h3 id="2020418"><a href="#2020418" class="headerlink" title="2020418"></a>2020418</h3><p>早上睡眼惺忪，打开手机看见一则新闻，川普又在开始朝中国推卸责任了。说中国正在修订武汉的死亡人数，美国不是死亡最多的国家，肯定是中国，中国有那么多的人口，怎么可能死亡比美国少。</p><p>气的一下子就激灵醒了，难怪美国人都骂他妈的川普就是个没脑子的，就知道推卸责任。难怪奥巴马再也看不下去，公开支持拜登当下任总统。<br>他妈的就知道每天变着花样的推卸责任。</p><p>我还是有点愤青的吧？我在反问自己。特别记得刚上大学那会经常光顾军事网站，妥妥的一个五毛愤青。后来惊醒，现在也挺怕自己不知不觉陷入到狭隘的民族主义当中。</p><p>之所以有此一问，是希望自己骂川普不是因为狭隘，而是因为他真傻逼。</p><p>新闻媒体，我是不太敢全信的。我更相信他们，是为了某种目的而做的报导和宣传，不管这个目的是好是坏。所以决不能因为听信了什么，就理所当然的接受，更不能受他们利用做出些冲动的事情。不过有时候，这也很难说，很多事情分不了对错，也没法不选边站，如果你独立中庸，作为群居动物的人类来说，应该是很难做到的。</p><p>就拿美国来说，怀疑批判主义都用到外国人英语托福考试当中了。他们也自诩自己自由，具有怀疑精神，其实还真有很多值得赞赏的地方。不过大多数的人，也还是被新闻，被政客给忽悠来忽悠去，他们也利用你所谓的自由，怀疑主义来洗脑你。自由什么的，神圣不可侵犯，要什么经济，要什么和平，生命尤可抛。中国政府一定在践踏人权，一定在隐藏什么。<br>美国国民呢，其实有很多”新时代的文盲”，有直接信的，也有怀疑的，但接触到的“灌输”实在太多，他们的怀疑早就非常带有偏见了。</p><h3 id="20200419"><a href="#20200419" class="headerlink" title="20200419"></a>20200419</h3><p>今天来说说美国人不戴口罩的问题。不仅仅是美国人不戴，是整个西方社会都不喜欢戴口罩。</p><p>看过一张图片中美超级英雄的对比，还真有点东西方文化的差别。美国队长，蝙蝠侠，绿箭侠，闪电侠…都只蒙上眼睛，面部从来是不遮的，要遮就全遮住，像钢铁侠蜘蛛侠那样。而中国素来有蒙面大侠的称呼。一个乐于展示，一个不希望别人认出，你说有没有点中西文化的差别的意思。而且近些年来，口罩文化在亚洲随着渐渐流行了起来，防病防寒防雾霾，防明星被认出，更有以戴口罩为时尚的趋势。</p><p>而这次疫情期间，美国乃至西方社会不愿意戴口罩的问题，却不仅仅是因为文化的原因。更多的原因是政府的宣传。政府从一开始，就不认为口罩会对疫情有什么很好的帮助。之所以得出这样的结论，是因为口罩是否有助于疫情控制的争论从很久很久之前就一直争论不休，然而却没有什么结论。</p><p>而且也更多是因为迫于产能的压力，政府给民众宣传，口罩没必要的论断，呼吁他们不用去买口罩，抢夺医务资源。你要知道，直到现在，医院的口罩，防护服还处于严重短缺的状态。而全美现在大约有上万被感染的医护人员，想想跟国内的对比。</p><p>我想其实更多专家应该是支持戴口罩有助于抗疫的论断，无他，很简单的一条：如果口罩没用，你医院戴什么口罩啊。所以到如今疫情越来越严重，政府终于开始号召戴口罩，但自己制作也可以的，不要去抢夺医务资源。你看一瞬间，社交媒体上，大家晒起了自己制作的各种口罩，以及各种口罩艺术创作油然而生，俨然成了一种流行。真香。</p><p>所以政府宣传真的很重要。宣传有了，就不会出现老外呵斥戴口罩的华人滚出超市的无稽新闻了。</p><p>但为什么不早点宣传呢？除了以上我说的一些原因。还有一层更深的原因:人类这个动物，自己如果不知道痛，永远不会长教训的。中国以及亚洲也是经过了SARA之痛，口罩防疫已经深入到政府和老百姓的心里，而西方刚刚才知道痛。</p><img src="/images/boston_mask.jpeg" width = 70% height = 70% div align=center /><img src="/images/boston_mask2.jpeg" width = 70% height = 70% div align=center /><h3 id="20200420"><a href="#20200420" class="headerlink" title="20200420"></a>20200420</h3><p>4月18号举行的世界群星演唱会：One world: Together at home，邀请了世界上著名的歌手，歌唱家，音乐家等等一起为新冠疫情筹集资金。由世界卫生组织和世界公民组织举办，Lady gaga牵头举行的线上直播演唱会。</p><p>疫情之下，公众的活动受到了限制。演唱会希望通过这种方式给世界民众送去慰藉。我没有看这场演唱会，因为太长了…但我看了几个片段，Lady Gaga演唱的《smile》，她笑起来时真的很美。郎朗与几位世界歌唱艺术家合作的《The Prayer》，不禁让人停下，仔细聆听，仿佛能抚平所有的灾难…</p><p>这个活动是一个了不起的活动，谁说演唱会一定要有绚丽的舞台，热歌和劲舞。感召人心的歌曲与艺术更能深入人心。这就是音乐的力量吧。</p><p>这次活动让我知道了世界公民（Global citizen）这个国际组织。他们的宗旨是扶贫。喜欢这个国际化视角的组织，和平与发展应该是人类的共同主题，帮助他人，实现共同进步，我去，这不就是我们伟大祖国的宗旨吗。</p><h3 id="20200422"><a href="#20200422" class="headerlink" title="20200422"></a>20200422</h3><p>今天又是一个万里无云，阳光普照的一天。总是不自觉的迈开步子走到窗前（其实就2，3步，屋子比较小），看向窗外。窗外蓝天白云，窗外微风袭人，窗外鸟语花香，窗外…咋这么多人。</p><p>疫情让原本热闹的街道，变得冷冷清清。可对于完美的下午天气，你不出来对不起自己。所以你能看到，三三两两的人开始活跃躁动起来，有说有笑。不断的有在我们学校标志那拍照的，更有甚者，一家老小在学校的草坪上开始野餐起来，躺在那，嗮太阳。</p><p>我去，心好痒，但出去一趟太麻烦，装备，来回消毒，而且耳边响起父母的每天叮嘱，“不要出门，不要出门，不要出门…”</p><p>那只好，我站在窗口，看窗外的人，成为我的景。<br><img src="/images/campus.jpeg" width = 70% height = 70% div align=center /></p><p>我们老板是个西班牙人，开会的时候，总会问我们有出去透气没，还郑重其事的说，这很重要，保持你的头脑清醒。果然老外的想法是不一样的。而国内的老爸老妈呢，每天叮嘱，恨不得让我去楼下扔垃圾，都不要去。</p><h3 id="20200423"><a href="#20200423" class="headerlink" title="20200423"></a>20200423</h3><p>今天看了学校的邮件，学校开展了COVID-19的研究项目。</p><h3 id="20200424"><a href="#20200424" class="headerlink" title="20200424"></a>20200424</h3><p>纽约市长现在每天的记者会基本上就是日常怼川普，发牢骚，为什么联邦政府怎么怎么样，导致了什么什么样。也真是蛮难的，物资各种紧缺，向联邦政府发牢骚很正常（确实川普做的很差劲），推卸推卸责任，在民众面前，连线记者弟弟的来几句兄弟情，安抚下纽约市民的不安情绪。</p><p>在美国的做一个政客，首先必须得是个能上镜头的人。对于个人主义盛行的美国，没有一点政治经验也是能够当上总统的，只要你能在镜头面前忽悠住民众。希拉里作为一个长期的政治家，却很不受美国民众待见，即使他在镜头前说的再好。而川普凭借一波路人转正的操作，新鲜出炉，赢得了民众的选票。</p><p>美国政客依赖于选民，所以他们总是在镜头前卖力讨好选民，虽说没做什么实事，但这种形式还是比国内好很多（政客像上级领导，不贴近民众，关系饭局一堆）。不管是总统，还是市长，总是会被拉出来接受记者的质询，甚至跟记者吵起来：“fake news”, “You are a bad person”(来自川普)。</p><h3 id="20200425"><a href="#20200425" class="headerlink" title="20200425"></a>20200425</h3><p>滑天下之大稽，最近美国有一个州复工了，之前支持复工的川普都批评说复工太早了。</p><p>随着疫情越来越严重，貌似美国的抗疫之路还有很长时间要走。之前各个州制定的复工期限，肯定是需要延长的。但真正实施起来的难度，真的很大。</p><p>美国是一个很矛盾的国家，不仅仅是美国，欧洲很多所谓的发达国家也是。明明有着比中国更好的医疗条件，可惜面对疫情，远远赶不上中国的应对。他们的医疗是很好，但不是给所有人的，他们的物资分配更是不足以应对很多人，所以经过这件事之后，必定会更加喊着把制造业从中国撤回到美国。</p><p>美国的贫富差距也比较大。对很多人来说，如果真的几个月不复工，他们就真没钱生活了。对于没有存款的他们来说，感染的风险跟没饭吃相比，真的不值一提。</p><p>由复工想的比较远了。<br>总体来说，西方社会是发达的，但是是不稳定的。民众并没有生活的很安稳，他们有晚上不单独出门，或者不要去危险的地方的习惯。有一些恐怖分子，各种帮派火拼。怎么说呢，个人觉得西方社会一直有着战斗，面对死亡的预期。所以对于疫情，或者复工的个人来说，并没有国人想象的那么害怕。</p><p>而在亚洲，尤其是中国，国家把人民保护的好好的，人民安居乐业，一心求发展，生命危险的心理负担并没有多少。当然在外界看来，这种所谓的发展是牺牲了“人权”，“自由”。国内跟国外完全是不同的心理视角。</p><p>面对日益全球化的今天，两种不同的视角势必会越来越冲击对方。博弈的双方，很难说有赢家，不管怎么说，中国的道路任重道远，逐渐开放，走向世界是必然，在保持自己文化，用自己的文化去影响世界的同时，也需要一个国际化的视角去兼容并蓄。</p><h3 id="20200428"><a href="#20200428" class="headerlink" title="20200428"></a>20200428</h3><p>美国多地方举行了示威游行，反对政府的隔离措施。你看那些新闻画面，你就忍不住气骂这帮人，怎么还会有那么多傻逼，不顾自己的生命，不顾他人的生命，去争取他们所谓的自由呢。首先我不认为他们争取的是什么“自由”。就像一个倔强偏执的孩子，认定了一件事情，为认定而认定，而行动。然而孩子的认知水平肯定是看不到更多的东西，所以有时候，家长一般会采取一些强硬的措施，也有些聪明的家长会通过其他手段，让孩子认识到事情的正确性，从而孩子可以走向正常的发展道路。</p><p>但西方其实是会放纵这些“孩子”，跟他们从古到今的哲学思想也不无关系。他们说，人人都有说话，争取自由的权利，不管你是基于高见，还是基于浅薄，无知。其实我是同意的这样观念。并不是只有有远见的人才有发表意见的权利，没什么认知的人也有表达的权利，完全认同。但作为一个前进目标或方向，有时候必须要有所抉择，根本没法兼顾。有人说可以通过沟通，让信息实现对等，那大家就可以目标一致了。我也完全同意，当然这种情况最好，但如果这张网络节点过多，实现信息对等，不说消耗的通信成本有多少，况且还会有人说你在洗脑大众。</p><p>其实不难看出，人类前进的方向，总是由那些真正的“精英”带领的，由此可以看出尊重你表达的权利完全没问题，但怎么做，还是需要选择一个接近最优解的路线来实施的。</p><h3 id="20200429"><a href="#20200429" class="headerlink" title="20200429"></a>20200429</h3><p>今天终于开始看方方日记，果然是文协主席，文笔没话说，没有绚丽的语言，只是朴实的记录每天所见，所听，所感。记录每天的小事，于小事中感受方方眼中的武汉疫情。<br>。</p><p>我们生活中看惯了“表面”，“似是而非”的文章，从而接触到的消息都是别人过滤了好多层的信息。只有在平实的叙述中，最好不带个人情感色彩的文章，才可称得上中正。可惜我的日记不是，方方的也不是。如果你看了我多篇文章，应该可以看出我的日记带有强烈的个人想法在里面，方方日记里面也有。所以这些想法，是我过滤过的信息，大家应该抱着怀疑的态度接收这些信息。</p><p>其实也怪不得我，每天接触的事情就是那些，甚至有时一直在工作，记录一两次工作可以，记录很多次，那该是多无聊的事情。如果不在里面加入一些自己的想法，或者情绪来增加一些变量，“提升一下文章的深度”，那得是我小学写的日记吧。不知道方方是不是有相同的”顾虑”。</p><p>我的日记形式跟方方日记还是有很大不同的。<br>我的日记形式很简单，一件事，有趣的，或可引发思考的，我就会写下来。话说我这个人还是很懒的，其实应该为了写作而去多多体验。回想起来，去过的美国大部分的地方，还是刚来美国前两年，甚至大多数是第一年经历的。熟悉之后，就是从好奇回归冷漠吧。甚至熟悉都说不上，就是形成了一个自己的“舒适圈”了吧。</p><p>有时候，我觉得作家都是悲观的，他们特别容易去看到社会的阴暗一面。这也说得通，就像新闻一样，发生了突发事件才可以是爆炸新闻。如果大家都写每日习以为常的事情，歌功颂德，那样的文章，“影响和营养”都是不多的吧。</p><p>存在即合理。这个社会需要有人看到阳光，也需要人看到黑暗，这样的社会才是可以进步。互联网让信息交互变得特别容易，这也间接造成了偏于简单的信息容易被传播（知道为什么谣言那么容易传播了吧）。特别需要警惕的是，被裹挟的民意。著名外交家<a href="https://mp.weixin.qq.com/s/zwZzL2g84Aw8AGkPzVDv7Q" target="_blank" rel="noopener">袁南生先生</a>有一句话让我印象深刻，“了解民意，尊重民意，不唯民意”。中国的精英政治形式(from 张维为教授)，我也觉得是优于美国甚至部分西方的地方。话语权应该每个人都有，但没有节制的自由不是自由，是放纵自私。这里面的“度”真的是很难把握，就像机器学习一样，一个复杂的函数，我们想要找到最优解是非常困难的，永远对一些点误差较小，有些点误差较大。想要一个最优解，我想只有等人类绝对理性的时候，那可能是另外一种物种了吧。</p><p>我去，突然发现治理社会或许也是一个数据科学的问题啊，比如上文说的“度”是个阈值，我们需要收集足够的数据，来发现什么样的“度”是可以本地最优化（Local optimial）的，甚至做到“全局最优化”（Global optimial）。有点反“人性”的感觉，让尤其是一些美国人知道在研究他们，把他们当成数据，估计又要炸开锅了，“神圣自由不可侵犯”。其实这些研究应该都在进行着吧。</p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/02/15/hello-world/"/>
      <url>/2020/02/15/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2019总结大会</title>
      <link href="/2020/01/01/2019%E6%80%BB%E7%BB%93%E5%A4%A7%E4%BC%9A/"/>
      <url>/2020/01/01/2019%E6%80%BB%E7%BB%93%E5%A4%A7%E4%BC%9A/</url>
      
        <content type="html"><![CDATA[<p>3年前的这一天，拖着沉重的躯体来到美国，充满对未知的害怕与兴奋。<br>设想了自己接下来的道路，饱含自信，却源于对环境和自我的无知。  </p><p>2年前快毕业时，仍然对工作与未来产生恐惧，还是准备不充分吗？应该这么说，永远没有充分的时候。   </p><p>彷佛是宇宙听见了我内心的呼唤，竟然给了一个读博的机会：你总认为自己学的不到位，那好了，给你个博士读读，你该没有借口了吧。  </p><h2 id="对于学习："><a href="#对于学习：" class="headerlink" title="对于学习："></a>对于学习：</h2><p>这一年，应该学的很多，从最初听不懂他们的讨论，到现在可以参与进去，并逐级能探索和主导研究的方向。</p><p>我们组特别棒，尤其是老板。西班牙人，做任何事情，从来都是有条不紊，非常具有亲和力，在他手下，想能学到不少学术之外的东西。</p><h3 id="整理下学了哪些个东西呢："><a href="#整理下学了哪些个东西呢：" class="headerlink" title="整理下学了哪些个东西呢："></a>整理下学了哪些个东西呢：</h3><ol><li>这一年主要是在折腾贝叶斯滤波。kalman, uncertainty, GP等。</li><li>代码能力的提升，从很乱，到现在有条理些。</li><li>神经网络现在认知没怎么提高，学会一个bayesian NN</li><li>科研能力提升。论文看的不再很慢，很不懂。</li><li>参加了ion会议，发表一篇会议论文，并演讲，还引起多个的人兴趣，包括Ford, Apple.</li></ol><h3 id="做的太不够："><a href="#做的太不够：" class="headerlink" title="做的太不够："></a>做的太不够：</h3><ol><li>9月份开始，研究基本停滞，停留在原来的研究上。尽管学了新东西，还没有应用到上面。</li><li>平时并没有全心全意放到科研上。这一块下面分析：当压力来的时候，需要调节。需要习惯的力量，不应该依赖自制力。</li><li>博客教程从下半年开始，基本停滞了。没有借口。</li></ol><h3 id="接下来"><a href="#接下来" class="headerlink" title="接下来"></a>接下来</h3><ol><li>一月份之前论文雏形出来，关于bayesian NN训练的；</li><li>4月份之前完成IPIN会议的论文提交；</li><li>找一份关于研究相关的实习（待）</li><li>按照后面做的6小时工作法，进行学习。</li><li>读至少100篇文论，每个月读10篇至少，一周2~3篇。</li></ol><h2 id="对于生活："><a href="#对于生活：" class="headerlink" title="对于生活："></a>对于生活：</h2><p>不知道对于生活，能聊什么。</p><p>喜欢自己呆着，也喜欢跟朋友出去觅食。<br>也不怎么想认识新朋友，但老朋友快走光了都。 </p><p>有一颗想出去浪的心，但没钱，没时间，没朋友:cry:。 </p><p>想搞对象了，但不愿意将就(扯吧就是找不到)。出国前，谁他妈跟我说出国一抓一大把，我一定是留错了学。</p><h3 id="生活得到："><a href="#生活得到：" class="headerlink" title="生活得到："></a>生活得到：</h3><ol><li>找了个学校对面的住宿。虽然破了点，但太方便了吧。非常适合我这种懒人。</li><li>去哪旅游了？刚去了纽约圣诞，让我翻翻相册，还有哪。有在开会间歇看的迈阿密，墙绘涂鸦很赞；有去附近的海边和小岛。然后没了貌似。</li><li>想买个游戏机，但一直没勇气…后来买了个FB的VR眼镜，果然玩了一会现在都不怎么碰了。</li></ol><h3 id="做的不够："><a href="#做的不够：" class="headerlink" title="做的不够："></a>做的不够：</h3><ol><li>生活首先没有规律，经常性的想放飞自我。</li><li>好久没跟老朋友联系了</li><li>没有认识更多的人</li></ol><h3 id="接下来-1"><a href="#接下来-1" class="headerlink" title="接下来"></a>接下来</h3><ol><li>形成习惯，每天早上健身，保证3个小时学习；</li><li>坚持博客创作，每天写在什么地方，可以周末的时候总结。</li><li>去真诚的认识更多的人，也要跟老朋友不时的联系。</li><li>找到对的那个人。</li><li>再强调一遍，有规律，健身，早睡。</li></ol><h2 id="对于创业"><a href="#对于创业" class="headerlink" title="对于创业"></a>对于创业</h2><p>这一块从加入创业团队，到成型，到上线起到了至关重要的作用。<br>做的不够的地方也很多：主要就是没能够花到大量的时间在上面，所以很多功能不是很完善。  </p><h2 id="其他的瞎逼逼"><a href="#其他的瞎逼逼" class="headerlink" title="其他的瞎逼逼"></a>其他的瞎逼逼</h2><h3 id="专注"><a href="#专注" class="headerlink" title="专注"></a>专注</h3><p>我这个人，对什么都感兴趣，但都是3分钟热度。  </p><p>小时候还没有电脑手机，无聊的时候，我自己一个人怎么打发时间。从看小人书，搭积木，到自己挖泥巴组建恐龙战队，搞水泥造四驱车赛道也改装过四驱车，再到自己发明象棋冰球新玩法等等。也写过小说，画过画，吹过笛子，练过‘武’… 我去，原来那时候的童年这么有趣的嘛</p><p>很怕无聊，需要新兴的刺激，但这些刺激都是内驱的，外在的刺激有时候会反其道行之。而且刺激来的快，去的也快。一旦努力，无聊渐渐大于刺激感后，就很难继续实施下去了。</p><p>喜欢创造，然而特别怕陷入创造过后，精细化，专业化的无聊。所以学过很多东西，都半途而废了。浮于表面的总是充满各种欢乐，潜入底层的才能使创造不断延伸开。</p><p>长大了，道理是明白了些。<br>然人却变得越来越浮躁。再也不能如小时候，那般无所事事的仰望蓝天。所有的人告诉你，你要这么做，那样做，不做就落后，不做就失败。看起来你要学的东西太多了，却缺失了专注。</p><p>我们都说要掌握一技之长，然人变得不再纯粹，一技之长再难更长。</p><p>拒绝外在的干扰，太难。什么都不管不顾，又不是一个成年人该有的生活态度。经常萌发这样的想法，自己躲起来，闭门造车数十载，一招惊世。屁，躲不过1个星期。既然躲不过，何不正心对待。</p><p>推崇斯多葛学派：掌控自己能掌控的，做好自己，接下来的不能掌控的就留给老天，不纠结自己不能掌控的事与人。</p><p>2020，请少玩手机，多看书。杜绝睡前不摸下手机，都睡不着觉的坏习惯。</p><h3 id="厚脸"><a href="#厚脸" class="headerlink" title="厚脸"></a>厚脸</h3><p>其实还是做事。厚脸的人，做事更加容易，做自己其实也更加容易。</p><p>我们的文化总是把厚脸作为一种贬义词。但奇怪的是，做人做事成功者，无一不是厚脸者。中国很多的事情，我都怀疑是成功者不想让人后者上位，有意传播的‘歪门邪道’。</p><p>还比如说盛赞老实人…</p><p>厚脸不好做，它意味着你不惧无端指责，意味着你虚心接受批评，意味着你不在乎他人看法，意味着敢于实践不退缩，意味着为达目的‘不择手段’(注意，此处不择手段的对象是自己)，意味着不怕负担责任…</p><p>2020，要把脸涂厚一点，过滤掉别人的看法或者拒绝，正念面对当下。</p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站备忘</title>
      <link href="/2019/12/30/%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98/"/>
      <url>/2019/12/30/%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98/</url>
      
        <content type="html"><![CDATA[<h3 id="更新于2020-2"><a href="#更新于2020-2" class="headerlink" title="更新于2020.2"></a>更新于2020.2</h3><p>想换到其他静态博客平台，hugo等。但折腾了很长时间，还是回到了hexo平台，原因是hexo支持比较丰富，而且格式使用hexo的方式写的，改到其他的平台，实在是改起来力不从心。<br>换了电脑，hexo同步是一大问题，待解决这个。<br>要有一颗谦卑的心，以为自己弄过就会搞，很容易忘掉，浪费精力，重复劳动。  </p><h3 id="Mac-安装注意事项及教程"><a href="#Mac-安装注意事项及教程" class="headerlink" title="Mac 安装注意事项及教程"></a>Mac 安装注意事项及教程</h3><ul><li><a href="https://bowenli86.github.io/2016/01/23/hexo/Hexo-How-to-install-hexo-on-Mac-with-github-pages/" target="_blank" rel="noopener">安装教程</a>，<a href="https://hexo.io/docs/#Install-Hexo" target="_blank" rel="noopener">安装2</a></li><li>安装npm的 permission 错误及<a href="https://stackoverflow.com/questions/33725639/npm-install-g-less-does-not-work-eacces-permission-denied" target="_blank" rel="noopener">解决办法</a></li><li>安装hexo后，需要运行代码npx hexo .. 不是直接hexo</li></ul><h3 id="hexo同步到github"><a href="#hexo同步到github" class="headerlink" title="hexo同步到github"></a>hexo同步到github</h3><p>把整个文件夹同步到github上，换台电脑直接在clone下来，在该文件夹下创建hexo，不会覆盖clone下来的文件，所以直接就可以用了</p><h3 id="书写格式备忘markdown"><a href="#书写格式备忘markdown" class="headerlink" title="书写格式备忘markdown"></a>书写格式备忘<a href="https://www.jianshu.com/p/b03a8d7b1719" target="_blank" rel="noopener">markdown</a></h3><h4 id="插入链接"><a href="#插入链接" class="headerlink" title="插入链接"></a>插入链接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">例1</span><br><span class="line">[markdown](https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;b03a8d7b1719)</span><br><span class="line">例2</span><br><span class="line">[百度2][2]&#123;:target&#x3D;&quot;_blank&quot;&#125;</span><br><span class="line">[2]: http:&#x2F;&#x2F;www.baidu.com&#x2F;   &quot;百度二下&quot;</span><br></pre></td></tr></table></figure><h4 id="插入图片"><a href="#插入图片" class="headerlink" title="插入图片"></a>插入图片</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![](.&#x2F;01.png &#39;描述&#39;)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src&#x3D;&quot;http:...&quot; width &#x3D; &quot;100&quot; height &#x3D; &quot;100&quot; div align&#x3D;right &#x2F;&gt;</span><br></pre></td></tr></table></figure><p>也可以输出百分比多少，width =20%, height=20%</p><h4 id="插入图片带有链接"><a href="#插入图片带有链接" class="headerlink" title="插入图片带有链接"></a>插入图片带有链接</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[![](.&#x2F;01.png &#39;百度&#39;)](http:&#x2F;&#x2F;www.baidu.com)</span><br></pre></td></tr></table></figure><h4 id="Markdown-在-Atom-中的-preview："><a href="#Markdown-在-Atom-中的-preview：" class="headerlink" title="Markdown 在 Atom 中的 preview："></a>Markdown 在 Atom 中的 preview：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shift + ctrl + m</span><br></pre></td></tr></table></figure><h4 id="markdown创建表格"><a href="#markdown创建表格" class="headerlink" title="markdown创建表格"></a>markdown创建表格</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">|id|name|</span><br><span class="line">|:-|:-|</span><br><span class="line">|1|A1|</span><br><span class="line">|2|A2|</span><br><span class="line">|3|A3|</span><br></pre></td></tr></table></figure><p>效果如下：</p><table><thead><tr><th align="left">id</th><th align="left">name</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A1</td></tr><tr><td align="left">2</td><td align="left">A2</td></tr><tr><td align="left">3</td><td align="left">A3</td></tr></tbody></table><p>表格调整：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- 默认左对齐</span><br><span class="line">:- 左对齐</span><br><span class="line">-:右对齐</span><br><span class="line">:-:居中</span><br></pre></td></tr></table></figure><p>列宽度调整：<br>-表示列的宽度权重，比如如下，–、-，表示第一列的宽度是第二列的俩倍：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">|id|name|</span><br><span class="line">|:--|:-|</span><br><span class="line">|1|A1|</span><br></pre></td></tr></table></figure><p>效果如下：</p><table><thead><tr><th align="left">id</th><th align="left">name</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">A1</td></tr></tbody></table><h3 id="markdown-emoji"><a href="#markdown-emoji" class="headerlink" title="markdown emoji"></a>markdown emoji</h3><p>文字已经不能表达我的愤怒了，来个 :rage:</p><p><a href="https://www.cnblogs.com/chenych/p/8623353.html" target="_blank" rel="noopener">Emoji表情</a><br><a href="https://www.webpagefx.com/tools/emoji-cheat-sheet/" target="_blank" rel="noopener">github emoji</a></p><h3 id="Github和hexo建站-tutorial"><a href="#Github和hexo建站-tutorial" class="headerlink" title="Github和hexo建站 tutorial"></a>Github和hexo建站 tutorial</h3><ul><li><p><a href="https://hexo.io/zh-cn/docs/" target="_blank" rel="noopener">官方中文文档</a></p></li><li><p><a href="https://blog.csdn.net/working_harder/article/details/52437783" target="_blank" rel="noopener">github+hexo</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/22498474" target="_blank" rel="noopener">Hexo(2)-部署博客及更新博文</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/78467553" target="_blank" rel="noopener">git怎么部署</a></p></li><li><p><a href="https://github.com/tufu9441/maupassant-hexo" target="_blank" rel="noopener">我用的主题</a></p></li></ul><h3 id="安装时的问题："><a href="#安装时的问题：" class="headerlink" title="安装时的问题："></a>安装时的问题：</h3><ul><li><a href="https://blog.csdn.net/qq_21808961/article/details/84476504" target="_blank" rel="noopener">hexo d命令报错 ERROR Deployer not found: git</a><br>npm install hexo-deployer-git –save</li></ul><h4 id="写博文"><a href="#写博文" class="headerlink" title="写博文"></a>写博文</h4><ul><li><p>创建新页面</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page &quot;about&quot;</span><br></pre></td></tr></table></figure></li><li><p>创建笔记</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new &quot;Hexo教程&quot;</span><br></pre></td></tr></table></figure></li><li><p>创建draft</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo new [layout] &lt;title&gt; 这里的layout&#x3D;draft</span><br><span class="line">if you want to pushish the draft</span><br><span class="line">hexo publish draft &quot;title&quot;</span><br></pre></td></tr></table></figure></li><li><p><img src="/images/githubhexo1.png" alt="post_title"></p></li><li><p>引用站内文章<br>方法1：markdown 语法插入已有链接</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[引用文章](http:jononearth&#x2F;categoriesname&#x2F;title)</span><br></pre></td></tr></table></figure><p>方法2：  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;% post_link 文章文件名（不要后缀） 文章标题（可选） %&#125;</span><br><span class="line">如：</span><br><span class="line">&#123;% post_link Hello-World %&#125;</span><br><span class="line">&#123;% post_link Hello-World 你好世界 %&#125;</span><br></pre></td></tr></table></figure></li></ul><h4 id="发博文"><a href="#发博文" class="headerlink" title="发博文"></a>发博文</h4><p>在 Git Shell 中进入 Hexo 文件夹，执行下面几条命令，将博客部署到 GitHub 上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo generate</span><br><span class="line">(若要本地预览就先执行 hexo server)</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">或者直接用组合</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure><p>快捷命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hexo g &#x3D;&#x3D; hexo generate</span><br><span class="line">hexo d &#x3D;&#x3D; hexo deploy</span><br><span class="line">hexo s &#x3D;&#x3D; hexo server</span><br><span class="line">hexo n &#x3D;&#x3D; hexo new</span><br></pre></td></tr></table></figure><p>另外还有一些其他方法来发布博文，如 <a href="https://www.zhihu.com/question/27384681/answer/87037317" target="_blank" rel="noopener">hexo-admin插件</a></p><h4 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h4><p><a href="https://blog.csdn.net/sherlockzoom/article/details/43835613" target="_blank" rel="noopener">mathjax + latex</a><br><a href="http://www.codecogs.com/latex/eqneditor.php" target="_blank" rel="noopener">在线转换工具</a></p><p>行间公式用两个 $$ 符号表示，行内用 一个 $ 表示。</p><h4 id="插入视频"><a href="#插入视频" class="headerlink" title="插入视频"></a>插入视频</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;video src&#x3D;&#39;https:&#x2F;&#x2F;youtu.be&#x2F;Boy3zHVrWB4&#39; type&#x3D;&#39;video&#x2F;mp4&#39; controls&#x3D;&#39;controls&#39;  width&#x3D;&#39;100%&#39; height&#x3D;&#39;100%&#39;&gt;&lt;&#x2F;video&gt;</span><br></pre></td></tr></table></figure><p>or  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% raw %&#125;&lt;iframe width&#x3D;&quot;854&quot; height&#x3D;&quot;480&quot; src&#x3D;&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;embed&#x2F;Boy3zHVrWB4&quot; frameborder&#x3D;&quot;0&quot; allow&#x3D;&quot;autoplay; encrypted-media&quot; allowfullscreen&gt;&lt;&#x2F;iframe&gt;&#123;% endraw %&#125;</span><br></pre></td></tr></table></figure><h3 id="Github和jekyll建站-tutorial"><a href="#Github和jekyll建站-tutorial" class="headerlink" title="Github和jekyll建站 tutorial"></a>Github和jekyll建站 tutorial</h3><p><a href="https://www.jekyll.com.cn/docs/" target="_blank" rel="noopener">jekyll tutorial</a></p><p><a href="https://blog.csdn.net/xudailong_blog/article/details/78762262" target="_blank" rel="noopener">github快速建博客指南</a></p>]]></content>
      
      
      <categories>
          
          <category> 极客时间 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 建站 </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning 资源汇总</title>
      <link href="/2019/12/29/2018-05-07-machine-leaning-index/"/>
      <url>/2019/12/29/2018-05-07-machine-leaning-index/</url>
      
        <content type="html"><![CDATA[<p>机器学习资源整理，网站地图(Website Maps)</p><blockquote><p>Warning</p><blockquote><p>This document is under early stage development. If you find errors, please raise an issue or contribute a better definition!</p></blockquote></blockquote><h2 id="Some-cheatsheets"><a href="#Some-cheatsheets" class="headerlink" title="Some cheatsheets:"></a>Some cheatsheets:</h2><p><a href="https://zhuanlan.zhihu.com/p/28467910" target="_blank" rel="noopener">值得收藏的 27 个机器学习的小抄</a></p><p><a href="http://ml-cheatsheet.readthedocs.io/en/latest/" target="_blank" rel="noopener">Machine Learning Cheatsheet</a></p><p><a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/" target="_blank" rel="noopener">如何在 Kaggle 首战中进入前 10%</a></p><p><a href="https://medium.com/machine-learning-in-practice/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6" target="_blank" rel="noopener">The cheatsheet of different cheatsheets</a></p><h2 id="非常值得一看的机器学习博客-持续更新-："><a href="#非常值得一看的机器学习博客-持续更新-：" class="headerlink" title="非常值得一看的机器学习博客(持续更新)："></a>非常值得一看的机器学习博客(持续更新)：</h2><ul><li><p><a href="https://chrisalbon.com/" target="_blank" rel="noopener">chrisalbon博客</a>, 废话不多，但很直观。直接上代码，值得多看看。</p><img src="/images/chrisalbon博客.png" width = 80% height = 80% div align=center /></li><li><p><a href="https://pathmind.com/wiki/graph-analysis" target="_blank" rel="noopener">pathmind</a></p></li><li><p><a href="http://krasserm.github.io" target="_blank" rel="noopener">bayesian neural network</a></p></li></ul><h2 id="值得一看的教程"><a href="#值得一看的教程" class="headerlink" title="值得一看的教程"></a>值得一看的教程</h2><p><a href="http://www.julyedu.com/video/play/18/429" target="_blank" rel="noopener">机器学习公开课</a></p><p><a href="http://www.julyedu.com/video/play/25" target="_blank" rel="noopener">算法公开课</a></p><p><a href="https://otexts.com/fpp2/autocorrelation.html" target="_blank" rel="noopener">Forecasting: Principles and Practice</a></p><p><a href="https://online.stat.psu.edu/stat510/" target="_blank" rel="noopener">Applied Time Series Analysis</a></p><p><a href="https://www.itl.nist.gov/div898/handbook/index.htm" target="_blank" rel="noopener">Engineering stattistics handbook</a></p><h2 id="网站地图："><a href="#网站地图：" class="headerlink" title="网站地图："></a>网站地图：</h2><p>Machine learning with Python 系列：</p><ul><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-1/">machine learning with python(1)</a>: 环境搭建，及Python处理数据基础教程</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-2/">machine learning with python(2)</a>: 理解你的数据常用手段</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-3/">machine learning with python(3)</a>：特征工程</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Machine-learning-with-python-4-classical-algorithm/">Machine learning with python(4)_classical algorithm</a>：几种经典算法整理</li></ul><p>Deep Learning 系列：</p><ul><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Deep-Learning-1/">Deep Learning(1)</a>: 深度学习基本概念</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Deep-Learning-1/">Deep Learning(1)</a>: Keras基本用法，以及几种深度学习算法简介</li><li><a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/">调参总结</a>: 应用深度学习时的调参总结</li></ul><p>Python语言基础(data方向)：</p><ul><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/python-cheatsheet1/">python 小抄</a></li><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/Numpy-%E5%B0%8F%E6%8A%84/">Numpy 小抄</a></li><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/Pands%E5%B0%8F%E6%8A%84/">Pandas小抄</a></li></ul><p>数据结构和算法：</p><ul><li><a href="http://jononearth.com/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/Algorithm-1/">Data structure and algorithm(1)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Indoor Positioning Resources Document</title>
      <link href="/2019/12/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E4%B9%8B%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/"/>
      <url>/2019/12/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E4%B9%8B%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>This blog contains the papers I have read for my summer research: Applying machine learning to indoor fingerprinting positioning, especially using deep learning.</p><h2 id="开篇入门-chinese-version-："><a href="#开篇入门-chinese-version-：" class="headerlink" title="开篇入门(chinese version)："></a>开篇入门(chinese version)：</h2><p>Please skip this part if you don’t speak chinese.<br><a href="http://www.cnblogs.com/rubbninja/tag/%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D/" target="_blank" rel="noopener">机器学习与室内定位技术</a></p><h2 id="Overview-of-indoor-positioning"><a href="#Overview-of-indoor-positioning" class="headerlink" title="Overview of indoor positioning"></a>Overview of indoor positioning</h2><ol><li><a href="https://arxiv.org/abs/1709.01015" target="_blank" rel="noopener">A Survey of Indoor Localization Systems and Technologies</a></li><li><a href="https://ieeexplore.iee.org/abstract/document/8400090/" target="_blank" rel="noopener">Overview of indoor positioning system technologies</a></li><li><a href="http://www.mdpi.com/2220-9964/6/5/135/htm" target="_blank" rel="noopener">Indoor Fingerprint Positioning Based on Wi-Fi: An Overview</a></li></ol><h2 id="Dataset-used"><a href="#Dataset-used" class="headerlink" title="Dataset used"></a>Dataset used</h2><p>Crowdsourced WiFi database and benchmark software for indoor positioning</p><ul><li><a href="https://zenodo.org/record/889798#.WvsnbogvzD4" target="_blank" rel="noopener">Dateset1</a>  </li><li><a href="http://www.mdpi.com/2306-5729/2/4/32/htm#B15-data-02-00032" target="_blank" rel="noopener">illustration</a></li></ul><p>Long-Trem WiFi Fingerprinting Dataset for Reserach on Robust Indoor Poitioning</p><ul><li><a href="https://zenodo.org/record/1066041" target="_blank" rel="noopener">Dateset2</a>  </li><li><a href="http://www.mdpi.com/2306-5729/3/1/3/htm" target="_blank" rel="noopener">illustration for dataset2</a></li></ul><h2 id="Papers"><a href="#Papers" class="headerlink" title="Papers:"></a>Papers:</h2><h3 id="deep-learning"><a href="#deep-learning" class="headerlink" title="deep learning"></a>deep learning</h3><ol><li><p>Low-effort place recognition with WiFi fingerprints using deep learning<br><a href="https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8184808048122111760" target="_blank" rel="noopener">https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8184808048122111760</a></p><blockquote><p>This article use deep neural networks and Autoencoders to do the floor Classification, but no positioning prediction. I have done the positioning part, get a good result.<br>code of this paper: <a href="https://github.com/mallsk23/place_recognition_wifi_fingerprints_deep_learning" target="_blank" rel="noopener">github</a></p></blockquote></li><li><p>A Deep Learning Approach to FingerprintingIndoor Localization Solutions<br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215428" target="_blank" rel="noopener">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215428</a></p><blockquote><p>It use two methods to solve the small dataset problem. one is using data augmentation. The sequence of the APs will change(very doubt at this method) ; Anther method is to use transfer learning, only similar dataset can help.<br>github：<a href="https://github.com/MaiziXiao/IndoorLocalization" target="_blank" rel="noopener">https://github.com/MaiziXiao/IndoorLocalization</a></p></blockquote></li><li><p>Large-Scale Location-Aware Services in Access: Hierarchical Building/Floor Classification and Location Estimation Using Wi-Fi Fingerprinting Based on Deep Neural Networks<br><a href="https://www.tandfonline.com/doi/abs/10.1080/01468030.2018.1467515" target="_blank" rel="noopener">https://www.tandfonline.com/doi/abs/10.1080/01468030.2018.1467515</a></p><blockquote><p>get more advance based on paper 1. Not only floor detection, but also positioning estimation.<br><a href="http://kyeongsoo.github.io/research/projects/indoor_localization/index.html" target="_blank" rel="noopener">http://kyeongsoo.github.io/research/projects/indoor_localization/index.html</a></p></blockquote></li><li><p>Indoor Fingerprint Positioning Based on Wi-Fi: An Overview<br><a href="http://www.mdpi.com/2220-9964/6/5/135/htm" target="_blank" rel="noopener">http://www.mdpi.com/2220-9964/6/5/135/htm</a></p><blockquote><p>This is an overview. Two keywords: fingerprint, WiFi</p></blockquote></li><li><p>Learning the Localization Function: Machine Learning Approach to Fingerprinting Localization<br><a href="https://arxiv.org/abs/1803.08153" target="_blank" rel="noopener">https://arxiv.org/abs/1803.08153</a></p><blockquote><p>basically the same as paper 2.</p></blockquote></li></ol><ol start="6"><li><p>CNN based Indoor Localization using RSS Time-Series<br><a href="https://www.researchgate.net/publication/325678644_CNN_based_Indoor_Localization_using_RSS_Time-Series" target="_blank" rel="noopener">https://www.researchgate.net/publication/325678644_CNN_based_Indoor_Localization_using_RSS_Time-Series</a></p><blockquote><p>using CNN to deal with long-term(time—series) dataset</p></blockquote></li><li><p>todo</p></li></ol><h3 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h3><ol><li><p>Clustering benefits in mobile-centric WiFi positioning in multi-floor buildings<br><a href="https://ieeexplore.ieee.org/abstract/document/7533846/" target="_blank" rel="noopener">https://ieeexplore.ieee.org/abstract/document/7533846/</a></p><blockquote><p>cluster method for our dataset1</p></blockquote></li><li><p>基于K均值聚类算法的位置指纹定位技术(chinese version)<br><a href="https://wenku.baidu.com/view/941a46e0192e45361166f505.html" target="_blank" rel="noopener">https://wenku.baidu.com/view/941a46e0192e45361166f505.html</a></p><blockquote><p>该文章很好的整理了为什么会用到kmeans和knn,以及提到kriging方法可用于创建指纹库</p></blockquote></li><li><p>Comprehensive analysis of distance and similarity measures for Wi-Fi fingerprinting indoor positioning systems<br><a href="https://www.sciencedirect.com/science/article/pii/S0957417415005527" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S0957417415005527</a>*</p><blockquote><p>The UJI kNN algorithm for dataset.<br>This article mainly concentrates on the Wi-Fi Indoor positioning systems based on fingerprinting and k-NN. It mentions Non-heard data processing, data preprocessing, and all kinds of distance calculating.</p></blockquote></li></ol><ol start="4"><li>Adaptive K-nearest neighbour algorithm for WiFi fingerprint positioning<br>Website: <a href="https://www.sciencedirect.com/science/article/pii/S240595951830050X" target="_blank" rel="noopener">https://www.sciencedirect.com/science/article/pii/S240595951830050X</a><blockquote><p>This article focus on how to improve K nearest neighbour algorithm</p></blockquote></li></ol><h3 id="Kriging-algorithm"><a href="#Kriging-algorithm" class="headerlink" title="Kriging algorithm:"></a>Kriging algorithm:</h3><ol><li><p>Method for yielding a database of locationfingerprints in WLAN<br><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1522067" target="_blank" rel="noopener">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1522067</a></p><blockquote><p>using kriging to generate fingerprint data</p></blockquote></li><li><p><a href="http://download.atlantis-press.com/php/download_paper.php?id=16366" target="_blank" rel="noopener">Fingerprint Space Building Algorithm with Kriging for Large Positioning Regional Environment</a></p></li><li><p><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7565018" target="_blank" rel="noopener">Applying Kriging Interpolation for WiFiFingerprinting based Indoor Positioning Systems</a></p></li><li><p>kriging tutoril(Chinese version)<br><a href="https://xg1990.com/blog/archives/222" target="_blank" rel="noopener">克里金(Kriging)插值的原理与公式推导</a></p></li></ol><h3 id="other-algorithms-need-arrange"><a href="#other-algorithms-need-arrange" class="headerlink" title="other algorithms need arrange"></a>other algorithms need arrange</h3><ol><li>Dealing with Insufficient Location Fingerprints in Wi-Fi Based Indoor Location Fingerprinting<br><a href="https://www.hindawi.com/journals/wcmc/2017/1268515/" target="_blank" rel="noopener">https://www.hindawi.com/journals/wcmc/2017/1268515/</a></li></ol><h2 id="other-maybe-useful-articles-or-resources"><a href="#other-maybe-useful-articles-or-resources" class="headerlink" title="other maybe useful articles or resources:"></a>other maybe useful articles or resources:</h2><ul><li><p><a href="https://medium.com/jungle-book/towards-data-set-augmentation-with-gans-9dd64e9628e6" target="_blank" rel="noopener">Towards data set augmentation with GANs</a></p><blockquote><p>maybe a way of data augmentation</p></blockquote></li><li><p>App for collecting the data of indoor positioning：<a href="https://github.com/schollz/find" target="_blank" rel="noopener">https://github.com/schollz/find</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 密室逃脱 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 室内定位 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>每日一吐201812</title>
      <link href="/2019/01/01/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90201812/"/>
      <url>/2019/01/01/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90201812/</url>
      
        <content type="html"><![CDATA[<p>很早之前，坚持过一段时间的，每日吐槽身边的事和人，以及自己。期望自己能够发现生活中有趣，无趣的一面，关注生活，而不是让时间悄悄溜走。</p><p>有了自己的博客，没什么人关注。目前的状态还是蛮喜欢的。努力记录，努力创造。</p><p>主要以生活小见闻，小感悟，以及读书笔记，随笔等为主。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>硕士毕业啦，开启博士啦，放假啦，过节啦，搬家啦，新年啦，加油啦，啦啦啦！</p><h3 id="12-1-2018"><a href="#12-1-2018" class="headerlink" title="12/1/2018"></a>12/1/2018</h3><p>《写在读博之前》<br>想写写关于读博的事情，但不知道自己能写些什么，写多少，所以没必要另开一篇博客来写了。从考虑读博到决定读博的过程中，徘徊犹豫了好久。就像我爱看的奇葩说一样，一会觉得正方读博挺好的，一会觉得反方工作才是上佳的。</p><p>导师问我：“你真的想读博，真的适合读博吗？别回答我，回答你自己”。 我去，还真回答不上来。心底存在着两个声音。一个声音跟我说，适合的，这是个机会，你不是一直很希望自己真正搞懂那些科学技术吗？而不是流于表面。另外一个声音告诉我，适合个屁，那么大岁数了，你看看你同学工作的，创业的，成家的，哪个不比你厉害。还想要在象牙塔里呆多久啊，况且博士你真的能做好吗？</p><p>我是个优柔寡断的人，对于两可的决定，总下不定决心做一个。其实这是贪心，因为不愿意失去任何可以期待得到的事物。但实际上，无论此时做不做决定，最终都会有个决定，而往往，越早做决定，未来的收获会大于此时的不快。短期的不快与长期的收益之间，请尽量选择后者。在考虑读不读博，其实时间上也不允许我能够深入的思考这个问题。记得出国前，一个朋友说，既然你选择了这条路，跪着也要走完。所以选择就好啦。既然选择了，那么就做好呗。</p><p>对于博士，我对自己的期许是：去假存真。再也不是流于表面，学过，会用，了解，一知半解。而现在要做到理解，从原理到公式推导都要明白。现在跟导师交流时，老会夸下海口，学过，会的。其实心里直发虚。希望未来，我能够坦诚的跟导师说，我会或者我不会。做事，做人，期望都要做到去假存真。做人，真心的帮助别人，如果没空，甚至不想帮助的时候，请直接回绝。不要怕如果伤害了别人，你的印象在别人眼中大跌。</p><p>上次偶尔看到，我这样性格的人对自己是蛮苛刻的。如果做不到自己想要的样子，会对自己很失望，进而会失去自身的控制能力。也许这是一个机会，是重新活出你想要的样子的机会。但我希望也不要苛责自己，即使没有活出自己最想要的样子，也请接受自己，那就是我，一个平凡人，与生活中挣扎。</p><p>我会把这次读博士，当作工作来做。但这工作更自由，也多了些趣味。要多接触接触大神，跟同行交流才能获取更多的力量。蒙头搞科研，知识是永远学不完的。</p><p>对自己，要更加自律。很多时候，凭借一时热情和压力在做事情。这种状态深知是不长久的。自律不会剥夺我什么的，想要自由，没有自律的自由是一盘散沙，毫无意义，自律下的自由才是维持内心秩序平和的关键。</p><p>着眼与未来，落实与当下。没有过多的期许，just to be a better man.</p><h3 id="12-7-2018"><a href="#12-7-2018" class="headerlink" title="12/7/2018"></a>12/7/2018</h3><p>从群里面看见有人发一个教授自杀了的文章，起先没注意。接着看了他的阴谋论的论调，让我点开了文章。一下看到了张首晟！很震惊，不会吧，会不会是谣言，上百度搜了下，还真是。一开始不太理解这么伟大的一位科学家为什么会自杀，当看到抑郁症的时候，释然了。内心当时不知道是什么心情，有点难过。可能是我从小喜欢科学的原因，看到这么优秀的科学家去世，太多的遗憾可惜了。但面对抑郁症，你是神也没用。抑郁是病，有时候靠个人的意志去挡，没有外界介入的话，很容易走向终点。教授，走好！你留给人类的遗产是这么丰富，此生你值了！</p><h3 id="12-8-2018"><a href="#12-8-2018" class="headerlink" title="12/8/2018"></a>12/8/2018</h3><p>这期奇葩说辩题：知道了别人的死亡时间，要不要告诉他？</p><p>这是一个探讨我们如何面度死亡的辩题。正方说，他们觉得如果告诉了别人，可以让他们更好的知道接下来的路该怎么走。反方说，如果告诉了他们并不一定会帮助他们，反而引发不必要的麻烦；而且每一个人都对自己的生命负责，想过的好为什么不现在就开始。</p><p>虫仔邱晨讲了她自己的抗癌过程，以及面对癌症的态度。当面对癌症，除了对死亡的恐惧，其实也没那么恐惧，她说，剩下的该怎样还是怎样。<br>康永哥说，一个成熟的人即使知道自己没几天就死了，但他依然会把他认为该做的事情做完，即使这件事是每天都会做的平常事。<br>我们人是一个伟大的物种，能够赋予做某件事的意义，这才是支撑我们花时间做的动力，以及因此也就赋予了生存的意义。<br>所以如果说还有3天可活，那么问问什么是你生存的意义，去做吧。</p><p>写到这，让我想起了爷爷去世前的几天。回光返照，我还以为他稍微好转了一些。但他当时应该很清楚，大限将至。他见了我们每一个人，儿子，女儿，孙儿。一个个交代，说我走了后，葬礼该怎么弄，在哪弄，还有他存的钱该怎么分，甚至他还会叮嘱谁谁有矛盾，需要和解理解。当奶奶相叫他休息时，他甚至会吼叫不去休息，让他把话讲完。或许有条不紊的处理完这些事情，他才会没有遗憾的离去吧。那个时候，死亡并不可怕，可怕的是他惦念的事情没有做完。</p><p>我生存的意义是什么？我不敢问自己，因为真的不知道。<br>如果还有3天可活的话，我肯定会回到父母身边，他们是我最爱的人，也是对我最无私的人。<br>然后我会跟我的朋友们和爱过的人告别吧，因为你们，我的生活变得精彩好多，我希望当面跟你们告别，但时间不允许的话，我会跟你视频。<br>还有我其他的亲人，你们也是我生活必不可少的一部分，我希望跟你们好好的交代几句。但请不要过分的担忧，也不希望你们过多的占据我仅有的时间。<br>接下来，我会列出来对待人和事情，还有哪些遗憾，能当面跟人说的说，不能的写下来。<br>还有，我希望能写一下自己的诰文，策划下自己的葬礼。我很不喜欢传统的葬礼仪式，希望越简单越好，大家坐在一个礼堂或者我家，聊一聊，祭奠下我就行，不要花圈（千万）。我希望朋友能在葬礼上说一些关于我好玩的事情，夸我也行，但不希望是网上抄来的千篇一律（考验你们功底的时候到了）。我希望那个时候，也能留下些什么东西，在葬礼上展示，比如说这篇文章，一些照片。<br>最后，我不会静静的等待死亡来临，特别讨厌等待的感觉，让人无所适从。我希望那个时候，我在跟朋友或许父母聊天，或者在写文章，或者在筹划葬礼，突然就来了那种最好。</p><p>如果还有30天呢？我接着问，<br>如果还有1年呢？<br>如果还有5年呢？<br>如果还有10年呢？<br>如果还有30年？<br>如果还有50年？<br>…</p><p>最后献上邱晨的一句话，</p><blockquote><p>当你真的尝试过竭尽全力之后，你会尝不到一点点遗憾，你会没有一丝的悔恨，你会感受的到的是无限的平静和喜悦” —— 邱晨</p></blockquote><h3 id="12-9-2018"><a href="#12-9-2018" class="headerlink" title="12/9/2018"></a>12/9/2018</h3><p>今天朋友组局去她家吃火锅。感觉好久没出来社交了，出门前还在想怎么跟陌生人去交流，晕~</p><p>蛮佩服这个认识不久的朋友。喜欢到处旅游，到处游山玩水。点开她的朋友圈，都是去哪玩的照片。听她说，她的 schedule 都排的很满，每到周末就忙着去哪里hiking, 爬山，社交等等。</p><p>但她自己本职工作，从来没有因此放松过。这不，说对什么 life insurance 感兴趣，报了一个这个班正在学习。</p><p>就是感觉她过的很自由。高中随家里移民来到美国，大学毕业后试了几份工作，最终确定了现在的方向。在辞职期间，去过很多国家，想回国定居，回来住了很长时间发现不适合自己，又回美国了，最终定居在了波士顿。</p><p>她身边总是有超多的朋友，特别善于组局，闲不下来吧，哈哈… 她总是想到的就去做，没有什么瞻前顾后，但说话也特别会考虑别人感受，所以这就是她组的局，大家都愿意去的原因吧。</p><h3 id="12-10-2018"><a href="#12-10-2018" class="headerlink" title="12/10/2018"></a>12/10/2018</h3><p>奇葩说结束了。老奇葩颜如晶再一次错失了冠军宝座，陈铭也真的是实至名归。</p><p>最后一期的辩题是：我不合群，我要不要改。<br>这是一个对于自己身份认同的题。每个人从小打大，或多或少都会出现身份认同的危机吧。马东说，这是想给正在挣扎的成长的孩子们的一些建议。所以当大多数人都站在不要改的立场时，他站了出来。是的，辩论不是为了一方观点压制另一方，至少在奇葩说，它想传递的是不同的思考，不同的价值观，没有非黑即白，没有高级低级，有的只是不同的适用环境。</p><p>我是一个从小到大，看起来合群却不合群的人。还能这么说吗？从小到大，我在经历中各种各种的群，看似其中一员，但总是融不进去，内心深处特别希望得到别人的认可。听完奇葩说反方辩论，反思自己，其实那些群是不适合自己的吧，都是自己去硬凑的。这就是改的立场吧，但真的成功融入了吗？</p><p>我们的内心深处，都是需要得到认可的，可以是父母的，可以是朋友的，可以是群的，甚至可是某个兴趣的，更有的可以是自己的认可。所以我不合群，需不需要改。看你自己喽。你的自我认可来自哪儿。内心强大的人，都说自己是不合群的。其实合群也蛮好的啊，过的开心，也甚至可以发掘出自己的潜力，跟一帮朋友互相进步。</p><h3 id="12-13-2018"><a href="#12-13-2018" class="headerlink" title="12/13/2018"></a>12/13/2018</h3><p>看了篇报道，现在一些小学生，都会自己购买得到app的课程，自我学习进化了。特别羡慕现在成长起来的孩子。刚出生就接触到我们20多岁才能接触到的事物。他们的成就不可限量，未来什么样子充满期待。但同时感到一阵前浪拍死在沙滩上的感觉。世界是我们的，但终归是你们的。</p><h3 id="12-14-2018"><a href="#12-14-2018" class="headerlink" title="12/14/2018"></a>12/14/2018</h3><p>在我的组织下，我们组第一次正式的聚餐选在了学校不远的中餐厅。导师也终于硬气了一回，”it’s my treat”。记得上学期送访学的学生时，大家在一个不知道哪国的餐厅聚餐，每人点一样，然后各付个的钱。对于初来咋到的我，一个中国人，完全不能理解老外的这种行为啊，好尬。记得国内，聚会不都是导师老板付钱的嘛。</p><p>那次的聚餐氛围也差劲，每人端着个盘子，还有一个其他组教授，远远的坐着，根本聊不到一块去。这次氛围就很好了，不仅是因为我组织的，更多的是中餐的就餐文化。大家都是分享食物，这就在无形之中，拉近了大家的距离（所以说中国人是最会组吃局的呢）。<br>反观老外，每个人端着个盘子。<br>问你吃啥。<br>‘beans’，<br>‘啥beans’，<br>‘green beans’,<br>‘哦’ ,<br>‘it’s very delicious’。<br>…（无语）…<br>查，绿豆有啥好吃的，有啥好讲的，还delicious 。作为吃文化最厉害的中国人，内心不知道鄙视了他们多少次。</p><p>不过他们对于中餐，还是非常推崇的。让我惊讶的是，都会用筷子，甚至巴西哥们用的比我还标准。之前带印度朋友吃饭（印度人非常爱交朋友），基本不会用，最后甚至动上了手。我在想，是不是西方的传统美食有，但不适用每天做，每天吃（看电视上，欧洲古代，吃就是面包就汤，有钱的就加上肉）。而亚洲的传统美食，从上到下都能美美的吃上一顿有味的饭。印度这个神奇的国度，吃的文化可能也比较厚重，所以中餐的渗透还是需要些时间吧。</p><p>嗯，不用怀疑，中餐会统一世界。</p><h3 id="12-15-2018"><a href="#12-15-2018" class="headerlink" title="12/15/2018"></a>12/15/2018</h3><p>今天一大早就醒了，睡了5个多小时，睡不着了。不知道是因为要搬家的事情，还是因为昨天感到碌碌无为而上心。每年总有那么几次，突然的觉得不爽，不爽自己的一事无成，不爽自己虚度光阴。但这种不爽来的快，去的也快。很快就淹没在茫茫的生活（存）中。</p><p>到底为什么而活？问这种终极问题的人不是有点抑郁倾向，就是在其他人看来无病呻吟。有抑郁倾向的人，甚至怀疑自己的整个人生，活和不活是个问题。无病呻吟的那种，在别人看来，没啥本事，不干活，劲瞎想。有本事，先活得精彩一些先看看啊。</p><p>但即使是很多所谓的成功人士，也会在某个时间点，陷入到这种虚无主义中。只是大家都不说了。生活还得继续，能给自己多找点意义，就多找点。<br>嗯？你说找不到。那也继续吧，一阵的瞎想很快就淹没在生活（存）中了。因为毕竟生存是动物的本能啊。</p><h3 id="12-17-2018"><a href="#12-17-2018" class="headerlink" title="12/17/2018"></a>12/17/2018</h3><p>终于补完了科幻美剧《Origin》。讲的是一伙身份各异的人移民外星球，在途中遭遇外星寄生生物而发生的故事。</p><p>很喜欢科幻。最近些年看的科幻剧中，最喜欢《Expanse》。场景宏大，剧情也非常不错。而Origin的场景就显得很小，基本就是在宇宙飞船里，甚至是几个固定仓里，遭遇外星生物，然后人们互相猜忌而发生的一系列故事。</p><p>外星生物可以寄生人脑，从而控制人。但由于仪器遭到破坏无法检测，所以大家只能互相猜忌求证是否有人被寄生。这是典型的美剧套路，与其说是科幻剧，不如说人性剧。</p><p>剧中，当面对危机情况时，每个人的本性就会暴露出来。以前有信任危机的，遇到情况，最先考虑自己，不相信他人；以前是黑帮的，遇到情况，处理果断，但过于冷血；还有很多以前做错事，无法面对的那帮人，依然在遇到相似抉择时，悔恨不已；但也有一些人，看起来坏坏的，其实内心却很柔软…</p><p>为什么起名字叫《Origin起源》。它彷佛在告诉我们：你想告别过去，重新开始，但实际上，你的过去控制着你，是什么样的人还是什么样的人，改变不了什么。</p><p>但真的不会改变吗？一帮有着各种过去的人，面对突如其来的灾难，紧紧抱在一块，又互相猜忌。文明的本性在黑暗中时不时的亮起。</p><p>这里面有个有意思的设定，被控制的人会渐渐遗忘记忆。而飞船数据库里存储着大家的过去，也就是记忆。那么大家可以通过这些数据来判断谁被寄生了。但貌似所有人都不愿意自己的过去被人知晓，甚至其中一个人不惜毁掉了这些数据（而她并没有被寄生）。</p><p>过去真的这么重要吗？重要。但同时毁掉过去数据，这个设定，或许是想告诉我们，只要内心向往美好，过去也许不是阻碍。</p><h3 id="12-20-2018"><a href="#12-20-2018" class="headerlink" title="12/20/2018"></a>12/20/2018</h3><p>最近打美服王者荣耀，总结了一些经验。队伍里经常互喷的，大概率赢不了；喷的不多的，赢面一半；开始说话积极的，死的特别惨，坑的一笔。还有一些队伍，说话不多的，但有一些提醒没有谩骂的，赢面非常大。但也有啥都不沟通的队伍，一般靠运气。</p><p>非常讨论游戏里面，特别逼逼的那种。尤其是那种还没怎么打，就开始逼逼骂人的，直接带坏了团队的氛围，导致团队配合不畅GG。</p><p>有很多次，遇到无脑骂人的，我都先不说话，用事实来说话。但有的时候真心忍不住，跟他怼了几句。发现，真是越怼越生气，甚至有一次，气的手都哆嗦了。放下手机，一想这不行啊，凭什么让这帮人控制了情绪？不过是个游戏。</p><p>怎么抽离出来？ “不过是个游戏”这样来宽慰自己？我先试着看最高境界的佛陀，他会怎么想。首先他是想着度化众生的心愿，当有人打他，他不还手，骂他不还嘴。但是他沾染了这段游戏中的因果啊，他会怎么做？道歉，该自己承担的责任承担，不该自己的责任，以一个相对客观的方式呈现？…我发现编不下了，佛陀不会打游戏的。他们抽离出来的方式，是从一开始就不跟你产生这样的因果。</p><p>普通人的我，发现这或许是不可避免的，不仅仅是游戏，做很多事情，都会遇到这些事情。试着用礼貌或者理性的方式去回应这些无端的指责谩骂，不过多沉浸在别人中，回归自己，你自己是不是问心无愧。还有非常重要的，你身边的朋友是不是也能够给予你支持，相信你。</p><p>ps:还想说一点，游戏对我们大脑的影响。有时候打游戏中很长时间后，大脑会自动的适应游戏中的环境。比如看见操作很很傻X的人，会很随意的骂一句煞笔。即使放下游戏，还是沉浸在厮杀谩骂的情境中。遇到一点小事，容忍度也降低很多。<br>发现还挺可怕的，尤其是那些暴力血腥的游戏，长期沉浸其中，势必对大脑造成一定的影响。尤其是发育中的孩子们。所以游戏还真不应该多玩，或者玩一些比较好的，甚至帮助孩子培养性格的游戏。</p><h3 id="12-21-2018"><a href="#12-21-2018" class="headerlink" title="12/21/2018"></a>12/21/2018</h3><p>人们往往对自己不了解的事情，充满着偏见。也正是这些偏见，才是构成认知的主要成分。</p><h3 id="12-22-2018"><a href="#12-22-2018" class="headerlink" title="12/22/2018"></a>12/22/2018</h3><p>缘分这个东西，怎么老耍人呢！</p><h3 id="12-24-2018"><a href="#12-24-2018" class="headerlink" title="12/24/2018"></a>12/24/2018</h3><p>Merry Christmas!</p><p>感觉国内的圣诞节气氛都要比我们这高啊。室友说主要是在Malden，都是移民，而且没啥钱，就不怎么装饰，波士顿市区那还是很不错的。原来如此吗。国外的人都是一家子在家聚餐，老外的餐馆基本都关了，不过中餐馆骄还是正常营业的，爆满。</p><h3 id="12-29-2018"><a href="#12-29-2018" class="headerlink" title="12/29/2018"></a>12/29/2018</h3><p>搬家搬到了一个新的地方。</p><h3 id="12-31-2018"><a href="#12-31-2018" class="headerlink" title="12/31/2018"></a>12/31/2018</h3><p>新的一年里，要说的是事情真的很多。有好玩的，也有郁闷的。无聊的可以点开这无聊的<a href="http://jononearth.com/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8/2019%E6%96%B0%E5%B9%B4flag/">2019新年flag</a>文</p><p>主要讲了这些乱七八糟的东西  </p><ul><li>学业</li><li>遇到的人，朋友，过客，缘分</li><li>生活</li><li>写作（博客）</li><li>娱乐</li></ul><p>最后了，附上新年拍的图片。很喜欢这个小松鼠和被扭曲的哥们哈:sweat_smile:。<br><img src="/images/squirrel.jpg" width = 40% height = 40% div align=left /><br><img src="/images/niuqu.png" width = 40% height = 40% div align=right /><br><img src="/images/coffeebar.jpg" width = 40% height = 40% div align=left /><br><img src="/images/isec.jpg" width = 40% height = 40% div align=right />    </p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2019新年flag</title>
      <link href="/2019/01/01/2019%E6%96%B0%E5%B9%B4flag/"/>
      <url>/2019/01/01/2019%E6%96%B0%E5%B9%B4flag/</url>
      
        <content type="html"><![CDATA[<p>凯哥给我的新年祝福短信是：新年里，祝鹏博士找到属于自己的女超人。<br>凯哥是我两年来美国，遇到的最重要的贵人了。虽然一年前他就离开了，但我们一直保持联系。他是北理的教授，来MIT访学，非常幸运的跟他做了室友。一个个性十足，超级努力的人。很奇怪，我就简单地被他这种奇异的特质给吸引了。尽管有时候我会受不了他的跳脱和自负而“嫌弃”他。</p><p>当身边的人是这么个优秀的人的时候，你就会想努力的去接近。这或许就是我想要读博的原因之一吧。<br>同时他有时又像我的家长似的，很是关心我的终身大事。“这么帅的小伙，要不我帮你找个富婆嫁了得了”，我“呵呵，好啊，找啊”。其实呢，他是不会瞎找的。反而，自居很了解我，说“得找这样那样的才适合你。很难找，但别着急，会有女超人出现的”。这不，还送了我个女超人。</p><p>话说两边，新年了，不应该是对过去的总结吗？为什么写above。因为言简意赅的总结了过去的2年。</p><hr><h2 id="2018总结Y"><a href="#2018总结Y" class="headerlink" title="2018总结Y"></a>2018总结Y</h2><p>那么来看看冗长无聊的版本吧。</p><p>美国2年了。来这是因为在国内的时候，过的不是很如意。不喜欢按部就班的上班，而且从底层干起。你说我浮躁也罢，好高骛远也罢，我是不后悔出来的这2年。在此，再次感谢父母这么放任这么任性的我，提供财力上的支持。</p><p>那么2年里，你收获了什么？</p><h3 id="学业-当头喝棒，为时不晚"><a href="#学业-当头喝棒，为时不晚" class="headerlink" title="学业: 当头喝棒，为时不晚"></a>学业: 当头喝棒，为时不晚</h3><p>来美国主要是学习嘛。可刚来那会我不这么想，一心想创业，操着一口不流利的Chinglish就闯进社团，现在想来都佩服当时的勇气。然后想着去认识各种各样牛逼的人，拉他们创业。结果搞了一个学期，发现是这样的：牛逼的人不理你，大部分人只想进大公司找到工作；美国创业社群也很难融进去，沟通很成问题；然后，妈的，学期末差点挂科。</p><p>这一下算打醒了我。牛逼人不鸟你，首先是自己能力不够，那么自己可不可以去做个这个专业的牛逼人呢；而且初来咋到，凭着一腔热血，到处瞎转，根本没有涉及到一些创业的圈子，啥人脉都没有。接下来，我算开始融入到正常的留学圈，不再到处折腾。开始认真学习。慢慢的，绩点提高了上来，而且也知道了将来要在什么专业上有所建树，不再做个无头苍蝇了。</p><p>正因为在专业知识的自我学习下，我才会有了一个机会，接触到了我的博导，并且有机会被他看中做了博士。</p><p>从我这边同学的角度来看，有点不能理解，刚开始来的时候跟他大谈特谈理想的呢？但似乎又能理解，来美2年了吗，总归是认清了现实。</p><p>是啊，可能是认清现实了吧。读博是近些年来做的最重大的决定，但其实并没费什么力气做这个决定，一切都是顺其自然。</p><h3 id="社交：真诚待人，且行且珍惜"><a href="#社交：真诚待人，且行且珍惜" class="headerlink" title="社交：真诚待人，且行且珍惜"></a>社交：真诚待人，且行且珍惜</h3><p>2年，遇到了很多人。除了凯哥是我遇到的一个最重要的人外，还有一些人对我的帮助或影响也很大。如Mingmin同学，也是室友，对我的生活影响蛮大的，带着我从生活的小事到去美国各地游玩。有了他，刚来的我生活变的easy很多，精彩了很多。   </p><p>还有我的房东，一个老移民，室友对他的贬褒不一。我觉得他人很好，他不喜麻烦，我也是。于是我们两相处的应该很愉快，有时他会带着我们买买菜，钓钓鱼啥的。更重要的是，对于孑然一身来到美国的我，他提供了一个类似家的地方，还有个人可以依靠商量下（主要是心里上的）。</p><p>还有一些可爱的同学们，大多数比我小好几岁。跟他们相处，很容易看出来，欠缺历练。很难处到一个聊的很来的朋友。不过幸好，有这么几个。18年8月初去黄石的2个同学就是，一个比我大，一个比我小。大的工作了很多年，在约束性的韩企里工作，向往自由，于是辞职留学了。跟他相处，也收获了不少，比较会做人做事，很会照顾人，会生活，跟他去黄石，他笑着说2个少爷带着一个仆人。但跟他熟了之后，他强硬的一面就露了出来，不过好在，他在得到我“不露声色”的暗示后，能及时收敛住。<br>小的，用大的话说，眼睛很透明，写满了未经世事，天真无邪。他不善于社交，我们在课堂上偶然认识之后，竟然一直保持联系。熟悉了之后，就更依赖我们了。不过有了他，我们去哪都很方便了，因为他有车一族啊。而且我读博士，要不是他拉着我找的现在的老师，估计就不会有接下的事情了。<br>现在发现，我们3个人，还真不能缺少他，2人都没法成局。这不自从他回国之后，我和大的M同学竟很少碰面了。有时我也会烦他，但现在他走了，还真有点想他了。</p><p>还有一些其他的同学，如第一学期就认识的，现在竟然都跟同一个老师读博了，一个美国人，一个中国人。接下来的几年里，相信我们会处的很愉快。</p><p>第一个学期认识一个印度的朋友，一直到现在，都保持联系，有时小聚一下。印度人很喜欢交朋友，很能说会说，这是不是也从一面述说了为什么印度人能称霸硅谷高管阶层。</p><p>还有一些校外认识的朋友，参加了一个华人创投组织，里面能人不少。期待下一年跟多跟他们学习学习。</p><p>还有很多的朋友，我发现写都写不过来了。有鼓励我一起读博的大龄同学；有被我忽悠一起打壁球的小伙；有特别爱组局，能玩会玩的阳光小姐姐；有目的性很强的但很nice的学长；有睡一间房（她睡地上）但啥事都没发生的小胖美女室友；有生活习惯超级马虎，一不留神，就操英语的室友；有强势个性的北京糙汉子室友；有佛系，但熟了之后才看清也有不能侵犯的南方室友；有相处特别好，但没机会相处的同学，有时候觉得很遗憾；有一起聊创业理想的小姐姐；有一起看音乐会，但之后没有联系的一起入学的女同学；有考试，大家聚在一起互抱大腿的同学；有陪我练口语的华裔小姐姐，是个gay，活的很认真，很美国；有帮助我们处理各种学业上问题的老外辅导员，回复超快，想给他送锦旗…</p><p>遇到的大多数，基本都比较nice。这边的环境比较简单，大家基本没有什么利益上纠葛。互相尊重，也不需要溜须拍马，搞阶级斗争啥的，工作还是要靠实力来说话。</p><h3 id="生活：是啥？暂时没有"><a href="#生活：是啥？暂时没有" class="headerlink" title="生活：是啥？暂时没有"></a>生活：是啥？暂时没有</h3><p>没有女朋友，所以很宅。不去学校的时候，就自己刷剧。不过，幸好，我有一颗好奇探索的心。跟着朋友们去过不少地方。Boston周边的大小景点，海边沙滩，海岛，去了不少。纽约华盛顿也去过一次。黄石也自驾去了一次，详看《<a href="http://jononearth.com/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/">盐湖到黄石6天不完全攻略</a>》。不过跟朋友圈里的朋友一比，就相差太多了。贫穷限制了我的行动力，还有一堆due.</p><h3 id="其他：没有八卦"><a href="#其他：没有八卦" class="headerlink" title="其他：没有八卦"></a>其他：没有八卦</h3><p>2018年做的最好的事情，应该说是开了个博客。这个博客主要是写给自己的。对于我的起了个很大的作用，因为把很多的东西都组织整理了起来，知识结构通过整理更加清晰。就像是我大脑的衍生，随调随用。</p><p>找到女朋友的目标没有兑现。家里介绍了一个，聊的不错，人也不错。可惜天不时，尤其是地不利，人和也枉然。对于我们两人来说，终究是一个该错过的缘分。愿各自安好，我都说没有八卦。</p><p>认知这个好难说。反正现在不再浮躁了，随着年龄的增长，见识的人和事多了，认知自然而然都会增长。</p><img src="/images/niuqu.jpg" width = 40% height = 40% div align=center /><hr><h2 id="2019展望A"><a href="#2019展望A" class="headerlink" title="2019展望A"></a>2019展望A</h2><h3 id="学业：这是工作"><a href="#学业：这是工作" class="headerlink" title="学业：这是工作"></a>学业：这是工作</h3><p>应该以一个什么样的身份去读博？学生吗？谢了，这次我想换个身份，以工作的态度去读这个书。工作的态度意味着离开学校，老师的庇护，能够学会独当一面；意味着遇到事情，不再以一个学生业余的方式对待，而是更加专业professional；意味着加倍努力的工作，完成老师的指标，完成工作上的超越。</p><p>狭隘目标，论文一年至少发2篇；数学基础越来越扎实；专业机器学习变得professional。</p><h3 id="社交：牛逼吸引牛逼"><a href="#社交：牛逼吸引牛逼" class="headerlink" title="社交：牛逼吸引牛逼"></a>社交：牛逼吸引牛逼</h3><p>吸引更多牛逼的人，不过首先自己要变得越来越好。<br>跟更多的优秀的博士交流，即使不能够获取啥有用信息，也能近朱者赤。<br>与更多工作的优秀朋友交流，获取不同视角的专业了解。<br>与更多积极进取的小伙伴交流，听听你们的故事。</p><h3 id="生活：三点一线牵"><a href="#生活：三点一线牵" class="headerlink" title="生活：三点一线牵"></a>生活：三点一线牵</h3><p>新年换了个新家，新的开始，离学校更近了。学校，健身房，家。大概大部分的时间都会在此度过了。工作既生活，生活既工作。</p><img src="/images/isec.jpg" width = 40% height = 40% div align=center />    <p>个人生活，家里面的压力已经让我到了不得不重视的程度。期望下学期遇到那个对的她，身边不再有其他人的她。</p><p>有机会，还是要去旅个游啥的。跟朋友去个小岛，小镇，爬个山。也能组织起我们组，出去游玩一次。但去更远的地方，就随缘吧。</p><h3 id="其他：练就易经第一重"><a href="#其他：练就易经第一重" class="headerlink" title="其他：练就易经第一重"></a>其他：练就易经第一重</h3><p>易经第一重：习惯培养，有规律的生活。<br>易经第二重：其他能力的培养。第二重第一章演讲能力….<br>易经第三重：待开发<br>…<br>易经第N重：从心所欲而不逾矩  </p><p>多读书，一定要多读书。接触多元的知识，发现自我的偏见尤为关键。</p><p>博客内容要持续更新，一个月至少更新一篇。随笔的话，1~3天要写一段。对，写作能力也要培养下，不能写完自己都不想回看。</p><p>以罗胖罗振宇的一句话结尾：</p><blockquote><p>做事情最好的时间是10年前，次好的时间就是现在。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习知识点</title>
      <link href="/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/"/>
      <url>/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>学习中遇到的一些知识点，简单整理一些。持续更新中。。。。</p><h3 id="生成模型（generative-model）-VS-判别模型（discriminative-model）"><a href="#生成模型（generative-model）-VS-判别模型（discriminative-model）" class="headerlink" title="生成模型（generative model） VS 判别模型（discriminative model）"></a>生成模型（generative model） VS 判别模型（discriminative model）</h3><ul><li><p>判别模型是直接学习$P(y/x)$,或者直接从特征空间学习类别标签</p></li><li><p>生成模型事假定数据满足一定的分布特征，需要学习$P(x/y)$  </p></li><li><p>优缺点：1，生成式模型都会对数据的分布做一定的假设, 比如朴素贝叶斯会假设在给定y的情况下各个特征之间是条件独立的，当数据满足这些假设时, 生成式模型通常需要较少的数据就能取得不错的效果, 但是当这些假设不成立时, 判别式模型会得到更好的效果。<br>2，生成式模型最终得到的错误率会比判别式模型高, 但是其需要更少的训练样本就可以使错误率收敛；3，生成式模型更容易拟合, 比如在朴素贝叶斯中只需要计下数就可以, 而判别式模型通常都需要解决凸优化问题；4， 生成式模型可以更好地利用无标签数据(比如DBN), 而判别式模型不可以；5，判别式模型可以对输入数据x进行预处理, 使用ϕ(x)来代替x, 如下图所示, 而生成式模型不是很方便进行替换（？）.<br><a href="https://blog.csdn.net/Fishmemory/article/details/51711114" target="_blank" rel="noopener">https://blog.csdn.net/Fishmemory/article/details/51711114</a><br><a href="http://www.cnblogs.com/kemaswill/p/3427422.html" target="_blank" rel="noopener">http://www.cnblogs.com/kemaswill/p/3427422.html</a></p></li></ul><h3 id="隐马尔可夫模型-Hidden-Markov-Model"><a href="#隐马尔可夫模型-Hidden-Markov-Model" class="headerlink" title="隐马尔可夫模型(Hidden Markov Model)"></a>隐马尔可夫模型(Hidden Markov Model)</h3><h3 id="分类器评价指标（metric）"><a href="#分类器评价指标（metric）" class="headerlink" title="分类器评价指标（metric）"></a>分类器评价指标（metric）</h3><p>对于分类器，主要的评价指标有precision，recall，F-score，Accuracy以及ROC曲线,,AUC</p><p>|   |         |预测的类|||<br>|:—–|:—-|:—-|:—|<br>|         |  |Yes|No|合计|<br>|实际的类|Yes|TP|FN|P|<br>|        |No|FP|TN|N|<br>|||P’|N’|||</p><p>首先来解释一下表格中的术语：</p><ol><li>真正例/真阳性(True Positive, TP)：指被分类器正确分类的正元组。令TP为真正例的个数。</li><li>真负例/真阴性(True Negative, TN)：指被分类器正确分类的负元组。令TN为真负例的个数。</li><li>假正例/假阳性(False Positive, FP)：指被分类器错误标记为正元组的负元组。令FP为假正例的个数。</li><li>假负例/假阴性(False Negative, FN)：指被分类器错误标记为负元组的正元组。令FN为假负例的个数。</li><li>正元组数(Positive, P)：样本中实际的正元组数。</li><li>负元组数(Negative, N)：样本中实际的负元组数。</li><li>P’：被分类器分为正元组的样本数。</li><li>N’：被分类器分为负元组的样本数。</li><li>真正率(True Positive Rate , TPR)【灵敏度(sensitivity)】：$TPR = TP /(TP + FN)$ ，即正样本预测结果数/ 正样本实际数</li><li>假负率(False Negative Rate , FNR) ：$FNR = FN /(TP + FN)$ ，即被预测为负的正样本结果数/正样本实际数</li><li>假正率(False Positive Rate , FPR) ：$FPR = FP /(FP + TN)$ ，即被预测为正的负样本结果数 /负样本实际数</li><li>真负率(True Negative Rate , TNR)【特指度(specificity)】：$TNR = TN /(TN + FP)$ ，即负样本预测结果数 / 负样本实际数</li><li>精确度(Precision): $P = TP/(TP+FP)$ . 精度就是我选择的这一类就是正确的概率是多少。力求 识别出来的的视频中，绝大部分都不含色情视频，“宁放过大部分视频，不错一个视频含sex”</li><li>召回率(Recall): R = TP/(TP+FN)，即真正率。召回率是选择的这一类是正确的，占实际总体的概率是多少。力求 在所有的病人中，都能识别出这些病人，“宁错诊有病，不放过一个真病”。</li><li>F-score：查准率和查全率的调和平均值, 更接近于P, R两个数较小的那个: $F=2* P* R/(P + R)$</li><li>准确率(Aaccuracy): 分类器对整个样本的判定能力,即将正的判定为正，负的判定为负: $A = (TP + TN)/(TP + FN + FP + TN)$</li><li>ROC(Receiver Operating Characteristic):ROC的主要分析工具是一个画在ROC空间的曲线——ROC curve，横坐标为false positive rate(FPR)，纵坐标为true positive rate(TPR)。”In signal detection theory, a receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as <strong>its discrimination threshold</strong> is varied.””</li><li>AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。<img src="/images/class_metric2.png" width = 100% height = 100% div align=center /></li><li>为什么使用ROC曲线？<br>既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候,”<a href="https://www.quora.com/Why-do-we-use-ROC-curves" target="_blank" rel="noopener">AUC</a>. It is already computed by varying class priors. Does not get affected much by size of data in a particular class”.。在实际的数据集中经常会出现类不平衡(class imbalance)现象，即负样本比正样本多很多(或者相反)，而且测试数据中的正负样本的分布也可能随着时间变化.  还有一个就是可以随着阈值的变化。</li></ol><img src="/images/class_metric.png" width = 100% height = 100% div align=center /><p>reference:<br><a href="https://www.cnblogs.com/gatherstars/p/6084696.html" target="_blank" rel="noopener">https://www.cnblogs.com/gatherstars/p/6084696.html</a>   <a href="https://blog.csdn.net/pipisorry/article/details/51788927#commentBox" target="_blank" rel="noopener">https://blog.csdn.net/pipisorry/article/details/51788927#commentBox</a><br><a href="http://blog.sina.com.cn/s/blog_629e606f0102v7a0.html" target="_blank" rel="noopener">http://blog.sina.com.cn/s/blog_629e606f0102v7a0.html</a>  </p><h3 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h3><p>机器学习中发生过拟合的主要原因有：</p><ol><li>使用过于复杂的模型；</li><li>数据噪声较大；</li><li>训练数据少。</li></ol><p>由此对应的降低过拟合的方法有：</p><ol><li>简化模型假设，或者使用惩罚项限制模型复杂度；</li><li>进行数据清洗，减少噪声；</li><li>收集更多训练数据。</li></ol><h3 id="梯度下降了解多少"><a href="#梯度下降了解多少" class="headerlink" title="梯度下降了解多少"></a>梯度下降了解多少</h3><p>three variants of gradient descent, among which mini-batch gradient descent is the most popular：</p><ol><li>批量梯度下降（batch Grandient descent）. BGD 是梯度下降算法最原始的形式, 其特点是每次更新参数 ω 时, 都使用整个训练集的数据.</li><li>随机梯度下降（stochastic grandient descent）.SGD 每次以一个样本, 而不是整个数据集来计算梯度.</li><li>小批量梯度下降（mini-batch grandient descent）. MBGD 是为解决 BGD 与 SGD 各自缺点而发明的折中算法, 或者说它利用了 BGD 和 SGD 各自优点. 其基本思想是: 每次更新参数时, 使用 n 个样本, 既不是全部, 也不是 1. (SGD 可以看成是 n=1 的 MBGD 的一个特例)</li></ol><p>|梯度下降算法|    优点|    缺点|<br>|:—–|:—-|:—-|:—|<br>|BGD    |全局最优解    |计算量大, 迭代速度慢, 训练速度慢<br>|SGD    |1.训练速度快 2. 支持在线学习    |准确度下降, 有噪声, 非全局最优解|<br>|MBGD |1. 训练速度较快, 取决于小批量的数目  2. 支持在线学习    |准确度不如 BGD, 仍然有噪声, 非全局最优解</p><p><a href="http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants" target="_blank" rel="noopener">Algorithms</a> that are most commonly used for optimizing SGD:</p><ul><li>Momentum.<br>当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程</li><li>Nesterov accelerated gradient.<br>NAG is a way to give our <strong>momentum</strong> term this kind of prescience. 对冲量梯度下降算法的改进版本，其速度更快。其变化之处在于计算“超前梯度”更新冲量项</li><li>Adagrad.<br>It adapts the <strong>learning rate</strong> to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data.</li><li>Adadelta.<br>Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w.</li><li>RMSprop<br>RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad’s radically diminishing learning rates.主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行<a href="https://blog.csdn.net/u013709270/article/details/78667531" target="_blank" rel="noopener">衰减</a></li><li>Adam<br>Adaptive Moment Estimation (Adam) [15] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients vt like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.<br>其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体.</li></ul><p>梯度下降需要关注的参数调参：</p><ul><li>学习率 learn rate $a$.</li><li>学习率衰减 decay</li><li>冲量 momentum</li><li>参数初始值（常常random）</li></ul><p>references:<br><a href="http://kissg.me/2017/07/23/gradient-descent/" target="_blank" rel="noopener">http://kissg.me/2017/07/23/gradient-descent/</a><br><a href="http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants" target="_blank" rel="noopener">http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants</a><br><a href="https://www.cnblogs.com/pinard/p/5970503.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/5970503.html</a><br><a href="https://blog.csdn.net/u013709270/article/details/78667531" target="_blank" rel="noopener">https://blog.csdn.net/u013709270/article/details/78667531</a><br><a href="http://scikit-learn.org/stable/modules/sgd.html" target="_blank" rel="noopener">SGD,Sklearn中应用</a></p><h3 id="Lasso-Ridge"><a href="#Lasso-Ridge" class="headerlink" title="Lasso, Ridge"></a>Lasso, Ridge</h3><blockquote><p>然后针对L2范数 $\Phi \left ( w \right ) = \sum_{j=1}^{n}w_j^2 $ ，同样对它求导，得到梯度变化为 $∂Φ(w)/∂wj=2wj$ (一般会用$λ2$来把这个系数2给消掉)。同样的更新之后使得$wj$的值不会变得特别大。在机器学习中也将L2正则称为weight decay，在回归问题中，关于L2正则的回归还被称为Ridge Regression岭回归。weight decay还有一个好处，它使得目标函数变为凸函数，梯度下降法和L-BFGS都能收敛到全局最优解。——<a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html#4.2" target="_blank" rel="noopener">逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降）</a></p></blockquote><blockquote><p>Lasso回归和岭回归最重要的区别是，岭回归中随着惩罚项增加时，所以项都会减小，但是仍然保持非0的状态，然而Lasso回归中，随着惩罚项的增加时，越来越多的参数会直接变为0，正是这个优势使得lasso回归容易用作特征的选择（对应参数非0项），因此lasso回归可以说能很好的保留那些具有重要意义的特征而去掉那些那些意义不大甚至毫无意义的特征（如果是超多维的稀疏矩阵，这难道不是在垃圾中寻找黄金的“掘金术”吗？），而岭回归永远不会认为一个特征是毫无意义的。 —— <a href="https://blog.csdn.net/qq_34531825/article/details/52689654" target="_blank" rel="noopener">Spark2.0机器学习系列之12： 线性回归及L1、L2正则化区别与稀疏解</a></p></blockquote><blockquote><p>正则化参数等价于对参数引入 先验分布，使得 模型复杂度 变小（缩小解空间），对于噪声以及outliers的鲁棒性增强（泛化能力）。整个最优化问题从贝叶斯观点来看是一种贝叶斯最大后验估计，其中 正则化项 对应后验估计中的 先验信息 ，损失函数对应后验估计中的似然函数，两者的乘积即对应贝叶斯最大后验估计的形式。——<a href="http://charleshm.github.io/2016/03/Regularized-Regression/" target="_blank" rel="noopener">Regularized Regression: A Bayesian point of view</a></p></blockquote><p>总结作用：</p><ul><li>解决共线性问题</li><li>解决变量多于数据数的情况</li><li>解决过拟合的问题，不至于使得参数取的过大，过小等问题</li><li>解决选取特征（lasso）</li></ul><p>L1_lasso 优缺点：</p><ul><li>简化模型复杂度。因为引入参数的先验分布，拉普拉斯分布</li><li>善于处理稀疏数据，选取有用特征</li><li>缺点：处理速度快，但可能牺牲了一些准确性</li></ul><p>L2_Ridge 优缺点：</p><ul><li>简化模型复杂度，因为引入参数的先验分布，高斯分布</li><li>解决logistic regression共线性问题。</li><li>解决样本点比较少，而特征比较多，特征个数多于样本个数的情况。</li></ul><p><a href="https://blog.csdn.net/qq_34531825/article/details/52689654" target="_blank" rel="noopener">Spark2.0机器学习系列之12： 线性回归及L1、L2正则化区别与稀疏解</a></p><p><a href="https://blog.csdn.net/kejiaming/article/details/64439664" target="_blank" rel="noopener">逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降）</a></p><p><a href="https://www.zhihu.com/question/23536142/answer/90135994" target="_blank" rel="noopener">贝叶斯角度Regularized Regression: A Bayesian point of view</a></p><p><a href="http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression" target="_blank" rel="noopener">http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression</a></p><h3 id="什么是共线性问题"><a href="#什么是共线性问题" class="headerlink" title="什么是共线性问题"></a>什么是共线性问题</h3><p>共线性问题对线性回归模型有如下影响：</p><ul><li>参数的方差增大；</li><li>难以区分每个解释变量的单独影响；</li><li>变量的显著性检验失去意义；</li><li>回归模型缺乏稳定性。样本的微小扰动都可能带来参数很大的变化；</li><li>影响模型的泛化误差。</li></ul><p>共线性问题的解决方法:</p><ul><li>增加数据</li><li>对模型施加某些约束条件（L2,L1）</li><li>删除一个或几个共线变量</li><li>将模型适当变形</li><li>主成分回归，降维</li></ul><p><a href="https://www.jianshu.com/p/ef1b27b8aee0?from=timeline" target="_blank" rel="noopener">讲讲共线性问题</a></p><p><a href="https://blog.csdn.net/diyiziran/article/details/17025471" target="_blank" rel="noopener">多重共线性的产生原因、判别、检验、解决方法</a></p><h3 id="哪些算法可以online-learning"><a href="#哪些算法可以online-learning" class="headerlink" title="哪些算法可以online learning"></a>哪些算法可以online learning</h3><h3 id="Gradient-boosting"><a href="#Gradient-boosting" class="headerlink" title="Gradient boosting"></a>Gradient boosting</h3><p><a href="http://www.cnblogs.com/willnote/p/6801496.html" target="_blank" rel="noopener">Gradient boosting</a></p><h3 id="GBDT-vs-Random-forestes"><a href="#GBDT-vs-Random-forestes" class="headerlink" title="GBDT vs Random forestes"></a>GBDT vs Random forestes</h3><p>GBDT是计算每个模型训练后的残差<br><a href="https://blog.csdn.net/login_sonata/article/details/73929426" target="_blank" rel="noopener">随机森林和GBDT的区别</a></p><h3 id="GMM-Gaussian-mixture-model"><a href="#GMM-Gaussian-mixture-model" class="headerlink" title="GMM(Gaussian mixture model)"></a>GMM(Gaussian mixture model)</h3><p>假设模型服从几个高斯分布相叠加结果。计算这个点属于每个模型的概率，哪个最大就是属于哪一类的。</p><p>一个简单的解释：<br><a href="https://zhuanlan.zhihu.com/p/31103654" target="_blank" rel="noopener">一文详解高斯混合模型原理</a></p><p>公式：高斯模型叠加<br>$$p(x)=\sum_{k=1}^{K}p(k)p(x|k)=\sum_{k=1}^{K}\phi_{k}N(x|\mu_{k},\varepsilon_{k})$$</p><p>利用<a href="https://www.cnblogs.com/Gabby/p/5344658.html" target="_blank" rel="noopener">EM算法</a>来更新迭代数据，有三个数据需要更新，$\phi_{k}, \mu_{k}, \varepsilon_{k}$</p><img src="/images/GMM.png" width = 80% height = 80% div align=center /><ul><li>与Kmeans的<a href="https://blog.csdn.net/tingyue_/article/details/70739671" target="_blank" rel="noopener">关系</a>：<br>K-Means算法其实是GMM的EM解法在高斯分量协方差ϵI→0时的一个特例，GMM输出的是数据点属于每个每类的概率，我们用最大似然方法去确定分类。就严谨性来说，用概率进行描述数据点的分类，GMM显然要比K-mean好很多。<br>实际应用中，对于 K-means，我们通常是重复一定次数然后取最好的结果，但由于 GMM 每一次迭代的计算量比 K-means 要大许多，使用GMM时，一个更流行的做法是先用 K-means （已经重复并取最优值了）得到一个粗略的结果，然后将其作为初值（只要将 K-means 所得的 聚类中心传给 GMM即可），再用 GMM 进行细致迭代。</li></ul><p>Reference:<br><a href="https://blog.csdn.net/jinping_shi/article/details/59613054" target="_blank" rel="noopener">高斯混合模型（GMM）及其EM算法的理解</a><br><a href="https://blog.csdn.net/llp1992/article/details/47058109" target="_blank" rel="noopener">https://blog.csdn.net/llp1992/article/details/47058109</a><br><a href="https://blog.csdn.net/tingyue_/article/details/70739671" target="_blank" rel="noopener">https://blog.csdn.net/tingyue_/article/details/70739671</a></p><h3 id="EM-算法"><a href="#EM-算法" class="headerlink" title="EM 算法"></a>EM 算法</h3><p>数学原理：<br>求被选到的概率最大。</p><p><a href="http://blog.51cto.com/9269309/1892833" target="_blank" rel="noopener">http://blog.51cto.com/9269309/1892833</a><br><a href="https://blog.csdn.net/linyanqing21/article/details/50939009" target="_blank" rel="noopener">https://blog.csdn.net/linyanqing21/article/details/50939009</a></p><h3 id="SVM推导，对偶性的作用，核函数有哪些，有什么区别"><a href="#SVM推导，对偶性的作用，核函数有哪些，有什么区别" class="headerlink" title="SVM推导，对偶性的作用，核函数有哪些，有什么区别"></a>SVM推导，对偶性的作用，核函数有哪些，有什么区别</h3><p>确定下界 最大化</p><p><a href="https://www.cnblogs.com/xxrxxr/p/7536131.html" target="_blank" rel="noopener">关于SVM数学细节逻辑的个人理解（二）：从基本形式转化为对偶问题</a></p><p><a href="http://www.hanlongfei.com/convex/2015/11/05/duality/" target="_blank" rel="noopener">http://www.hanlongfei.com/convex/2015/11/05/duality/</a></p><p><a href="https://blog.csdn.net/Sunshine_in_Moon/article/details/51321461" target="_blank" rel="noopener">https://blog.csdn.net/Sunshine_in_Moon/article/details/51321461</a></p><p><a href="https://www.zhihu.com/question/58584814" target="_blank" rel="noopener">https://www.zhihu.com/question/58584814</a></p><h3 id="bp算法介绍，梯度弥散问题。"><a href="#bp算法介绍，梯度弥散问题。" class="headerlink" title="bp算法介绍，梯度弥散问题。"></a>bp算法介绍，梯度弥散问题。</h3><h3 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h3><p>以后如果有机会多学下NLP的话，就专门写一片自然语言教程。<br>先记下一些看过的非常好的教程。<br><a href="http://www.sohu.com/a/221418079_817016" target="_blank" rel="noopener">一文读懂自然语言处理（NLP）入门学习要点</a><br><a href="https://www.jiqizhixin.com/articles/081203?from=synced&keyword=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86" target="_blank" rel="noopener">自然语言处理是如何工作的？一步步教你构建 NLP 流水线</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data structure and algorithm(1)</title>
      <link href="/2018/08/16/Algorithm-1/"/>
      <url>/2018/08/16/Algorithm-1/</url>
      
        <content type="html"><![CDATA[<p>Notes from Udacity Data Science Interview Preparation lesson.</p><ul><li>big O Notation<br><a href="http://bigocheatsheet.com/" target="_blank" rel="noopener">http://bigocheatsheet.com/</a></li></ul><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><h3 id="The-difference-between-Array-and-list"><a href="#The-difference-between-Array-and-list" class="headerlink" title="The difference between Array and list"></a>The difference between Array and list</h3><ul><li>Array has index</li><li>List has order but no indices</li><li>Python list is built as an array</li></ul><h3 id="linked-lists"><a href="#linked-lists" class="headerlink" title="linked lists"></a>linked lists</h3><ul><li>will tell you where is the next list</li><li>the same as array is that store the value information</li><li>the difference with array is that they store the different information. array store the index inforation, and linked list store a reference to the next element in the list.<img src="/images/linked_list.png" width = 80% height = 80% div align=center /></li></ul><p>when coding：<br> it has two parts in one node. one of them is value of linked list(self.value), another is pointer(self.next) which point to next list.<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lass Node():</span><br><span class="line">   __slots__&#x3D;[&#39;_item&#39;,&#39;_next&#39;]    #限定Node实例的属性</span><br><span class="line">   def __init__(self,item):</span><br><span class="line">       self._item&#x3D;item</span><br><span class="line">       self._next&#x3D;None     #Node的指针部分默认指向None</span><br><span class="line">   def getItem(self):</span><br><span class="line">       return self._item</span><br><span class="line">   def getNext(self):</span><br><span class="line">       return self._next</span><br><span class="line">   def setItem(self,newitem):</span><br><span class="line">       self._item&#x3D;newitem</span><br><span class="line">   def setNext(self,newnext):</span><br><span class="line">       self._next&#x3D;newnext</span><br></pre></td></tr></table></figure><br>reference:<br> <a href="http://python.jobbole.com/83953/" target="_blank" rel="noopener">python数据结构——链表的实现</a> <a href="http://www.cnblogs.com/linxiyue/p/3551633.html" target="_blank" rel="noopener">http://www.cnblogs.com/linxiyue/p/3551633.html</a></p><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><ul><li>like stack pancake, list-based data structure. But different from linked_list and Array</li><li>LIFO: last in first out.</li><li>PUSH:insert； POP：remove</li></ul><h3 id="Queues"><a href="#Queues" class="headerlink" title="Queues"></a>Queues</h3><ul><li>FIFO: first in first out; list-based</li><li>dequeues or double-end queues can dequeue and enqueue in both head and tail<img src="/images/queues.png" width = 80% height = 80% div align=center /></li><li>priority queue. dequeue with highest priority(1). if have same priority, dequeue the oldest one.<img src="/images/priority_queue.png" width = 80% height = 80% div align=center /></li></ul><h3 id="sets"><a href="#sets" class="headerlink" title="sets"></a>sets</h3><ul><li>the difference from list is that it don’t have orders, but can’t have repeated elements</li></ul><h3 id="Maps-dictionary"><a href="#Maps-dictionary" class="headerlink" title="Maps(dictionary)"></a>Maps(dictionary)</h3><ul><li>like dictionary. Maps = &lt;key, value&gt;. a group of keys is a set.</li><li>Dictionaries are wonderfully flexible—you can store a wide variety of structures as values. You store another dictionary or a list:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Cities to add:</span><br><span class="line">Bangalore (India, Asia)</span><br><span class="line">Atlanta (USA, North America)</span><br><span class="line">Cairo (Egypt, Africa)</span><br><span class="line">Shanghai (China, Asia)&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">locations &#x3D; &#123;&#39;North America&#39;: &#123;&#39;USA&#39;: [&#39;Mountain View&#39;]&#125;&#125;</span><br><span class="line">locations[&#39;North America&#39;][&#39;USA&#39;].append(&#39;Atlanta&#39;)</span><br><span class="line">locations[&#39;Asia&#39;] &#x3D; &#123;&#39;Inida&#39;:[&#39;Bangalore&#39;]&#125;</span><br><span class="line">locations[&#39;Asia&#39;][&#39;China&#39;]&#x3D; [&#39;Shanghai&#39;]</span><br><span class="line">locations[&#39;Africa&#39;] &#x3D; &#123;&#39;Egypt&#39;: [&#39;Cairo&#39;]&#125;</span><br><span class="line"></span><br><span class="line">for city in sorted(locations[&#39;North America&#39;][&#39;USA&#39;]):</span><br><span class="line">    print city</span><br><span class="line"></span><br><span class="line">m &#x3D; []    </span><br><span class="line">for country, city in locations[&#39;Asia&#39;].iteritems():</span><br><span class="line">    m.append(city[0]+&#39;-&#39;+ country)</span><br><span class="line">for listm in sorted(m):</span><br><span class="line">   print listm</span><br></pre></td></tr></table></figure><h3 id="Hashing"><a href="#Hashing" class="headerlink" title="Hashing"></a>Hashing</h3><p>Using a data structure that employs a hash function allows you to look up in constant time.</p><ul><li>using the reminder as index, store this value at this place  <img src="/images/hashing1.png" width = 80% height = 80% div align=center /></li><li>if have collisions, two ways: 1, change hash function; 2, store all the value into one collection, the array will turn to ‘bucket’.  <img src="/images/hashing2.png" width = 60% height = 60% div align=center /></li><li>in bucket, still can apply hash function  <img src="/images/hashing3.png" width = 60% height = 60% div align=center /></li><li>For the load factor, you should divide the number of values by the number of buckets. $Load Factor = Number of Entries / Number of Buckets$<br><a href="https://classroom.udacity.com/courses/ud944/lessons/7118294395/concepts/79336691870923" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud944/lessons/7118294395/concepts/79336691870923</a><img src="/images/hashing4.png" width = 50% height = 50% div align=center /><img src="/images/hashing5.png" width = 50% height = 50% div align=center /></li></ul><h3 id="Hash-Maps"><a href="#Hash-Maps" class="headerlink" title="Hash Maps"></a>Hash Maps</h3><img src="/images/hash_maps.png" width = 80% height = 80% div align=center />- In interview, you will be asked about creating "Hash table" to test your understand hashing. also need to know the upside and downside of a hash function<h3 id="Hash-for-String-Keys"><a href="#Hash-for-String-Keys" class="headerlink" title="Hash for String Keys"></a>Hash for String Keys</h3><ul><li>string turn into ASCLL, A=65, B=66,C=67</li></ul><img src="/images/string_keys.png" width = 80% height = 80% div align=center />- why use 31, it's more likely convention.  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">example: Hash Value &#x3D; (ASCII Value of First Letter * 100) + ASCII Value of Second Letter</span><br><span class="line">class HashTable(object):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.table &#x3D; [None]*10000</span><br><span class="line"></span><br><span class="line">    def store(self, string):</span><br><span class="line">        hv &#x3D; self.calculate_hash_value(string)</span><br><span class="line">        if hv !&#x3D; -1:</span><br><span class="line">            if self.table[hv] !&#x3D; None:</span><br><span class="line">                self.table[hv].append(string)</span><br><span class="line">            else:</span><br><span class="line">                self.table[hv] &#x3D; [string]</span><br><span class="line"></span><br><span class="line">    def lookup(self, string):</span><br><span class="line">        hv &#x3D; self.calculate_hash_value(string)</span><br><span class="line">        if hv !&#x3D; -1:</span><br><span class="line">            if self.table[hv] !&#x3D; None:</span><br><span class="line">                if string in self.table[hv]:</span><br><span class="line">                    return hv</span><br><span class="line">        return -1</span><br><span class="line"></span><br><span class="line">    def calculate_hash_value(self, string):</span><br><span class="line">        value &#x3D; ord(string[0])*100 + ord(string[1])</span><br><span class="line">        return value</span><br></pre></td></tr></table></figure><h2 id="Trees-Data-Structure"><a href="#Trees-Data-Structure" class="headerlink" title="Trees_Data Structure"></a>Trees_Data Structure</h2><ul><li>tree is an extension of linked-list</li><li>the following figure, different color has its meaning.  <img src="/images/tree.png" width = 80% height = 80% div align=center /></li></ul><h3 id="Tree-Traversal"><a href="#Tree-Traversal" class="headerlink" title="Tree Traversal"></a>Tree Traversal</h3><ul><li><p>DFS(depth first search):</p><ul><li><p>pre-order  </p><img src="/images/DFS_preorder.png" width = 80% height = 80% div align=center /></li><li><p>in-order  </p><img src="/images/DFS_inorder.png" width = 80% height = 80% div align=center /></li><li><p>post-order  (right first)</p><img src="/images/DFS_postorder.png" width = 80% height = 80% div align=center /></li><li><p>Q&amp;A: D, F, E, B, C, A</p><img src="/images/tree2.png" width = 80% height = 80% div align=center /></li></ul></li><li><p>BFS(Breadth first search): check the same level node first</p></li><li><p>Search and Delete, insert. perfect tree, each node has two child  </p><img src="/images/tree3.png" width = 80% height = 80% div align=center /></li></ul><h3 id="Binary-search-tree-BST"><a href="#Binary-search-tree-BST" class="headerlink" title="Binary search tree(BST)"></a>Binary search tree(BST)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class BinaryTree(object):</span><br><span class="line">    def __init__(self, root):</span><br><span class="line">        self.root &#x3D; Node(root)</span><br><span class="line"></span><br><span class="line">    def search(self, find_val):</span><br><span class="line">        return self.preorder_search(tree.root, find_val)</span><br><span class="line"></span><br><span class="line">    def print_tree(self):</span><br><span class="line">        return self.preorder_print(tree.root, &quot;&quot;)[:-1]</span><br><span class="line"></span><br><span class="line">    def preorder_search(self, start, find_val):</span><br><span class="line">        if start:</span><br><span class="line">            if start.value &#x3D;&#x3D; find_val:</span><br><span class="line">                return True</span><br><span class="line">            else:</span><br><span class="line">                return self.preorder_search(start.left, find_val) or self.preorder_search(start.right, find_val)</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">    def preorder_print(self, start, traversal):</span><br><span class="line">        if start:</span><br><span class="line">            traversal +&#x3D; (str(start.value) + &quot;-&quot;)</span><br><span class="line">            traversal &#x3D; self.preorder_print(start.left, traversal)</span><br><span class="line">            traversal &#x3D; self.preorder_print(start.right, traversal)</span><br><span class="line">        return traversal</span><br></pre></td></tr></table></figure><ul><li><p>rule to BSTS: right of node is larger than left</p><img src="/images/tree4.png" width = 80% height = 80% div align=center /></li><li><p>unbalanced BSTS  </p><img src="/images/tree5.png" width = 80% height = 80% div align=center /></li><li><p>apply the rule to BSTS and pre-order Traversal. Search and insert  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">class BST(object):</span><br><span class="line">    def __init__(self, root):</span><br><span class="line">        self.root &#x3D; Node(root)</span><br><span class="line"></span><br><span class="line">    def insert(self, new_val):</span><br><span class="line">        self.insert_helper(self.root, new_val)</span><br><span class="line"></span><br><span class="line">    def insert_helper(self, current, new_val):</span><br><span class="line">        if current.value &lt; new_val:</span><br><span class="line">            if current.right:</span><br><span class="line">                self.insert_helper(current.right, new_val)</span><br><span class="line">            else:</span><br><span class="line">                current.right &#x3D; Node(new_val)</span><br><span class="line">        else:</span><br><span class="line">            if current.left:</span><br><span class="line">                self.insert_helper(current.left, new_val)</span><br><span class="line">            else:</span><br><span class="line">                current.left &#x3D; Node(new_val)</span><br><span class="line"></span><br><span class="line">    def search(self, find_val):</span><br><span class="line">        return self.search_helper(self.root, find_val)</span><br><span class="line"></span><br><span class="line">    def search_helper(self, current, find_val):</span><br><span class="line">        if current:</span><br><span class="line">            if current.value &#x3D;&#x3D; find_val:</span><br><span class="line">                return True</span><br><span class="line">            elif current.value &lt; find_val:</span><br><span class="line">                return self.search_helper(current.right, find_val)</span><br><span class="line">            else:</span><br><span class="line">                return self.search_helper(current.left, find_val)</span><br><span class="line">        return False</span><br></pre></td></tr></table></figure></li></ul><h3 id="Heaps"><a href="#Heaps" class="headerlink" title="Heaps"></a>Heaps</h3><ul><li>also a kind of tree has its own rule</li><li>Max heaps and min heaps. max: parent always big than child; min vice versa. <a href="https://www.youtube.com/watch?time_continue=14&v=M3B0UJWS_ag" target="_blank" rel="noopener">link</a><img src="/images/maxheaps.png" width = 80% height = 80% div align=center /></li><li>heaps can have several trees, not like binary</li><li>heaps binary trees search big 0 time is $O(n)$</li><li>heapify: insert in the child leaf first, then compare it to the parent, if bigger then swap their value. The big O time is $O(logn)$<img src="/images/heapify.png" width = 80% height = 80% div align=center /></li><li>heap implementation: heap often store as sorted array. as shown below, array only store value and index, but tree will store bunch of pointers, which mean array save space.<img src="/images/heap2.png" width = 80% height = 80% div align=center /><img src="/images/heap3.png" width = 80% height = 80% div align=center /></li></ul><h3 id="self-balanced-tree-red-black"><a href="#self-balanced-tree-red-black" class="headerlink" title="self-balanced tree(red-black)"></a>self-balanced tree(red-black)</h3><img src="/images/self_balanceing.png" width = 80% height = 80% div align=center /><ul><li><p>definition: minimize the level of tree</p></li><li><p><strong>Red Black-Tree</strong> an extension of Binary tree. 5 rules</p><ul><li>every node is red or black</li><li>Every leaf (NULL) is black. Every node in your tree doesn’t otherwise have two leaves, must have null children(black)</li><li>If a node is red, then both its children are black.</li><li>the root node is black</li><li>Every simple path from a node to a descendant leaf contains the same number of black nodes.  </li></ul><img src="/images/red_black_tree.png" width = 80% height = 80% div align=center /></li></ul><p>still watching:   <a href="https://classroom.udacity.com/courses/ud944/lessons/7122604912/concepts/78867246220923" target="_blank" rel="noopener">https://classroom.udacity.com/courses/ud944/lessons/7122604912/concepts/78867246220923</a><br>  <a href="https://www.youtube.com/watch?v=O5Yl-m0YbVA" target="_blank" rel="noopener">https://www.youtube.com/watch?v=O5Yl-m0YbVA</a></p><h2 id="Graphs-Networks"><a href="#Graphs-Networks" class="headerlink" title="Graphs(Networks)"></a>Graphs(Networks)</h2><img src="/images/graph.png" width = 80% height = 80% div align=center />- tree is a type of graph.- Node, edge  <img src="/images/graph2.png" width = 80% height = 80% div align=center /><h3 id="directions-and-cycles"><a href="#directions-and-cycles" class="headerlink" title="directions and cycles"></a>directions and cycles</h3><ul><li><p>directions like this figure show</p></li><li><p>avoid graphs has cycles, infinite loop  </p><img src="/images/graph3.png" width = 80% height = 80% div align=center /></li><li><p>DAG: directed Acyclic(no cycles) Graph. type show up often</p></li></ul><h3 id="connectivity"><a href="#connectivity" class="headerlink" title="connectivity"></a>connectivity</h3><img src="/images/graph4.png" width = 80% height = 80% div align=center />- right seems has more connection than left one.- remove one connection in left group, whole group will destoryed(disconnected)<h4 id="definition"><a href="#definition" class="headerlink" title="definition"></a>definition</h4><ul><li><p>disconnected:<br>Disconnected graphs are very similar whether the graph’s directed or undirected—there is some vertex or group of vertices that have no connection with the rest of the graph.</p></li><li><p>weakly connected:<br>A directed graph is weakly connected when only replacing all of the directed edges with undirected edges can cause it to be connected. Imagine that your graph has several vertices with one outbound edge, meaning an edge that points from it to some other vertex in the graph. There’s no way to reach all of those vertices from any other vertex in the graph, but if those edges were changed to be undirected all vertices would be easily accessible.</p></li><li><p>connected:<br>Here we only use “connected graph” to refer to undirected graphs. In a connected graph, there is some path between one vertex and every other vertex.</p></li><li><p>Strongly Connected:<br>Strongly connected directed graphs must have a path from every node and every other node. So, there must be a path from A to B AND B to A.</p></li></ul><h4 id="Graph-Representations"><a href="#Graph-Representations" class="headerlink" title="Graph Representations"></a>Graph Representations</h4><p>vertex orbject can have a list of edges it’s connected to and vice versa.<br><img src="/images/graph5.png" width = 80% height = 80% div align=center /></p><p>There are other graph represent ways:</p><ul><li><p>Edge list, could be 2D, and 3D</p><img src="/images/graph6.png" width = 80% height = 80% div align=center /></li><li><p>Adjacency list, store the node that adjacent to index  </p><img src="/images/graph7.png" width = 80% height = 80% div align=center /></li><li><p>Adjacency Matrix, store a matrix, if node 0 connect to node 1, then show 1, otherwise show 0 in matrix.  </p><img src="/images/graph8.png" width = 80% height = 80% div align=center /></li></ul><p>code from udacity <a href="https://classroom.udacity.com/courses/ud944/lessons/7114284829/concepts/79348548570923" target="_blank" rel="noopener">link</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">class Node(object):</span><br><span class="line">    def __init__(self, value):</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line">        self.edges &#x3D; []</span><br><span class="line"></span><br><span class="line">class Edge(object):</span><br><span class="line">    def __init__(self, value, node_from, node_to):</span><br><span class="line">        self.value &#x3D; value</span><br><span class="line">        self.node_from &#x3D; node_from</span><br><span class="line">        self.node_to &#x3D; node_to</span><br><span class="line"></span><br><span class="line">class Graph(object):</span><br><span class="line">    def __init__(self, nodes&#x3D;[], edges&#x3D;[]):</span><br><span class="line">        self.nodes &#x3D; nodes</span><br><span class="line">        self.edges &#x3D; edges</span><br><span class="line"></span><br><span class="line">    def insert_node(self, new_node_val):</span><br><span class="line">        new_node &#x3D; Node(new_node_val)</span><br><span class="line">        self.nodes.append(new_node)</span><br><span class="line"></span><br><span class="line">    def insert_edge(self, new_edge_val, node_from_val, node_to_val):</span><br><span class="line">        from_found &#x3D; None</span><br><span class="line">        to_found &#x3D; None</span><br><span class="line">        for node in self.nodes:</span><br><span class="line">            if node_from_val &#x3D;&#x3D; node.value:</span><br><span class="line">                from_found &#x3D; node</span><br><span class="line">            if node_to_val &#x3D;&#x3D; node.value:</span><br><span class="line">                to_found &#x3D; node</span><br><span class="line">        if from_found &#x3D;&#x3D; None:</span><br><span class="line">            from_found &#x3D; Node(node_from_val)</span><br><span class="line">            self.nodes.append(from_found)</span><br><span class="line">        if to_found &#x3D;&#x3D; None:</span><br><span class="line">            to_found &#x3D; Node(node_to_val)</span><br><span class="line">            self.nodes.append(to_found)</span><br><span class="line">        new_edge &#x3D; Edge(new_edge_val, from_found, to_found)</span><br><span class="line">        from_found.edges.append(new_edge)</span><br><span class="line">        to_found.edges.append(new_edge)</span><br><span class="line">        self.edges.append(new_edge)</span><br><span class="line"></span><br><span class="line">    def get_edge_list(self):</span><br><span class="line">        edge_list &#x3D; []</span><br><span class="line">        for edge_object in self.edges:</span><br><span class="line">            edge &#x3D; (edge_object.value, edge_object.node_from.value, edge_object.node_to.value)</span><br><span class="line">            edge_list.append(edge)</span><br><span class="line">        return edge_list</span><br><span class="line"></span><br><span class="line">    def get_adjacency_list(self):</span><br><span class="line">        max_index &#x3D; self.find_max_index()</span><br><span class="line">        adjacency_list &#x3D; [None] * (max_index + 1)</span><br><span class="line">        for edge_object in self.edges:</span><br><span class="line">            if adjacency_list[edge_object.node_from.value]:</span><br><span class="line">                adjacency_list[edge_object.node_from.value].append((edge_object.node_to.value, edge_object.value))</span><br><span class="line">            else:</span><br><span class="line">                adjacency_list[edge_object.node_from.value] &#x3D; [(edge_object.node_to.value, edge_object.value)]</span><br><span class="line">        return adjacency_list</span><br><span class="line"></span><br><span class="line">    def get_adjacency_matrix(self):</span><br><span class="line">        max_index &#x3D; self.find_max_index()</span><br><span class="line">        adjacency_matrix &#x3D; [[0 for i in range(max_index + 1)] for j in range(max_index + 1)]</span><br><span class="line">        for edge_object in self.edges:</span><br><span class="line">            adjacency_matrix[edge_object.node_from.value][edge_object.node_to.value] &#x3D; edge_object.value</span><br><span class="line">        return adjacency_matrix</span><br><span class="line"></span><br><span class="line">    def find_max_index(self):</span><br><span class="line">        max_index &#x3D; -1</span><br><span class="line">        if len(self.nodes):</span><br><span class="line">            for node in self.nodes:</span><br><span class="line">                if node.value &gt; max_index:</span><br><span class="line">                    max_index &#x3D; node.value</span><br><span class="line">        return max_index</span><br><span class="line"></span><br><span class="line">graph &#x3D; Graph()</span><br><span class="line">graph.insert_edge(100, 1, 2)</span><br><span class="line">graph.insert_edge(101, 1, 3)</span><br><span class="line">graph.insert_edge(102, 1, 4)</span><br><span class="line">graph.insert_edge(103, 3, 4)</span><br><span class="line"># Should be [(100, 1, 2), (101, 1, 3), (102, 1, 4), (103, 3, 4)]</span><br><span class="line">print graph.get_edge_list()</span><br><span class="line"># Should be [None, [(2, 100), (3, 101), (4, 102)], None, [(4, 103)], None]</span><br><span class="line">print graph.get_adjacency_list()</span><br><span class="line"># Should be [[0, 0, 0, 0, 0], [0, 0, 100, 101, 102], [0, 0, 0, 0, 0], [0, 0, 0, 0, 103], [0, 0, 0, 0, 0]]</span><br><span class="line">print graph.get_adjacency_matrix()</span><br></pre></td></tr></table></figure><h4 id="Graph-Traversal"><a href="#Graph-Traversal" class="headerlink" title="Graph Traversal"></a>Graph Traversal</h4><ul><li>just like tree traversal</li><li>DFS(depth first search)</li><li>BFS(breadth first search)</li></ul><h2 id="Algorithm-Searching-and-Sorting"><a href="#Algorithm-Searching-and-Sorting" class="headerlink" title="Algorithm_Searching and Sorting"></a>Algorithm_Searching and Sorting</h2><p>code visualization: <a href="https://visualgo.net/en" target="_blank" rel="noopener">https://visualgo.net/en</a></p><h3 id="Binary-Search"><a href="#Binary-Search" class="headerlink" title="Binary Search"></a>Binary Search</h3><ul><li><p>efficiency: $O(log(n))$</p><img src="/images/binary_search.png" width = 80% height = 80% div align=center /><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def binarySearch(listData, value):</span><br><span class="line">    low &#x3D; 0</span><br><span class="line">    high &#x3D; len(listData)-1</span><br><span class="line">    while (low &lt;&#x3D; high):</span><br><span class="line">       mid &#x3D; (low+high)&#x2F;2</span><br><span class="line">       if (listData[mid] &#x3D; value):</span><br><span class="line">           return mid</span><br><span class="line">       elif(listData[mid] &lt; value):</span><br><span class="line">          low &#x3D; mid+1</span><br><span class="line">       else:</span><br><span class="line">          high &#x3D; mid-1</span><br><span class="line">    return -1</span><br></pre></td></tr></table></figure><p>Resource: <a href="https://www.cs.usfca.edu/~galles/visualization/Search.html" target="_blank" rel="noopener">https://www.cs.usfca.edu/~galles/visualization/Search.html</a></p></li></ul><h3 id="Recursion"><a href="#Recursion" class="headerlink" title="Recursion"></a>Recursion</h3><ul><li>1, call it self; 2, base case(get out the loop); 3, alter the input parament<img src="/images/recursion.png" width = 80% height = 80% div align=center /><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">the Fibonacci Sequence.</span><br><span class="line">def get_fib(position):</span><br><span class="line">    if position &#x3D;&#x3D; 0 or position &#x3D;&#x3D; 1:</span><br><span class="line">        return position</span><br><span class="line">    return get_fib(position - 1) + get_fib(position - 2)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h3><ul><li>in plce or not, in place: lest space; not: lest time. Tradeoff</li></ul><h4 id="bubble-Sort-in-place"><a href="#bubble-Sort-in-place" class="headerlink" title="bubble Sort(in place)"></a>bubble Sort(in place)</h4><ul><li><p>Big O is $(n-1)(n-1)=n^2$, best case is $O(n)$, the array is already sorted.</p><img src="/images/Bubble_sort.png" width = 80% height = 80% div align=center /><p>reference: <a href="https://en.wikipedia.org/wiki/Bubble_sort" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Bubble_sort</a></p></li></ul><h4 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h4><ul><li><p>divide the arrray into one by one, then combine every two element except the first element. Then the combined two element compare first, then compare to others just as following images.  </p><img src="/images/merge_sort1.png" width = 80% height = 80% div align=center /><img src="/images/merge_sort2.png" width = 80% height = 80% div align=center /></li><li><p>Merge sort efficiency: $O(n*logn)$  </p><img src="/images/merge_sort3.png" width = 60% height = 60% div align=center /><img src="/images/merge_sort4.png" width = 60% height = 60% div align=center /><img src="/images/merge_sort5.png" width = 60% height = 60% div align=center /></li></ul><h4 id="Quick-Sort"><a href="#Quick-Sort" class="headerlink" title="Quick Sort"></a>Quick Sort</h4><ul><li>random choose one of element, then compare and move as following figure show.  <img src="/images/quick_sort.png" width = 60% height = 60% div align=center /><img src="/images/quick_sort2.png" width = 60% height = 60% div align=center />- efficiency: the worst case is O(n^2), the average and the best case is $O(nlogn)$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for each (unsorted) partition</span><br><span class="line">set first element as pivot</span><br><span class="line">   storeIndex &#x3D; pivotIndex + 1</span><br><span class="line">   for i &#x3D; pivotIndex + 1 to rightmostIndex</span><br><span class="line">    if element[i] &lt; element[pivot]</span><br><span class="line">      swap(i, storeIndex); storeIndex++</span><br><span class="line">  swap(pivot, storeIndex - 1)</span><br></pre></td></tr></table></figure><p>reference: <a href="http://visualgo.net/en/sorting/" target="_blank" rel="noopener">http://visualgo.net/en/sorting/</a></p><h3 id="some-Resources"><a href="#some-Resources" class="headerlink" title="some Resources"></a>some Resources</h3><p><a href="http://python.jobbole.com/87440/" target="_blank" rel="noopener">常用查找数据结构及算法（Python实现)</a></p>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> Data structure </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>盐湖到黄石6天不完全攻略</title>
      <link href="/2018/08/10/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/"/>
      <url>/2018/08/10/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<p>黄石6天游玩不完全攻略（多图，慎点）</p><h3 id="游玩关键词："><a href="#游玩关键词：" class="headerlink" title="游玩关键词："></a>游玩关键词：</h3><p>盐湖城，荒漠，摩门教，大盐湖；黄石公园，喷泉盆地（geyser basin），大棱镜喷泉，瀑布山川河流草地，徒步hiking，野牛，鹿；大提顿，湖，河，熊</p><h3 id="第1天"><a href="#第1天" class="headerlink" title="第1天"></a>第1天</h3><p>盐湖城有三大特征，荒漠，摩门教，大盐湖。我们是从Boston转机到了盐湖城，然后租车，在盐湖城玩了半天，看了最著名的摩门教圣地。盐湖城是摩门教徒所创建的一座城市。走在摩门教公园里，看见不多的宣教人员，人很友好。盐湖城大约有三分之二信仰摩门教，著名的杨百翰大学就是个摩门教会大学（不在盐湖城）。</p><img src="/images/yellowstone/摩门教.jpg" width = 80% height = 80% div align=center /><p>傍晚的时候,我们在盐湖城市区转了转，去了爵士主场场馆看了看。走在他们中心的街道上，竟然看不到几个人。这就是犹他州最大的城市?!<br><img src="/images/yellowstone/saltlake.jpg" width = 80% /> <img src="/images/yellowstone/saltlake2.jpg" width = 80% /></p><p>然后我们开车到了盐湖城和黄石中间的一个小城，logan住宿，一个超漂亮的农场。<br><img src="/images/yellowstone/农场.jpg" width = 80% height = 80% div align=center /></p><h3 id="第2天"><a href="#第2天" class="headerlink" title="第2天"></a>第2天</h3><p>第二天我们开车从黄石西边入口进入，35刀门票，直达第一个间歇喷泉。第一次见到这种火山形成的地貌奇观，很是兴奋。接下来又看了各种不同的间歇性喷泉，在看完大棱镜之后，就再也提不起看喷泉的兴致了。大棱镜实在太美了。<br><img src="/images/yellowstone/gayser basin.jpg" width = 80% height = 80% div align=center /><br><img src="/images/yellowstone/大棱镜.jpg" width = 80% height = 80% div align=center /></p><p>我们的路线是第一天玩北线，在谷歌trip app上把景点标出来，就可以显示在谷歌地图上，然后离线谷歌地图。黄石没有信号，所以离线地图真的非常重要。<br><img src="/images/yellowstone/猛犸温泉.jpg" width = 80% height = 800% div align=center /><br>北线上面有温泉盆地，最著名的是猛犸温泉景区，很漂亮。不过相比较看岩石喷泉盆地，我更喜欢他周边山谷的小溪，可以去hiking，可以钓鱼，可以淌水。</p><img src="/images/yellowstone/小溪.jpg" width = 80% height = 80% div align=center /><p>晚上回家住在Ashton的一个地方，开车1个半小时，想住在公园里面，要么太贵，要么早被订完了。据说很多都是提前一年预订的。还有一种方式，露营。可以租个房车，可以去营地搭个帐篷，有些地方要预订，有些地方是先到先得。我们比较遗憾，看着别人家的camping很是羡慕。不过Airbnb真的也不错，现在乡村或小镇的Airbnb设施完备，比酒店住的还舒服。<br><img src="/images/yellowstone/回家路上.jpg" width = 80% height = 80% div align=center /></p><h3 id="第3天"><a href="#第3天" class="headerlink" title="第3天"></a>第3天</h3><p>接下来一天，我们玩了南线上的景色，南线主要是大棱镜喷泉，必须爬到山上才能俯瞰到她的全貌，之后去old faith 那边看一些小盆地就见怪不怪了。然后主要去yellowstone lake那边玩湖，其实是没啥意思的，中途我们想去深林里找熊🐻，可是刚走不远就被自己吓了出来。hiking深林的话最好带着防熊喷雾，蚊虫喷雾等装备。老老实实的去湖那看风景和一些间歇性喷泉，可是此时我们已经提不起什么精神了，快审美疲劳。所以下面南线的景色很快就转完了。<br><img src="/images/yellowstone/yellow lake.jpg" width = 80% height = 80% div align=center /></p><p>于是我们提前去了东南线上的风景，Hayden 山谷看野牛。然后傍晚去了峡谷村庄(canyon village)看lower fall。登上高处，俯瞰远处黄色，红色的峡谷间倾泻而下的瀑布，实在是太美了，瞬间满血复活。私以为傍晚去看最好，光不那么刺眼，照的峡谷更加鲜艳动人。<br><img src="/images/yellowstone/lowerfall.jpg" width = 80% height = 80% div align=center /></p><h3 id="第4天"><a href="#第4天" class="headerlink" title="第4天"></a>第4天</h3><p>早上还是先去峡谷村庄去看剩下的upper fall，要爬山，下山，好一阵子。早上的光照着瀑布，不那么柔和，总感觉有一层雾气隔着你和瀑布，欲遮还羞。<br><img src="/images/yellowstone/uperfall.jpg" width = 80% height = 80% div align=center /></p><p>接下来我们去玩了东北那条线，那边的风景跟其它的线又有些不同，山和草地比较多，爬上山顶，一览无余。ps：用苹果手机vivid滤镜照出来的风景显得更加鲜艳漂亮，拍照新手推荐。</p><p>景色虽然很美，但看多了也乏，我们找到了一个hiking 的地方，一个沿着山谷河流的悬崖。路其实不算危险，虽然有的路就在悬崖边上。对面是我们之前开车过来的路，之前在那边看到时候，景色很美，但在这边看，风景不仅更壮美，也多了一种刺激感和满足感。<br><img src="/images/yellowstone/hiking.jpg" width = 80% height = 80% div align=center /></p><img src="/images/yellowstone/hiking2.jpg" width = 80% height = 80% div align=center /><p>来这边游客很少，大多数都是有经验的远足者。遇到其他人的时候，都会Hi，hello寒暄下，颇有种知音的感觉。走了大概有3个小时左右，连绵不绝的山脉，估计走上一天也到不了头。</p><img src="/images/yellowstone/hiking3.jpg" width = 80% height = 80% div align=center /><img src="/images/yellowstone/hiking4.jpg" width = 80% height = 80% div align=center /><p>接下来，我们吃完饭，沿着东北方向的线继续前进。因为这条线上，游客实在不多，我们可以放肆的随时停下来拍拍照片，淌淌小溪，看看野牛，好不自在。<br><img src="/images/yellowstone/野牛.jpg" width = 80% height = 80% div align=center /><br><img src="/images/yellowstone/小溪2.jpg" width = 80% height = 80% div align=center /></p><p>这条线上景点虽然不多，但我们一致觉得很美，没有太多游客的嘈杂，让我们随意的深入这样的大自然。不需要做什么攻略，自己探索就行。山路18弯，开起来据说感觉beir(天津话)爽。<br><img src="/images/yellowstone/十八弯.jpg" width = 80% height = 80% div align=center /></p><img src="/images/yellowstone/十八弯2.jpg" width = 80% height = 80% div align=center />晚上住在东线出去的cody，一个安安静静地小城，建筑很美，设施也比较齐全。朋友笑称可以在这里住一辈子。<img src="/images/yellowstone/青山.jpg" width = 80% height = 80% div align=center /><h3 id="第5天"><a href="#第5天" class="headerlink" title="第5天"></a>第5天</h3><p>第五天早上开车穿过黄石前往大提顿国家公园，我们准备玩上一天来着。可是，前有黄石，再看大提顿就没啥意思了。如果先看大提顿，再去看黄石，估计会好很多。在著名的Jackson湖畔hiking，兴许是太累了，或提不起兴致，我们绕到湖的另一边，就直接坐船回来了。<br><img src="/images/yellowstone/大提顿.jpg" width = 80% height = 80% div align=center /></p><p>不过大提顿还是给了我们惊喜。在黄石公园我们没有找到熊，在这里却看到好多只。一个熊妈妈带着好几只小熊仔过马路。看着熊妈妈不大的身材，我们嘲笑自己之前的退缩，还真以为熊都像是熊大熊二那么大吗？<br><img src="/images/yellowstone/bear.jpg" width = 80% height = 80% div align=center /></p><p>这边有一个漂流项目，沿着Snake river，沿途看景应该会不错。<br><img src="/images/yellowstone/DSC02544.jpg" width = 80% height = 80% div align=center /></p><p>晚上就在大提顿边上住了下来。一个叫Victor 的小镇，搜yelp的时候，竟发现小镇啥吃的都有，法餐，意餐，中餐，泰餐，韩餐，当然还有美餐。我们去吃了顿韩餐，不敢去烤肉。在黄石这些天，我们几个嘴唇特别干，各种上火。所以再来的话，防晒，唇膏等装备要备齐。<br><img src="/images/yellowstone/沿途美景.jpg" width = 80% height = 80% div align=center /></p><h3 id="第6天"><a href="#第6天" class="headerlink" title="第6天"></a>第6天</h3><p>一大早开回盐湖城，出发大盐湖。</p><img src="/images/yellowstone/saltlake4.jpg" width = 80% height = 80% div align=center /><p>一望无际的大盐湖，确实也属奇观。脱了鞋子，走在已经干涸的盐湖上，感受百万年来的环境变迁。</p><img src="/images/yellowstone/saltlake3.jpg" width = 80% height = 80% div align=center /><p>我们去玩的小岛名叫羚羊岛(Antelope island)，但其实在该小岛上的土生羚羊已经灭绝了。在小岛上hiking应该是非常好的，但这个季节不太合适，太热了。所以我们开车车绕了小岛一圈就出来了。<br><img src="/images/yellowstone/羚羊岛.jpg" width = 80% height = 80% div align=center /></p><p>然后直奔犹他大学，找停车的地方就找了好久。犹他大学历史比较悠久，里面的摩门教徒非常多，建筑还是蛮新的，但我们可能只看了外面大概，貌似山下，山上的一片都是他们的。犹他大学虽然排名不高，但他们学校的取得的成就却不少。反观我们学校，排名虽然比它高，但历史沉淀不足，距离世界名校可能还有很长的路要走。<br><img src="/images/yellowstone/犹他大学.jpg" width = 80% height = 80% div align=center /></p><p>之后我们还去了美国第一家KFC门店吃了顿自助。来美国还是第一次吃KFC，记得国内我还是蛮喜欢吃的。可是，在炸鸡遍天下的美国，即使是KFC，也真心吃不下多少了，也不知道跟国内口味还一不一样。<br><img src="/images/yellowstone/kfc.jpg" width = 80% height = 80% div align=center /></p><p>旅途结束，洗车，还车，上飞机，回家，躺尸。</p><p>说说这次旅程的我们的几点感受。</p><ol><li>我们发现黑人旅游非常少见，几天下来，见到的黑人不超过2位数。绝大多数是亚洲人和白人。不得不说，种族间的贫富差距还是有的吧。</li><li>旅游还是自助好。跟团基本上都是去看那几个固定的景点，看多了，还是很乏的。好多的美景需要自己去探索。在后面的阶段，我们就没看见旅游团，而恰恰是后面阶段更让我们觉得不虚此行。</li><li>人与自然…<img src="/images/yellowstone/羚羊岛2.jpg" width = 100% height = 100% div align=center /></li></ol>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>6月每日一吐</title>
      <link href="/2018/07/01/6%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/"/>
      <url>/2018/07/01/6%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/</url>
      
        <content type="html"><![CDATA[<p>很早之前，坚持过一段时间的，每日吐槽身边的事和人，以及自己。期望自己能够发现生活中有趣，无趣的一面，关注生活，而不是让时间悄悄溜走。</p><p>有了自己的博客，没什么人关注。目前的状态还是蛮喜欢的。努力记录，努力创造。</p><h3 id="6-30-2018"><a href="#6-30-2018" class="headerlink" title="6/30/2018"></a>6/30/2018</h3><p>热死了，热死了，今天热死了。月末最后一天，总结。</p><ol><li>室内定位项目，从确定怎么做室内定位，到尝试深度学习的简单算法，autoencoder，ensemble，方法的尝试，初步完成了精度的要求。下一步要改进目前的算法，以及把做的整理成文档。</li><li>完成技术博客4篇，主要是ensembel算法，机器学习经典算法总结，深度学习调参总结，以及数据的预处理特征工程等。2篇随笔。</li><li>简单的算法书中的几大算法已经看完；有待进一步整理及练习拓展；</li><li>确定自己职业方向：AI产品经理，修改简历，制作了简历个人网站。下一步，了解更多的关于AI产品经理的东西；继续修改简历。投简历。</li><li>周边游玩4次，2次艺术博物馆，1次海边，一次罗德岛。</li><li>每周坚持2次跆拳道练习，基本动作已经熟悉。</li></ol><h3 id="6-29-2018"><a href="#6-29-2018" class="headerlink" title="6/29/2018"></a>6/29/2018</h3><p>在家“休息了一天”，就没去学校，然后温度就飘升，干到了32度了。热死了，这是要逼我去学校好好学习天天向上啊。明天要修简历，找工作，准备面试，你说下个月这些能准备好吗？<br>话说今天把《太空无垠》美剧第三季追完了。看的好激动，末尾他们开启星门，将去宇宙深处去探索移民了。这是我近年来看的科幻中上佳之作。多希望我们也能进入宇宙大航海时代，那一天离我们还有多远啊。</p><h3 id="6-28-2018"><a href="#6-28-2018" class="headerlink" title="6/28/2018"></a>6/28/2018</h3><p>今天早上会上闲聊，我问一个法国来的小伙，巴黎的生活水平相比波士顿哪个比较贵。我开始觉得巴黎这么高大上的国际大都市应该会比波士顿贵吧。但他却说，波士顿要贵出一倍。就拿房租来说，800刀左右在巴黎能住一套房子，而我们这只能租一个单间。不过在美国其他偏远城市，租房可能就比巴黎便宜了。但巴黎可是国际性的大都市啊，波士顿没有她有名吧（虽然也有名），房租竟然这么贵。我好多同学都是1000多刀住一个破破的房间，没办法，离学校近啊。之前就看到过学校里有传单，美国人民抗议因为学校的原因，使周边房价上涨到住不起的地步。</p><h3 id="6-27-2018"><a href="#6-27-2018" class="headerlink" title="6/27/2018"></a>6/27/2018</h3><p>念头是不是我。</p><h3 id="6-26-2018"><a href="#6-26-2018" class="headerlink" title="6/26/2018"></a>6/26/2018</h3><p>中午跟一个朋友吃饭。问起他国内工作几年了。竟然有7年，但他看起来非常年轻。我很喜欢他，总是挂着笑脸，跟他聊天也蛮舒服的。我们聊起来，身边确实有不少工作几年后，然后出来读书的，而且这绝大多数人真看不出岁数，‘臭不要脸的’混迹在国内刚毕业出来的学生中。这是不是可以说明年轻心态对相貌的积极影响呢？</p><p>而且工作过后出来的，跟学生一比，交流起来也确实显得成熟多了。我觉得这是优势。学生应该增加工作经验，来扩大自己的视野，并且提高自己的情商。</p><h3 id="6-25-2018"><a href="#6-25-2018" class="headerlink" title="6/25/2018"></a>6/25/2018</h3><p>今天真是排了一天的队伍啊。今天和室友准备去拿驾照。没有SSN号，先去一个地方办一个材料，等了足足2个多小时。然后去拿驾照的地方办，这个快， 不过也排了半个小时的队伍。然后一看12点半多了，去吃个饭吧。哎，来到一个餐厅，还要排队，又排了将近半小时。如果不是看在店比较火，面比较好吃的份上，真是要怒了。因为旁边是电影院，我说好久没来看电影了，然后我们一合计吃完直接去看电影了…你晓得电影放映时间不可能我们想看就有的，于是我们也惯性似的等了40分钟。嗨，都排了一天的队了，还在乎这半个小时的等待吗。</p><h3 id="6-24-2018"><a href="#6-24-2018" class="headerlink" title="6/24/2018"></a>6/24/2018</h3><p>今天我们俱乐部聚会，我提前1小时15分钟出发，然后遇到地铁坏掉，转公交，再转地铁。然后地铁死活等在那不走，这是要我迟到啊。看着已经过了约定时间，我心里慌的不行了。迟到那么久，让各位大拿等着你，是不是不太好啊。于是我想来想去给负责人发了条微信：…哥，地铁卡在这…抱歉…去了会很晚，就不过去了吧…<br>然后负责人很快回了个“好”。心理痛斥自己，下次即使提前2个小时也要早点出来，让别人等是很可耻的，很没有时间观念。我在地铁上正做着深刻的检讨，只见微信群里，一个人说正在赶，还需要时间，然后陆陆续续的人，都说没到。看见最晚一个人是距离约定时间快半个小时了，还在找地方停车。<br>我的天呐，我要去的话，其实还不算最晚的。干嘛要给负责人发那条心虚的微信。气人啊。大家都这么没有时间观念吗？</p><h3 id="6-23-2018"><a href="#6-23-2018" class="headerlink" title="6/23/2018"></a>6/23/2018</h3><p>今天听朋友说开车的三个阶段：第一阶段，很虚。对待各种状况总感觉捉襟见肘；第二阶段，进行暴躁阶段。说明你已经开车比较稳了，对待一些路上车辆犯的新手错误越来越不能忍受了，总以为自己现在比这帮傻逼会开车；第三阶段，比较平复了。见惯了新手，老手开车的各种姿势，也就没那么大的感觉起伏了。</p><p>他说的不一定对，但听他这么一讲，我一下对自己开车有底了。知道自己处于什么水平。这是大家都会出现的阶段，所以不是你一个人笨，大家都这样。这让我联想到大学室友提炼的恋爱的几个阶段，这让我搞对象能就知道方向，而不会摸不着头脑。<br>我在想这种普世的分级手段，至少有两个好处：</p><ol><li>好像升级打游戏似的，目标清晰明确，不会让你摸不着头脑；</li><li>这是一个放之大部分人皆准的道理，而不是需要单枪匹马独自面对的状况，这种心理学上我不知道怎么说。但是人是来社交动物，如果就觉的只有你一个人笨笨的，那你的心情肯定是很糟糕的。但当发现所有人都跟你一样笨，你会不会轻松很多。</li></ol><h3 id="6-22-2018"><a href="#6-22-2018" class="headerlink" title="6/22/2018"></a>6/22/2018</h3><p>终于把自己的简历网站给做好了。梳理一下自己做过的事情，感觉调理多了。下一步就可以开始投简历了。</p><h3 id="6-21-2018"><a href="#6-21-2018" class="headerlink" title="6/21/2018"></a>6/21/2018</h3><p>遇到一朋友，问工作找的怎么样了。说还没什么眉目。国外工作这么难找吗？除了计算机专业的，貌似其他专业都不景气，尤其是去年各大公司都招满的情况下。我说那你回国吧，至少工资也能拿1万5了吧。然后他哈哈的对我说，以前工资就达到了那个水平了。这么说，感觉找不到2万以上的工资，都对不起2年国外的时间花费啦。</p><h3 id="6-20-2018"><a href="#6-20-2018" class="headerlink" title="6/20/2018"></a>6/20/2018</h3><p>今天回家比较早，下午4点钟坐地铁竟然都坐满了。你知道为什么吗？他们很多人下班了…对于天朝来的我们，感觉不可思议:astonished:。美国人真是会享受啊，4点钟就下班了。早上就算有人8点上班吧，去除中午吃饭1个小时，7个小时上班时间。想想是不是很幸福。</p><p>今天正好看到一篇硅谷嘲笑中国”996”的工作作息的文章。作者说，如果硅谷实行这种隐形制度，人就会跑光了，中国的这种奇葩作息说到底是被员工给惯的。</p><p>我认为有这几个原因形成了这种隐形制度：</p><ol><li>中国特殊的发展时期。说到底中国还是不够发达，资源有限，竞争压力大，新时代的机遇来临，谁能早日出头就能占据资源置高点。</li><li>中国独特的文化。隐忍，内向，等级思想，从重，不作为等等，无不助长着这种制度的诞生。</li></ol><p>不禁想起卓别林大师的经典电影《摩登时代》，何其相似。<br>相对来说，国外的个人主义比中国更加盛行，他们很早的就组成了工会维护劳动人民个人的权益。好消息是，新时代成长起来的孩子，也越来越有个性，相信随着时间推移，这种制度会成为历史吧。</p><h3 id="6-19-2018"><a href="#6-19-2018" class="headerlink" title="6/19/2018"></a>6/19/2018</h3><p>今天早上的跆拳道课上，老师叫我们组队练习各种动作。我跟一个60多岁的人分到了一组。别看他老，基本上，我们能做的动作，他都能做。甚至比我标准。<br>让我不禁想到在威海遇到的一个晨练大爷，明明80多岁了，看起来跟60多岁似的，竟然翻跟头一点都不费劲。而我们这些年轻人，一个简单的翻跟头都很难实现了吧。</p><h3 id="6-18-2018"><a href="#6-18-2018" class="headerlink" title="6/18/2018"></a>6/18/2018</h3><p>今天再次去罗德岛去参观那里的豪宅。去年7月份的时候去过，那时候办的会员正好用上了。第二次参观豪宅跟第一次有什么不同的感受吗？<br>因为是第二次，没有了第一次的震惊之感。你看，我的几个同学一边看一边“大惊小怪的”说着有钱人的世界果然不然一样，一个厕所都比我家大。<br>探索的欲望变小了，而是期待找到上次见到过的印象深刻的物品，或艺术。下面这个美女就是我找了5个豪宅才终于找回的。上次就对她“一见钟情”。只可惜这个女主人的命运并不是很好。<br>豪宅的后花园是面朝大海的，每次坐在宽广的草坪上，我就会想起来小时候，我们玩的时候经常从前庄跑到后庄，到处都是树木和花草，感觉这个世界好大。而现在开着车从一个省到另外一个省，也没有了这种感觉。不禁想如今的孩子没有广阔的天地够他们“驰骋”了，因为他们爸妈最多买的起100多平的房子。<br><img src="/images/Elizabeth in the Elms.jpg" width = 50% height = 50% div align=center /></p><h3 id="6-17-2018"><a href="#6-17-2018" class="headerlink" title="6/17/2018"></a>6/17/2018</h3><p>说说我在追的几部科幻美剧。<br>大家都在追西部世界，我却越来越提不起兴趣来了，已经旷了几集。都在回忆，可能是要把潜藏的线都给挖掘出来。但对于我来说，不够宏大，太过细节，有点拖拉。4颗星。</p><p>我更喜欢《太空无垠》这部太空科幻opera。已经追到3季了。前2季主题是移民太空的火星人，小行星人以及地球母星之间的三角关系。剧情跌宕，而且是完全有可能发生的。第三季潜藏的主题开始浮现，人类探索太空与外星高科技生物的相遇。每集看的我热血沸腾的。我最怕科幻剧停留在固有场景里面，期待这部剧有更多的场景突破。5颗推荐</p><p>另外还追了《真实人类》（Humans）,该剧展示了AI觉醒后与人类的冲突与相处。4颗星。</p><p>之前还追《黑暗物质》（The dark material）,也是太空opera，展示了宏大的天空场景，各大公司，政治团体相互争夺地盘的事情。不过里面有各种元素，AI, 平行空间，各种科幻概念等等。非常值得一看。 4.5星推荐</p><h3 id="6-16-2018"><a href="#6-16-2018" class="headerlink" title="6/16/2018"></a>6/16/2018</h3><p>今天跟朋友比谁宅，看来是我胜出了。我喜欢独处，也喜欢跟朋友在一块。很喜欢喧嚣后的平静那种感觉。<br>但长时间的独处会让人过多的沉浸在自己的世界之中，而与朋友，家人，社会人一起的时候，那种连接才是存在的基础。<br>所以说，我不很在乎去哪玩，哪吃，我更在乎的是跟谁。</p><h3 id="6-15-2018"><a href="#6-15-2018" class="headerlink" title="6/15/2018"></a>6/15/2018</h3><p>今天跟室友聊起来找对象的事情。她说她爸叫她赶紧毕业回家，不然这边找不到，快成老姑娘了。我也不好说意思说啥了。我自己都成老男孩了，没资格去说。</p><p>身在国外，找对象很难吗？我觉得相对国内来说，要难的多。但分人，受欢迎的美女到哪都是香饽饽吧？会撩妹的小哥，应该也不会觉得难吧。但对于我等一般人来说，几年的国外生活，找个对象真难。但奇怪的是，找对象的需求貌似非常旺盛。尤其是上研究生的女生，哈哈。</p><p>出国在外，大家变得小心翼翼，一是文化，二是钱吧。这里聚会变的少了，或者聚会范围也不像国内那么一桌一桌的，顶多几个人。这就在无形中让大家没有那么多机会去接触其他的人，更何况是搞对象呢。  </p><p>所以我早就想做一个海外，或者就是波士顿地区的交友平台了。肯定能火…</p><p>现在东风已备，只差…</p><h3 id="6-14-2018"><a href="#6-14-2018" class="headerlink" title="6/14/2018"></a>6/14/2018</h3><p>跟优秀的人为伍，真的会让你的血液都开始兴奋。与他们共事，可期，可成。又让我不安的血液躁动了起来。真的还有好多事情要做啊。</p><p>话说，做事情，我是非常控制欲的。每次做什么项目的时候，如果不能掌控主导，就会变的焉了。不过话说自己做的项目还真是自己主导的居多。不管是研究生课题，还是课程设计等等。</p><p>记得刚来美国上了一门创业课，跟2个外国人组队，最后没选我的idea，项目实施我没法掌控，所以那时候有些无所适从的。可能那时候英语也不好，被老外嫌弃的不要不要地…</p><h3 id="6-13-2018"><a href="#6-13-2018" class="headerlink" title="6/13/2018"></a>6/13/2018</h3><p>刚刚收到好友的微信，看的我莫名其妙。这微信像是继续我们刚刚没有说完的事情。可是跟他聊这件事情，不是前几天早上吗？ 看了他的上一条微信视频记录，竟然是今天早上打来的。 一下恍然，对啊。今天早上被他吵醒，聊了1个半小时。怎么大脑会把这个记录默认为好几天前的事情呢？</p><p>朋友在国内，晚上给我打的电话，一觉醒来，仍然念着这件事，于是顺手发了句昨晚没来及说的话。而我一天下来，经历了很多的事情。一下把时间度感给拉长了很多。无怪乎，人都说在同一时间尺度下，唯一能延长你寿命的方式就是增加你的体验感。</p><h3 id="6-12-2018"><a href="#6-12-2018" class="headerlink" title="6/12/2018"></a>6/12/2018</h3><p>刚刚晚上看了一个Youtube视频，讲李敖大师怒骂台湾军队发言的视频<a href="https://www.youtube.com/watch?v=hwg4TQOPVCU" target="_blank" rel="noopener">大陆打过来能抵抗多久</a>,如果能点开的话可以看看。</p><p>看到了一个还算理智的评论，台湾人认同中国的根，但要让他们回归统一，他们是不想的。</p><p>我觉得这个也是正常的，作为你固有的东西，任何人都会想捍卫一下的。如果我是台湾人，我也不想回归。我虽然能看到大陆的好，但同时我更能看清大陆的不好。想要让我回归，那你可能要改掉你所有的毛病才行。</p><p>我回复他的内容如下：<br>看到您比较理智的评论，特地点开看了下。貌似您的回复也是最多的。很喜欢理智的声音。首先声明两岸真是一家亲。我最喜欢的歌手是周杰伦，很欣赏蔡康永。现在大陆和台湾的综艺节目都在互播。单从文化娱乐来看上来说，就是不可分割的。<br>来美国期间，我很深刻的体会到了自由的感觉。这一点毋庸置疑，国内很多东西都是要噤声的，我也觉得这个是不对的。但同时我也看到了美国所谓的民主，贫富差距巨大。给你投票，你解决了民众多少生活问题。有时候，能说几句话就以为民主有多好，可是对于那些吃不起饭，看不起病的人，提供更多的帮助可能更实际些。<br>中国目前是有很多的问题的，但他至少现在是走在解决人民问题的道路上的（拿阴暗面来做反例，任何一个政体国家都是有的）。台湾问题比较复杂，武力统一台湾是绝对不可取的，反对一切的战争行为。中国只有越来越富强了，两岸交流越来越离不开对方了，自然而然就是统一的了。</p><p>补充个昨天就想说的：<br>美国民主貌似很好，很自由，但他问题真的很多；中国问题虽然很多，但是她做的越来越好。全面超越美国那只是时间的问题。</p><h3 id="6-11-2018"><a href="#6-11-2018" class="headerlink" title="6/11/2018"></a>6/11/2018</h3><p>转眼间，来美国已经一年半了。来时认识的一哥们今天见面(来我这边租房)，发现快成了小胖子了都。来的时候，挺帅的小伙子，经过美国1年多的“喂养”，快不复存在了。我记得当时还问他健身的技巧。回来后，我照了照镜子，看着微胖的小脸，下定决心，每周要去健身房3次，嗯，3次….不能少了</p><h3 id="6-10-2018"><a href="#6-10-2018" class="headerlink" title="6/10/2018"></a>6/10/2018</h3><p>实在想吐槽下，老师叫我们用matlab来实现python的代码。你知道用matlab有多麻烦，多落后吗？你知道机器学习用matlab我一窍不通吗？你知道现在大家做机器学习，尤其是深度学习都用python吗？你知道你的好基友Dens，他们组现在都用的python了吗？发现现在搞学术都喜欢用matlab，但那帮学术搞完搞业界的，都已经看到python才是正道啊。</p><p>一切都是最好的安排（心中默念），或许哪天真能用到呢。</p><h3 id="6-9-2018"><a href="#6-9-2018" class="headerlink" title="6/9/2018"></a>6/9/2018</h3><p>今天boston有龙舟，有慈善长跑，还有同性恋大游行。big day! 然而，我竟窝在家里改算法…说好的体验生活呢，说好的去认识更多的人呢？你就是个宅男，鉴定完毕！</p><h3 id="6-8-2018"><a href="#6-8-2018" class="headerlink" title="6/8/2018"></a>6/8/2018</h3><p>今天竟然是第一篇吐槽。没什么可说的，上来撕自己先。我很喜欢早上的时光，因为那时候还有自控力，看了会书，学了会习。吃完午饭，放个电影放松一下，就完蛋了。一下午就看电影睡觉…</p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ensemble</title>
      <link href="/2018/06/28/Ensemble/"/>
      <url>/2018/06/28/Ensemble/</url>
      
        <content type="html"><![CDATA[<p>Ensembel(集成学习)是一个简单，但非常有效的算法，在各大kaggle竞赛中，获得很高排名的，很多都应用了ensemble方法。这里是对ensemble learning 进行优秀资源的整理，便于以后查看。</p><p>了解集成学习可以从这篇blog开始：<br><a href="https://blog.csdn.net/qq_36330643/article/details/77621232" target="_blank" rel="noopener">集成学习(ensemble learning)原理详解</a></p><p>常见的Ensemble方法有这2种：Bagging and boosting。还有现在越来越多的stacking and blending。</p><h2 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h2><p>Bagging 算法如下图，通过随机采样训练集，进行训练，采集T个训练集，就训练T个弱学习器。然后通过一定的结合策略，如取平均，或者vote等形式变成一个强学习器。采集训练集时，是有放回的采集。<br><img src="/images/bagging.png" width = 80% height = 80% div align=center /></p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><p>从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p><p>Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(<a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Gradient-Boosting" target="_blank" rel="noopener">Gradient Boosting Tree,quora</a>)。AdaBoost和提升树算法的原理在后面的文章中会专门来讲。From: <a href="https://blog.csdn.net/qq_36330643/article/details/77621232" target="_blank" rel="noopener">link</a></p><img src="/images/boosting.png" width = 80% height = 80% div align=center /><p>原理解释：</p><iframe width="754" height="400" src="https://www.youtube.com/embed/fecp5nmetws" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><iframe width="754" height="400" src="https://www.youtube.com/embed/1GxscvKU2Ic" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="Stacking-amp-blending"><a href="#Stacking-amp-blending" class="headerlink" title="Stacking&amp;blending"></a>Stacking&amp;blending</h2><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p>数据比赛大杀器—-模型融合(stacking&amp;blending)<br><a href="https://blog.csdn.net/qq_18916311/article/details/78557722" target="_blank" rel="noopener">大话机器学习之STACKing,一个让诸葛亮都吃瘪的神技</a></p><img src="/images/stacking1.png" width = 80% height = 80% div align=center /><h3 id="错误认知"><a href="#错误认知" class="headerlink" title="错误认知"></a>错误认知</h3><p>研究了stack技能有一阵子，查到的资料和代码基本上都是<a href="https://zhuanlan.zhihu.com/p/25836678" target="_blank" rel="noopener">这样的</a>。里面给的图如下图</p><img src="/images/stacking2.jpg" width = 100% height = 100% div align=center /><p>看他的代码怎么都不能理解。KFold，cross-validation不是应该一个model就要训练了5次吗？为什么图中是用<strong>一个</strong>model来训练<strong>一个</strong>Fold集，而代码中是<strong>每个</strong>model都要训练<strong>每个</strong>fold集。</p><p>所以大部分转载或者写的stack方法都是错误的？不一定，有可能是没注意，有可能他们理解方式不一样。可惜对新手的我们很具有误导性啊。下文的中的作者就遇到跟我一样的情况。</p><h3 id="正确认知"><a href="#正确认知" class="headerlink" title="正确认知"></a>正确认知</h3><p>感谢<a href="https://zhuanlan.zhihu.com/p/26890738" target="_blank" rel="noopener">这篇文章</a>的作者跟我有一样的疑虑,正确的图应该是下面这样的。</p><img src="/images/stacking3.jpg" width = 100% height = 100% div align=center /><p>对于每一轮的 5-fold，Model 1都要做满5次的训练和预测, 之后的得到第一层的预测集P1，<br>model 2做满5次训练和预测，预测的集合是P2，once again, P3, P4, P5。 [P1,P2,P3,P4,P5]作为第二层训练集的input part. output part仍然是整个train set的label.</p><p>testset 在model 1中，每一次的训练就预测一次，总共有[T1,T2,T3,T4,T5],我们这时候后，取 $mean[T1,T2,T3,T4,T5]$ 作为我们下一层的测试集 t1，同样道理，model 2时，得到t2,….第二层的测试集就是[t1,t2,t3,t4,t5]，label仍然时整个test set的label.</p><p>附上该作者的代码解释：<br><img src="/images/stacking4.jpg" width = 100% height = 100% div align=center /></p><h3 id="多维数据的处理办法"><a href="#多维数据的处理办法" class="headerlink" title="多维数据的处理办法"></a>多维数据的处理办法</h3><p>以及我自己项目的部分代码。我的项目label值是一个二维的坐标，所以情况相比一维要复杂的多。在存储第二层的数据时，花了很多力气。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">kf &#x3D; KFold(n_splits&#x3D;5, random_state&#x3D;2018)</span><br><span class="line"></span><br><span class="line">def get_oof(clf, X_train, y_train, X_test):</span><br><span class="line"></span><br><span class="line">    blend_train &#x3D; np.zeros((y_train.shape[0],2))  # 二维label值</span><br><span class="line">    blend_test &#x3D; np.zeros((X_test.shape[0],2))</span><br><span class="line">    blend_test_skf &#x3D; np.zeros((X_test.shape[0],2,5)) # 5是因为有5次训练</span><br><span class="line"></span><br><span class="line">    for i, (train_index, test_index) in enumerate(list(kf.split(X_train))):</span><br><span class="line">        print(&quot;Fold&quot;, i)   </span><br><span class="line"></span><br><span class="line">        kf_X_train &#x3D; X_train[train_index]</span><br><span class="line">        kf_y_train &#x3D; y_train[train_index]</span><br><span class="line">        kf_X_test &#x3D; X_train[test_index]</span><br><span class="line">        kf_y_test &#x3D; y_train[test_index]</span><br><span class="line"></span><br><span class="line">        model &#x3D; clf(kf_X_train,kf_y_train)</span><br><span class="line"></span><br><span class="line">        blend_train[test_index]&#x3D;model.predict(kf_X_test)  #</span><br><span class="line"></span><br><span class="line">        blend_test_skf[:,:,i] &#x3D; model.predict(X_test)   #</span><br><span class="line"></span><br><span class="line">    blend_test[:,:]&#x3D;blend_test_skf.mean(axis&#x3D;2)</span><br><span class="line">    return blend_train, blend_test</span><br></pre></td></tr></table></figure><p>之后需要把P1,P2,P3,P4,P5给整合到表格中，因为P本身是坐标，2维的。就像这个样子。</p><table><thead><tr><th align="left">P1</th><th align="left">P2</th><th align="left">P3</th><th align="left">P4</th><th align="left">P5</th></tr></thead><tbody><tr><td align="left">(34,45)</td><td align="left">(22,44)</td><td align="left">(27,56)</td><td align="left">(43,56)</td><td align="left">(12,34)</td></tr><tr><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td><td align="left">…</td></tr></tbody></table><h4 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h4><p>跟我们处理的一维数据很不一样。首先怎么把P1,P2…组合起来就是个问题。用np.column_stack? 不行，那个函数只能表示一维。我们可以设置新的3D矩阵</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train &#x3D; np.zeros((len(p1),2,5)),</span><br><span class="line">train[:,:,0] &#x3D; P1</span><br><span class="line">train[:,:,1] &#x3D; P2</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>同样的道理，得到的 test的三维数据。</p><p>怎么来训练呢。一般的算法都是二维数据，当然也有一些算法是支持多维的，如KNN, decisiontree， extratree(看到有这样说的，但我还没尝试过，KNN试过可以)。</p><h4 id="方法二（reshape"><a href="#方法二（reshape" class="headerlink" title="方法二（reshape)"></a>方法二（reshape)</h4><p>利用reshape函数，把二维变1维, x_coordinate 对应 X_label, y_coordinate 对顶Y_label。转化成1维的情况，就很方便了。利用<code>np.column_stack</code>组合5个model得到的数据集。然后进行有监督训练，之后也可以再处理转化成二维的数组（x_coordinate，y_coordinate）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def reshaped(predict):</span><br><span class="line">    size &#x3D; predict.shape[0]</span><br><span class="line">    j &#x3D; predict.reshape((2*size, 1))</span><br><span class="line">    return j</span><br></pre></td></tr></table></figure><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a>Resources:</h2><p><a href="https://zhuanlan.zhihu.com/p/26890738" target="_blank" rel="noopener">Kaggle机器学习之模型融合（stacking）心得</a></p><p><a href="https://blog.csdn.net/shine19930820/article/details/75209021" target="_blank" rel="noopener">Ensemble Learning-模型融合-Python实现</a></p><p><a href="https://mlwave.com/kaggle-ensembling-guide/" target="_blank" rel="noopener">KAGGLE ENSEMBLING GUIDE</a></p><p><a href="https://blog.csdn.net/u014114990/article/details/50819948" target="_blank" rel="noopener">总结Kaggle-Ensemble-Guide</a></p><p><a href="https://github.com/emanuele/kaggle_pbr/blob/master/blend.py#L68" target="_blank" rel="noopener">github上一个stacking代码</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> ensmeble </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine learning with python(4)_classical algorithm(持续跟新)</title>
      <link href="/2018/06/21/Machine-learning-with-python-4-classical-algorithm/"/>
      <url>/2018/06/21/Machine-learning-with-python-4-classical-algorithm/</url>
      
        <content type="html"><![CDATA[<p>系列教程4，主要是几个classical的机器学习算法。</p><p>参考的资源有：<br>吴恩达机器学习课程笔记（需要的可以留下邮箱）<br>Udacity 机器学习工程师<br>以及文中各种blog链接  </p><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><p>讲的很好的一个<br><a href="https://www.jianshu.com/p/c7e642877b0e" target="_blank" rel="noopener">深入浅出–梯度下降法及其实现</a></p><p>随机梯度和batch梯度的区别：随机梯度更新一个数据，有时不易收敛，收敛结果波动。而batch一次需要样本多，耗时，尽管收敛效果更好。</p><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p><a href="https://www.jianshu.com/p/f1d3906e4a3e" target="_blank" rel="noopener">深入浅出最大似然估计</a></p><p>最大似然估计是利用已知的样本的结果，在使用某个模型的基础上，反推最有可能导致这样结果的模型参数值。</p><p>当这个正态分布的期望为多少时，产生这个采样数据的概率为最大？</p><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>简单来说就是用 y=ax+b 去拟合得到最好的a和b。怎么得到最好的a,b呢？用cost function 去寻找这个最佳的参数。<br>cost function 公式：mean quare error.</p><p>然后求这个cost function 的梯度值，下降到最小的梯度值时，那么就得到了最佳参数。</p><img src="/images/linear regression1.png" width = 50% height = 50% div align=center /><p><a href="https://blog.csdn.net/qq_19645269/article/details/78127785" target="_blank" rel="noopener">公式推导，西瓜书</a><br>这里注意。在推导过程中，很容易遇到矩阵不是满秩矩阵的情况，这个时候需要加上正则化项，L1,L2。具体参考<a href="http://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/#Lasso-Ridge">note</a></p><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>事实上，逻辑回归是一个分类算法，之所以这样弄，是因为逻辑回顾应用了线性回归类似的方法，只是增加了一个sigmod层。<br>事实上，logical regression是仅含有一个神经元的单层的神经网络。<br><img src="/images/logical regression.png" width = 90% height = 90% div align=center /></p><p>因为加入sigmod函数，使得结果在0~1之间，把它当成概率，$\phi (z)$ 可以视为类1的后验估计，所以可以写成：<br>$$P(y=1\mid x;w) = \phi(w^{T}x+b)=\phi(z)$$<br>$$P(y=0\mid x;w) =1- \phi(z)$$</p><p>一般形式： $$P(y\mid x;w)=\phi(z)^y(1-\phi(z))^{(1-y)}$$</p><p>极大似然：$$L(w)=\prod_{i=1}^{n}p(y^{i}\mid x^{i};w)=\phi(z)^y(1-\phi(z))^{(1-y)}$$</p><p>然后log取导数，sigmod函数有一个很好的性质$$\phi{(z)}=\phi(z)(1-\phi(z))$$</p><p>求导具体过程参考以下。最后得出更新公式。</p><p>参考：<a href="https://blog.csdn.net/zjuPeco/article/details/77165974" target="_blank" rel="noopener">逻辑回归(logistic regression)的本质——极大似然估计</a><br><a href="https://blog.csdn.net/kejiaming/article/details/64439664" target="_blank" rel="noopener">逻辑回归L1与L2正则，L1稀疏，L2全局最优（凸函数梯度下降)</a></p><p>Softmax regression其实是多维的Logistic regression，它其实可以看做是单层多个神经元的神经网络！<br><img src="/images/softmax regression.png" width = 60% height = 60% div align=center /></p><p>参考：<a href="https://blog.csdn.net/tz_zs/article/details/79069499" target="_blank" rel="noopener">逻辑回归和神经网络之间有什么关系？</a></p><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><p>知道先验概率，知道条件概率，然后根据贝叶斯公式，知道后验概率是多少。多用于分类。全概率公式展开 $$[p(y=c_{k}|x)=\frac{\prod_{i=1}^{M}p(x^{i}|y=c_{k})p(y=c_{k})}{\sum_{k}p(y=c_{k})\prod_{i=1}^{M}p(x^{i}|y=c_{k})}]$$</p><p>例子：<a href="https://blog.csdn.net/u013634684/article/details/49669081" target="_blank" rel="noopener">朴素贝叶斯算法之过滤垃圾邮件</a><br>非常好多解释资源，可以帮助你理解贝叶斯，及文本分类。</p><p>有三个延申的贝叶斯：<br>[G]aussian Naive Bayes Classifie](<a href="https://chrisalbon.com/machine_learning/naive_bayes/gaussian_naive_bayes_classifier/" target="_blank" rel="noopener">https://chrisalbon.com/machine_learning/naive_bayes/gaussian_naive_bayes_classifier/</a>), 条件概率的部分变成是高斯分布PDF分布形式。  </p><p><a href="https://chrisalbon.com/machine_learning/naive_bayes/multinomial_naive_bayes_classifier/" target="_blank" rel="noopener">Multinomial Naive Bayes Classifier</a>,当我们数据是离散的数据时，用这个</p><p><a href="https://chrisalbon.com/machine_learning/naive_bayes/bernoulli_naive_bayes_classifier/" target="_blank" rel="noopener">Bernoulli Naive Bayes Classifier</a> The Bernoulli naive Bayes classifier assumes that all our features are binary such that they take only two values (e.g. a nominal categorical feature that has been one-hot encoded).</p><h2 id="Decision-Tree"><a href="#Decision-Tree" class="headerlink" title="Decision Tree"></a>Decision Tree</h2><p>一堆数据给你，你怎么分类呢。计算这堆数据的entropy(熵)，计算公式是这样的：$$[entropy = - \sum_{i=1}^{n} p_i log_{2} p_i]$$<br><img src="/images/decision tree.png" width = 100% height = 100% div align=center /></p><p>可从图上看出，画第1条线的时候，我们就计算画什么位置，两边的entropy最小。<br>然后用 Entropy(前)-Entropy(后)=信息增益。信息增益越大，说明分的越正确。</p><p>一个例子：</p><iframe width="754" height="400" src="https://www.youtube.com/embed/3FgJOpKfdY8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>但是用信息增益的算法（ID3算法），缺点是信息增益偏向取值较多的特征。</p><p>C4.5决策树的提出完全是为了解决ID3决策树的一个缺点，当一个属性的可取值数目较多时，那么可能在这个属性对应的可取值下的样本只有一个或者是很少个，那么这个时候它的信息增益是非常高的，这个时候纯度很高，ID3决策树会认为这个属性很适合划分，但是较多取值的属性来进行划分带来的问题是它的泛化能力比较弱，不能够对新样本进行有效的预测。</p><p>具体公式查看下面链接。但是C4.5的缺点是偏向取值较少的特征。</p><p>还有一个基尼指数,CART算法。Cart提出了根据基尼系数划分，同时，它的树限定为二叉树，更容易解释，还能处理连续值。<a href="https://chrisalbon.com/machine_learning/trees_and_forests/decision_tree_classifier/" target="_blank" rel="noopener">参见</a></p><p>Further reading:<br><a href="https://www.cnblogs.com/muzixi/p/6566803.html" target="_blank" rel="noopener">决策树–信息增益，信息增益比，Geni指数的理解</a><br><a href="https://www.cnblogs.com/coder2012/p/4508602.html" target="_blank" rel="noopener">决策树</a></p><p>Random forests: 用决策树容易overfitting, 几次随机选取几列进行训练，最后voting得到最佳的结果。</p><iframe width="754" height="400" src="https://www.youtube.com/embed/n5DhXhcYKcw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><p>SVM 是为了使分类边界的距离（Margin）最大化, $[M = \frac{2}{\left | x \right |}]$，并且分类的错误率最低。它的loss function 包括两个部分，$classification error + Margin error$. 其中 $margin error = 1/2*||w||^2$。所以这个loss function形式很像是一般的error 加上了一个 L2(Ridge) punishment。两个error是一个tradeoff的过程，想要更好准确率，加大C，如果想要更大的margin，减小C。</p><p>$$[min\frac{1}{2}\left | w \right |^{2}+C\sum_{i=1}^{R}\varepsilon_{i} , s.t.,y_{i}(w^{T}x_{i}+b)\geq 1-\varepsilon_{i},\varepsilon_{i}\geq 0]<br>$$</p><iframe width="754" height="400" src="https://www.youtube.com/embed/nWGVAGXwvGE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><iframe width="754" height="400" src="https://www.youtube.com/embed/dSac8Gfgbok" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><iframe width="754" height="400" src="https://www.youtube.com/embed/A1wbrcSYc1c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><h3 id="kernal"><a href="#kernal" class="headerlink" title="kernal"></a>kernal</h3><p>在二维的情况下，用一条直线把两边数据分类。如果不可分的时候，通过核函数（Kernel）转换，找到一个超平面（hyper-plane）来分类。<br><img src="/images/SVM_4.webp" width = 70% height = 70% div align=center /><br><img src="/images/SVM_8.webp" width = 40% height = 40% div align=center /><img src="/images/SVM_9.webp" width = 40% height = 40% div align=center /></p><ul><li><p>polynomial kernal: (x,y) —-&gt;(x,y,xy,x^2,y^2)</p></li><li><p>RBF kernal: 移动每一个point到山range, 要想很好的区分出二类，得找到合适mountain weights. 怎么找呢？  </p><img src="/images/RBF1.png" width = 80% height = 80% div align=center /><iframe width="754" height="400" src="https://www.youtube.com/embed/xdkIulxXWfQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>找到之后，把点利用高斯分布 lift 到高维。</p><p>参数$\gamma $，越大越窄；越小越宽：   </p><<iframe width="754" height="400" src="https://www.youtube.com/embed/DctkE8kaWPY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></li></ul><p>讲svm比较好的资源如下：<br>第一版本：Margin方式：<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html" target="_blank" rel="noopener">支持向量机基础</a><br>第二版本：cost function方式：<a href="https://blog.csdn.net/ybdesire/article/details/53915093" target="_blank" rel="noopener">SVM理解与参数选择（kernel和C）</a><br>英文版：<a href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/" target="_blank" rel="noopener">Understanding Support Vector Machine algorithm from examples (along with code)</a><br>SVM参数选择：<a href="https://blog.csdn.net/bryan__/article/details/51506801" target="_blank" rel="noopener">SVM参数详解</a></p><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>传统KNN很简单，就是通过欧式距离或者曼哈顿距离，计算要分类点a和周边K个点的距离，然后看这K个点里，最多的数据点属于哪个类别，就把该要分类的点a归到那一类去。<a href="https://blog.csdn.net/qq_36330643/article/details/77532161" target="_blank" rel="noopener">最近邻算法原理详解</a><br><img src="/images/KNN.jpg" width = 40% height = 40% div align=center /></p><p>它还有几个延申：  </p><ul><li><a href="https://chrisalbon.com/machine_learning/nearest_neighbors/radius_based_nearest_neighbor_classifier/" target="_blank" rel="noopener">Radius-based Nearest Neighbor classifier</a>, 不是设K值，而是设一个圆周距离，在该圆里，看哪个类别最多。</li><li><a href="https://www.cnblogs.com/bigmonkey/p/7387943.html" target="_blank" rel="noopener">加权KNN</a> 有时候各个特征对于分类的贡献是不相同的。用$1/d$距离的倒数来做权重，越远的对分类越弱。</li></ul><p>为什么K越小，bias越小，variance越大；而K变大，high bias, low variance?</p><h2 id="Kmeans-无监督学习"><a href="#Kmeans-无监督学习" class="headerlink" title="Kmeans 无监督学习"></a>Kmeans 无监督学习</h2><p>Kmeans是无监督学习聚类算法。</p><ul><li>第一步，随机选几个点作为聚类中心C。</li><li>第二步，计算数据点跟聚类中心C的距离，离哪个点近就分到哪个类。</li><li>第三步，重新计算聚类中心点C，C = 整个簇的平均值。</li><li>第四步，重复第二步和第三步，直到没有数据会变化。</li></ul><p>Kmeans很简单，但很容易陷入局部最优化，跟初始点的选取很有关系。所以产生了Kmeans++算法。<br>假如有要分K个类，第一个点跟Kmeans一样也是随机选取，但N+1的点选取确实根据$D(x)$来选取的。<br>相比Kmeans,Kmeans++多了几步：</p><ol><li>从数据随机选取一个样本作为初始聚类中心$C_1$</li><li>首先计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离），用D(x)表示；接着计算每个样本被选为下一个聚类中心的概率$[\frac{D(x)^2}{\sum_x D(x)^2}]$. 最后，按照轮盘法选出下一个聚类中心。</li><li>重复第2步知道选出K个聚类中心</li><li>然后按照经典的K-means算法的第二步到第四步</li></ol><p>参考：<a href="https://www.cnblogs.com/wang2825/articles/8696830.html" target="_blank" rel="noopener">K-means与K-means++</a></p><h2 id="各个算法的优缺点"><a href="#各个算法的优缺点" class="headerlink" title="各个算法的优缺点"></a>各个算法的优缺点</h2><p>算法的优劣主要看它的误差，一般误差有方差和偏差组成。一个模型越复杂，拟合的越好，偏差会变小，但容易造成过拟合。对小数据，选取的高偏差，低方差的模型比选取高方差，低偏差的模型要好，因为后者会发生过拟合（overfiting）。然而，随着你训练集的增长，模型对于原数据的预测能力就越好，偏差就会降低，此时低偏差/高方差的分类器就会渐渐的表现其优势（因为它们有较低的渐近误差），而高偏差分类器这时已经不足以提供准确的模型了。</p><p><a href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/" target="_blank" rel="noopener">Choosing a Machine Learning Classifier</a></p><p><a href="http://www.csuldw.com/2016/02/26/2016-02-26-choosing-a-machine-learning-classifier/index.html" target="_blank" rel="noopener">机器学习算法比较</a></p><p><a href="https://blog.csdn.net/wuzqChom/article/details/75091612" target="_blank" rel="noopener">偏差和方差</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 经典算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep learning(3)_调参总结</title>
      <link href="/2018/06/15/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/"/>
      <url>/2018/06/15/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>调参的一些blog</p><p>本文blog主要整理自：（主要用于自己学习，不用做其它）<br><a href="https://blog.csdn.net/dugudaibo/article/details/77366245" target="_blank" rel="noopener">https://blog.csdn.net/dugudaibo/article/details/77366245</a></p><p><a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/7366925.html" target="_blank" rel="noopener">Deep learning网络调参技巧</a></p><p>首先，调参调的是超参数（hyperparameters）。超参数和参数的区别是什么呢？</p><p>参数：就是模型可以根据数据可以自动学习出的变量，应该就是参数。比如，深度学习的权重，偏差等</p><p>超参数：就是用来确定模型的一些参数，超参数不同，模型是不同的(这个模型不同的意思就是有微小的区别，比如假设都是CNN模型，如果层数不同，模型不一样，虽然都是CNN模型哈。)，超参数一般就是根据经验确定的变量。在深度学习中，超参数有：学习速率，迭代次数，层数，每层神经元的个数等等。   from:  <a href="https://blog.csdn.net/UESTC_C2_403/article/details/77428736" target="_blank" rel="noopener">link</a>  </p><h2 id="超参数主要包括："><a href="#超参数主要包括：" class="headerlink" title="超参数主要包括："></a>超参数主要包括：</h2><ol><li>学习率 $\eta$</li></ol><p>1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。    </p><p>不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。</p><ol start="2"><li>正则化参数 $\lambda$</li></ol><p>开始从1.0尝试，超过10的很少见</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regularzation_penalty &#x3D; 0.02</span><br><span class="line">model.add(Dense(256, activation&#x3D;&#39;relu&#39;, kernel_regularizer&#x3D;regularizers.l2(regularzation_penalty)))</span><br></pre></td></tr></table></figure><ol start="3"><li>神经网络的层数 $L$</li></ol><p>先从1层开始试</p><ol start="4"><li>每一个隐层中神经元的个数 $j$</li></ol><p>16 32 128，超过1000的情况比较少见。超过1W的从来没有见过</p><ol start="5"><li><p>学习的回合数Epoch</p></li><li><p>小批量数据 minibatch 的大小</p></li></ol><p>128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。</p><ol start="7"><li><p>输出神经元的编码方式(?不太明白这里)</p></li><li><p>代价函数的选择<br>代价函数有很多，例如, <a href="https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications" target="_blank" rel="noopener">更多cost function</a></p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; &#39;categorical_crossentropy&#39; #cross-entropy,</span><br><span class="line">loss &#x3D; mean_squared_error</span><br><span class="line">loss&#x3D;&#39;binary_crossentropy&#39;</span><br></pre></td></tr></table></figure><ol start="9"><li>权重初始化的方法</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">initilization_method &#x3D; &#39;he_normal&#39; #&#39;random_uniform&#39; ,&#39;random_normal&#39;,&#39;TruncatedNormal&#39; ,&#39;glorot_uniform&#39;, &#39;glorot_nomral&#39;, &#39;he_normal&#39;, &#39;he_uniform&#39;</span><br><span class="line"></span><br><span class="line">model.add(Dense(256, input_dim&#x3D;256, activation&#x3D;&#39;relu&#39;, kernel_initializer&#x3D;initilization_method)</span><br></pre></td></tr></table></figure><ol start="10"><li><p>神经元激活函数的种类<br>指的activation function，具体可以参考<a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/Deep-Learning-1/#Train-Optimization">deep learning(1)</a></p></li><li><p>参加训练模型数据的规模</p></li></ol><h2 id="实践经验"><a href="#实践经验" class="headerlink" title="实践经验"></a>实践经验</h2><p>实践中，一般先进行初步范围搜索，然后根据好结果出现的地方，再缩小范围进行更精细的搜索。</p><ol><li>建议先参考相关论文，以论文中给出的参数作为初始参数。至少论文中的参数，是个不差的结果。</li><li>如果找不到参考，那么只能自己尝试了。可以先从比较重要，对实验结果影响比较大的参数开始，同时固定其他参数，得到一个差不多的结果以后，在这个结果的基础上，再调其他参数。例如学习率一般就比正则值，dropout值重要的话，学习率设置的不合适，不仅结果可能变差，模型甚至会无法收敛。</li><li>如果实在找不到一组参数，可以让模型收敛。那么就需要检查，是不是其他地方出了问题，例如模型实现，数据等等。</li></ol><h2 id="自动调参"><a href="#自动调参" class="headerlink" title="自动调参"></a>自动调参</h2><ol><li>Grid Search</li><li>Random Search</li><li>Bayesian Optimization</li></ol><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>按吴恩达老师所说的，梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum）</p><p>这部分数学部分不做陈述，主要是代码应用。<br><a href="https://keras.io/optimizers/#adam" target="_blank" rel="noopener">keras optimizers</a></p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><a href="https://www.jianshu.com/p/aebcaf8af76e" target="_blank" rel="noopener">简单认识Adam优化器</a></p><p>随机梯度涉及到这几个参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adam(lr&#x3D;0.001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, epsilon&#x3D;1e-08,</span><br><span class="line">decay&#x3D;0.0,amsgrad&#x3D;False)</span><br></pre></td></tr></table></figure><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources:"></a>Resources:</h2><p><a href="https://blog.csdn.net/roslei/article/details/61916038" target="_blank" rel="noopener">18个技巧实战深度学习，资深研究员的血泪教训</a></p><p><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">optimizer总结</a><br><a href="http://ruder.io/deep-learning-optimization-2017/" target="_blank" rel="noopener">optimization for deep learning highlights in 2017</a></p><p><a href="https://blog.csdn.net/qq_20259459/article/details/70316511" target="_blank" rel="noopener">深度学习 14. 深度学习调参，CNN参数调参，各个参数理解和说明以及调整的要领。underfitting和overfitting的理解，过拟合的解释</a></p><p><a href="https://blog.csdn.net/xiaocong1990/article/details/72585696" target="_blank" rel="noopener">深度学习调参策略1</a><br><a href="https://blog.csdn.net/xiaocong1990/article/details/72585735" target="_blank" rel="noopener">深度学习调参策略2</a><br><a href="https://blog.csdn.net/chenzhi1992/article/details/52905569" target="_blank" rel="noopener">深度学习训练的小技巧，调参经验。总结与记录</a></p><p><a href="https://www.cnblogs.com/ljygoodgoodstudydaydayup/p/7366925.html" target="_blank" rel="noopener">Deep learning网络调参技巧</a></p><p><a href="https://zhuanlan.zhihu.com/easyml" target="_blank" rel="noopener">知乎专栏：炼丹心得</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> deep learning </tag>
            
            <tag> 调参 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>疫情日记5月</title>
      <link href="/2018/06/08/%E7%96%AB%E6%83%85%E6%97%A5%E8%AE%B05%E6%9C%88/"/>
      <url>/2018/06/08/%E7%96%AB%E6%83%85%E6%97%A5%E8%AE%B05%E6%9C%88/</url>
      
        <content type="html"><![CDATA[<h2 id="202005"><a href="#202005" class="headerlink" title="202005"></a>202005</h2><h3 id="20200501"><a href="#20200501" class="headerlink" title="20200501"></a>20200501</h3><p>早上醒来又是快10点了。默默的骂了自己几句，晚上要早睡。<br>话说在家隔离也快2个月了。前一个月还有各种due(截止日期)的压力，进入下个月，压力变得小了，其实更多也是越来越适应居家办公。之前设置了各种作息计划，基本没有成功实施的。不断的尝试，不断的制定新的计划，终于有了一个相对适应的作息工作版本。</p><p>首先不设置闹铃了，睡到自然醒，充足的睡眠是抵抗病毒的有力依仗不是。然后就是我把午饭变得越来越简单，越来越简短，越来越延后。<br>我发现之前很多计划实施不下去的原因是太把午饭太当回事了。</p><p>在作息初级的版本时：工作到12点，可以去做饭了，然后吃饭，吃饭时候还就着电视剧肥皂剧，很多时候，一看就收不住。你知道，“温饱思淫欲”，古人还是很有经验的。然后还看累了，午休一下呗。躺床上睡不着啊，那先刷会手机吧，抖音…回过头再一看手机，我去，已经下午4点了。赶紧眯15分钟，起来干活。喔，6点了哦，晚饭该开始了。尼玛，午饭的循环，晚饭又开始了。最后发现这都晚上9点多了，不看了，明早早点起来补上吧。<br>循环，不间断上演。</p><p>进阶版本，把午饭延后到下午2点左右，可以集中精力多干些活。不过并没有破除掉饭后放纵式休息的魔咒。</p><p>最近版本，把午饭延后到下午4点以后，中间饿了，就吃写饼干，坚果，或者快速吃点剩饭，但不碰剧。这样一直能保证连续学习7个小时左右。对于之前的不断失败，7个小时每天的学习时间，已经很知足了。</p><p>我很喜欢用一个记录时间的利器，番茄钟。设置了1个小时一个番茄，这样给我的感觉，是可以安心在这1个小时内不管不顾的工作学习，非常高效。同时也记录下自己每天学了多长时间，每天都可以给自己一个明确的反馈。</p><p>在家隔离虽然很难受，但对长时间专注工作，还是很有帮助，无人打断，效率倍升。不过对于依赖交流的工作，应该是不适用的。</p><h3 id="20200502"><a href="#20200502" class="headerlink" title="20200502"></a>20200502</h3><p>记一天的生活。<br>今天周六，虽然8点半的时候就被吵醒，但依然不想起来。翻了个身继续睡了会，又醒，算了，那就起吧（其实是拿起手机，刷了会抖音）。</p><p>9点半了，起来洗个了个澡。室友还没醒，在想要不要轻点。心里还埋怨着室友，昨晚夜里2点在走廊视频把我吵醒，是不知道这边隔音非常不好吗？<br>我以为我这个人就够随意（之前经常被前洁癖室友批评），没想到他比我还随意。好在人也还不坏。不过一想起来，隔离结束遥遥无期，还要继续这样”一起生活下去“，会不会终有一天受不了。<br>他经常出门，每次出门都是”哄佟“的一声巨响，我就知道又出去散心了吧。长时间呆在狭小的屋子里，不出去，真会逼疯人的吧。但我基本没出去过，我这边主卧，临街，很大的窗户，可以时刻看到窗外的风景，所以还算不是那么单调吧。</p><p>窗外，哎，今天拍照的人很多啊（我对面就是我们学校”大门“），一波接着一波的。很多人还是穿着学士服来拍照的。也对，学期刚结束了，很多人毕业。赶在这个时候，毕业也真是悲催。没有了盛大的毕业典礼，我不知道明年学校会不会让这帮人回来重新参加一下。</p><p>美国这边的毕业典礼还是非常隆重的，我参加过硕士的毕业典礼，观感还行。毕业典礼主要是邀请家人好友，来一起分享喜悦。特别是印度人，貌似硕士毕业很隆重，家人都盛装参加。而我们国人（顺便提一句，硕士学院，以中国人和印度人最多），貌似不是特别在意，自己穿的比较随意，家里人也没多少会来参加的。即使有，也是顺便来旅游的居多。他们并没有其他国家学生对这个学位看的那么重要，开心吧。不过我想，这里面跟国人腼腆，没有仪式感的习惯也不无关系。<br>据说本科生的毕业典礼更好玩，比硕士隆重的不是一星半点。他们是在NBA凯尔特人的主场Fenway体育馆举办的。想想就很激动不是。</p><p>窗外，不过这帮人怎么不带口罩啊。或许是怕挡住了他们的美。但拍一张戴口罩的毕业照，多年以后，是不是会对这段毕业经历更加能忘。</p><p>做了顿日常的早饭。早饭要吃好，这样才有精力干很长时间的活。这还是跟抖音上学的一个网红早点。两片面包，两个鸡蛋，面包放到刚下锅的蛋液上，然后翻一下，保证两边面包上都沾有蛋液。然后放上两片芝士，两片火腿（我用的两片火鸡肉），然后放上自己喜欢吃的酱即可。涂抹上最爱的甜面酱，完美的一天从吃早餐开始。</p><p>吃饭期间，喊Amazon Alexa (亚马逊智能小音箱)打开CNN news(一个美国新闻媒体) ，听一听新闻。英语水平还是欠佳，有时候还是会听不懂一些。不过既然当背景声放着，也不是很在意。不过耳朵在听到”China“的时候，总是会竖起，生怕错过什么。可是每次听完就很来气，又是报道关于怎么怎么往中国泼脏水的。这帮无良政客，昧着良心（抱歉，政客不能有良心），把自己的无能找借口，找转移的大众的目标。无疑，中国是非常好的挡箭牌。我不知道美国人怎么接收的信息，想象一下如果一个美国人整天沉浸在这些无良政客，与媒体言论之下，他可不就听到什么就信什么吗？美国的质疑精神，在这种情况下简直可笑。</p><p>想起我们组的美国小哥，就是个很有质疑精神的人，从我们的平时的研究讨论就可以看出来。有时候，我们一起聊天，总是会聊到中美之间的问题。他承认美国的很多消息不可靠，可是苍蝇不叮无缝蛋，肯定是有不好的地方。然后就会反驳我说，你能肯定中国政府不会怎么怎么样吗？我只能说，对，他们或许有做的不好的一面，但是他们做了更多的好，你看… 从我们的对话中可以看出，其实天然就是两个立场，他再怎么有质疑精神，美国自久以来长期黑中国的媒体还是对他们产生了根深蒂固的影响。他们关注的黑，是不好一面，我们关注的是白，是好的一面。你要知道有一点黑就很容易毁了全部。就像一个号称道德完人的人，突然做了一件不那么德道的事情，那么他之前所有的善，很容易被大家忽略，道德完人瞬间分崩离析。不禁想起娱乐圈（也不止娱乐圈），人设一定不要设，一设准塌。</p><p>吃完早饭，开始干活，今天周六可以轻松点，主要是补一补日记。<br>抖音也不能看，一看很容易收不住，这不，一个多小时又过了，真是罪过，赶紧起来把这篇补完。</p><p>下午有朋友喊线上玩剧本杀。我给拒绝了，他们就把我踢出了群…不是很擅长这类的游戏，而且对于已经有计划的我，打破计划，会是一件很让我失控的事情。疫情之下，让我有了更多跟自己相处的时间，希望能加深对他的了解。</p><p>写完这篇日记。也都晚9点多了。准备收工，玩会游戏，看会书，漫无目的，估计又要12点才能睡。4月初买的游戏机，疫情在家，怕在家会受不了。买了心心念念的游戏主机Xbox，还怕自己会荒废学业工作。没想到，就体验了1天，热情就降了大半。现在也就每天玩一会。看来我这三分钟热度的毛病一直都在。之前买的VR眼镜放在床底箱子里也有段时间了吧。</p><h3 id="20200503"><a href="#20200503" class="headerlink" title="20200503"></a>20200503</h3><p>昨晚临近睡觉了，跟一个老美朋友聊天，聊着聊着就吵起来。给我大祖国又泼脏水，气的我发抖，睡意全无。很久之前就有人跟我说，在国外不要妄聊政治，西方媒体的下的西方，他们早就形成了对共产党，对中国深深的偏见，这样带着偏见的争论，没有意义。不过也让我一探普通美国人的真正想法了。</p><p>可这不是赶上Covid-19了吗，中美的对立变的十分尖锐起来，一下子没忍住，丢了一朋友。美国国内为了转移压力，不断地向中国泼脏水，你说为什么有人会信。中国的名声在西方媒体的长期浸染下，说什么，他们都会信(Critical think简直笑话)。</p><p>为了跟这个老美争论，登录上电脑微信。但还是速度比他还是慢了好多（妈的，有本事咱用中文辩论，这帮傻X），也犯了辩论的大忌，太过于激动，应该更加理性一点就好了。</p><p>他们总说我们被中共洗脑，何尝他们没有被西方媒体洗脑呢。况且这老美还是一个老兵。我X了，跟那个国务卿的言论一摸一样，估计就是人说什么，他就信什么。不过也最后跟国务卿一样，反嘴就说有可能不会是人工的（man-made）。人家是政治家，说瞎话是人设，你丫的怎么就信了呢。</p><p>还说其实政府做的不错。我说你管100多万（目前为止）叫不错。他反击说那是因为我们测试的多，你再看看欧洲，伊朗。我抬起头，god,我能说什么呢？你看看你的信众，到底是乐观还是傻逼？<br>还说你看哪个政府给人民发钱了。这一点是不错，这才是世界上最强大的国家应该做的不是。而不是推卸责任，甩锅中国。</p><p>还跟我说历史上的病毒大多来自中国。操，这下把我惹火了，说瞎话呢啊，流感呢，艾滋呢？然后被他逮住了漏洞，说我事实都不知道就来跟白人辩论，你再这样，可能会失去我这个朋友（Whatever）。流感来自英国，艾滋来自非洲。我说的是爆发。但他还是生气了，说艾滋被中共长期宣传，把责任推到美国同性恋那(他一个Gay)，非常卑鄙。他把Gay这张牌突然打出，我突然一愣。意识到我们被长期宣传告知的东西，跟他们完全不一样，甚至相反。用其中任何一个去攻击另外一个，只会两败俱伤，没有意义。</p><p>于是我跟他向艾滋这个话题道歉: 从来没有觉得Gay people应该来为此负责。这是我们人类的共同的灾难，不希望任何一种灾难发生在任何人身上。</p><p>意识到这一点之后，我也变得稍微平静下来。跟他说结束谈话吧，这样的争论没有意义，我们所处的环境和立场无法让我们做有意义的辩论，你用你的fact, 我用我的fact…</p><p>之后他还给我发微信，说他有很多中国北京朋友，因为covid-19失去了工作，很困难。还说有个中国朋友告诉他，姐姐丈夫死了，中共不让她回家直到跟着一个党员才行，这很让他震惊。<br>尼玛，我没明白他跟我说这个什么意思，也有可能他没说清楚。没有人权？没有自由？<br>不过西方就是这个样子，抓住一个黑料，否定你的全部。我们也从来没说我们完美无瑕不是。</p><p>之后我再也没有理他。冷静下来之后，我决定不再跟他做朋友。这老美是割裂的，一边他喜欢跟亚洲朋友，尤其是中国朋友在一起玩（他中国男朋友是我同事）。另一方面，却总说中国不是，认为中国偷技术，不遵守国际规则（应该是美国规则吧），有很多不人道，侵害人权的事情，最不可原谅的是共产党。</p><p>这让我很不舒服，你要喜欢跟中国人在一起，那请不要在中国朋友那说中国的不好，多了解下中国。</p><p>这让我怀疑，喜欢跟中国人在一起，是想享受他们的‘好脾气’，享受他们的‘事不多’，还是享受自己高人一等的优越感？<br>FUCK YOU! 老子不陪了。</p><p>突然产生了一种责任感，必须为祖国，为世界做点什么。希望世界是和平的，人类是幸福的。但幸福之路，靠充满感性的现阶段人类来说，太远了。人工智能和太空探索等科技发展，我觉得会帮助人类少点纷争，多些连接，少些猜忌，多谢透明，少些感性，多谢理性。</p><p>想起这句话：被误解是做事者的宿命（原句是表达者）。这世界太多的是非曲直，没法判断，要想做成一件事情，背负误解会很多，能解释就解释，无法解释多用结果来说话。</p><h3 id="20200504"><a href="#20200504" class="headerlink" title="20200504"></a>20200504</h3><h3 id="20200505"><a href="#20200505" class="headerlink" title="20200505"></a>20200505</h3><p>莲花清瘟终于是买到了6盒，尽管很贵，但买了也比较安心。<br>父母的不断叮嘱，很是不耐烦，但还是很有效果的，让你意识到事情的重要性。在他们的催促下，之前就买了一次莲花清瘟，可惜店主跟我说货卡在海关那过不了。后来大使馆两次发急救包，也错过了。这次无意间刷到有美国境内邮寄连花清瘟的，赶紧下单。</p><p>国内莲花清瘟已被证实有效的情况下，国外还是不相信，说没啥用。不得不说，政治的不认同，甚至延续到了“科学界”。</p><p>最近很讨厌瑞典这个国家，以前听说跟中国很友好，可自从那次可恶的辱华事件后，关系现在应该说很不好了。不得不说，中国的强大，总会让先前的发达国家很不开心。这就像你认识一个人，穷，没什么危害，你给予善意，或者给与帮助，充分展示你的同情心，而同情心就是一股自上而下的情感（默认你比较高级）。后来他发达了，希望为自己争取更多的权利，你受不了了，因为你认识的他是那么卑微，应该是祈求着你施与帮助，现在竟然跟你平起平坐，甚至比你厉害了。这时候你应该会酸溜溜的说一句:”遥想当年，我们是怎么帮助他的，你看他现在的德行，还是跟以前一样粗鄙。”</p><p>国家之间的争斗，跟人之间的争斗没有什么区别，而且更加直接，更加显得原始。毕竟国家也是人组成的不是。</p><h3 id="20200506"><a href="#20200506" class="headerlink" title="20200506"></a>20200506</h3><p>真是奇怪，自然醒的时候，倒挺早。有事早起，却怎么也起不来。从6点半醒了一下，换个姿势继续睡。然后再一睁眼，8点多，还是起不来，最终9点多才拿起手机看看资料。不知道有没有叫“睡眠逆反情绪”的心理名词，越有事情要做，越不想起，越想睡。这是潜意识在逃避起床后的困难任务吧。如果是起床后是玩一款新游戏，保证早早就蹦起来了。</p><p>接着饱饱的吃一个网红早餐，打开CNN，开始一天的工作。<br>最近听到不少新闻开始说病毒很大可能不是来自中国的了，但还有些被采访的人尤致不肯死心，像小孩赌气似的，是的，是有可能不是来自中国，但目前不是特别确定。让这帮人说出这种的前后打脸的话，不得不说，那么多的科学界实证，你再扭曲也扭不了吧。<br>群里有朋友说CNN有点“赤化”。这只是个玩笑话，CNN只是说了实话，反川普罢了。或许我该听听Fox。</p><p>快结束工作时，好久没联系的朋友打来电话。问我咋快半年没信了，还问我是不是得了“肺炎”才没联系她，汗😓。都有这么长时间了吗？自从家里蹲之后，时间的流逝就成了指数式的，越过越快。根据爱因斯坦的相对论，跟姑娘聊天时间总是很快，在巨热的火炉边上，才会度日难熬。难道是我这居家的日子过的太逍遥了，所以时间飞快？</p><p>我们不知道不觉聊了1个多小时。聊了呆在家里的近况，或许是好久没联系，这把积攒快半年的话给说了。有时候距离产生美，天天碰面的或许还说不上几句交心的话。朋友因为疫情原因，工作找的并不是很顺心。就见不得朋友因为不顺心的事情陷入抑郁，于是我“引经据典”，跟她瞎扯。这种事情，说什么都没用，归根到底，自己要能消化掉。朋友家人就听听你的牢骚，大家说说笑笑，就能把不开心的事情给暂时忘了，甚至能重拾斗争的勇气。</p><p>我也是聊的兴奋了，好为人师的老毛病又犯了，好在朋友给面，愿意听。个人的经验永远是个人的，他人没经历过，再说多少遍也没有踩一次坑来得明白。</p><p>谈话期间，烤的披萨有点糊了，红薯却依然很坚硬，正好问了她红薯什么时候能熟，她说要2个小时（我去，网上为啥都写着25分钟啊）。写到这了，刚刚把2个多小时新鲜出炉的红薯给吃了，外皮烤的还挺硬的。一下子让我想起儿时，在田野里烤红薯的经历，拷出来的红薯外皮坚硬，内部软而不面，那时候感觉真好吃。长大了，身边朋友会买烤红薯来吃，我吃过几次，却再也没有儿时的味道，遂再也没怎么吃过。这次疫情，不是要存一些最坚挺的粮吗。不过，果然还是没意思的味道…</p><h3 id="20200507"><a href="#20200507" class="headerlink" title="20200507"></a>20200507</h3><p>今天天气是真的好。昨晚睡的晚了，早上起来很晚。浑身感觉感觉不对劲，到了该运动的界限了（懒人应该懂，一次健身管一个月）。一直在纠结要不要出去跑步。今天天气是真他妈的好。阳光，蓝天，白云，就像美女一样，诱惑我出门。出去一趟是比较耗心耗力的。我需要换衣服，需要全身消毒，需要把换洗衣服给洗了吧，需要洗澡…等等一系列下来，太麻烦了。</p><p>不过纠结了一下下，立刻行动换衣服出门（忍不了了）。第一次正式出门，装备还是齐全的，口罩，眼镜，帽子，手套。（千万不能让老爸老妈知道，他们天天叮嘱不要出门）。</p><p>环境不一样，大家的心态还真是不一样。之前国内家人，朋友都严格的居家隔离，非必要不出门。国外的环境，大家就没有那么严格。这边国人的防护肯定是比老美等其他人要好，但挂不住一种类似“从众心理”的心理，既然大家都能出门，没必要他一直都在家。跟朋友聊天，包括我室友，也会出门，有跑步，有散心，有买菜的。浑然不似国内，严阵以待。在父母看来，这个病得了就像绝症，很可怕。但这边他们被环境影响的，竟普遍认为没那么严重，得了也不会天塌下来。</p><p>不知道是不是带上口罩跑步的原因，竟没有感觉到空气的新鲜。想把口罩拿下，但是忍住了。打算绕着学校跑一圈，去实验室楼看一看。<br>学校这边人不算多，我散步似的在校园跑跑停停，拍一拍照。太漂亮了，树上树叶已经很多，眼看再过阵子，就郁郁葱葱，到夏天了。校园中，桃红色的梨花树在绿丛中很惹眼，让人忍不住驻足。记得梨树前的路是学校最繁忙的路，每天上下课期间，挤不动的人，那时候应该很少有人能注意到这美丽的梨花，也不知道她是否能开的那么旺盛。<br>不得不说，没有了人类的干预，让人感觉环境变得很干净，很新，很自然。应该说小动物最开心，没了人类，他们的活动范围想多大就多大了。</p><img src="/images/campus1.jpg" width = 70% height = 70% div align=center /><p>ISEC(我们实验楼)更加漂亮了。顺着校园的桥走到ISEC，朝门内望去，竟然有人在。在周边转了下，拍几张照片给实验室的伙伴发过去。心满意足，忍不住提起口罩，狠狠呼吸了下空气。</p><p>想着跑步那就绕一圈吧，当跑到大道上的时候，人开始多了起来，甚至有不少人没有口罩在路上行走。眼看前面岔道人更多了，果断拐向一个小道，准备打道回府。正好回来的时候，碰见楼下不远地方，开墨西哥卷的店主一家。貌似正在准备重新开店。不少店铺，竟然写着大字“Open”。想想餐厅是真的惨，以前都挤不动的人。他应该也是希望复工的那一拨人吧，生意做不了，没有经济来源。</p><p>终于到家了，我去，楼道的空气怎么这么难闻。出去的时候没啥感觉，回来倒是立刻感到室外空气的清新了。消毒，换衣服，洗澡，接着出门去隔壁洗衣服，回来再消毒，顺便把室内卫生也搞了下。这一顿下来，都下午3点了。懒人的我奉行原则是，要么不动，要动就把所有的事情一次性给做了。今天也一定要做一顿美食来配今天的“放纵”，和美丽的天气。鸡肉炒黄瓜，就你了。</p><img src="/images/ISEC0.jpg" width = 70% height = 70% div align=center /><img src="/images/ISEC1.jpg" width = 70% height = 70% div align=center /><h3 id="20200508"><a href="#20200508" class="headerlink" title="20200508"></a>20200508</h3><p>到了周五了，前室友结束一周的辛苦居家工作，喊我们一起看云电影（用hougout分享屏幕一起看）。以前她还在时，我们有时周末的时候会把电影投影到墙上，一边吃着零食一边看电影。<br>现在远隔一方，很少有机会看投射电影了。疫情搞的大家只能在家呆着，也是一个人太无聊。一起看电影，也看好热闹热闹。她跟我说之前还跟人一起看综艺。</p><p>我问她选什么电影，她说随意，好吧，既然随意，那我就选我最爱的《大话西游》了。她竟然没看过，说基本上没看过周星驰的电影。天呐，听说她小时候家里从来不开电视…可怜，没有童年的孩子。跟我们同龄人没共同话题了。好吧，就让她见识见识吧。</p><p>果然兴趣要从小培养，她竟然没啥感觉，好让我挫败，中途加入的另一个朋友也是没啥感觉（他竟然也只看过片段）。不知道该说啥了，换朋友吧。<br>所以看第二部的打算也就泡汤了，那可是紫霞仙子啊！</p><p>然后我们就开始闲聊了。主要也还是围绕疫情，工作和吐槽美国。Y朋友（另一位朋友）住在芝加哥，也因为疫情原因，实习期一过留不下，所以现在还在找工作。现在失业率太高了，想找到工作应该说很难。他一个人在芝加哥说太无聊了，还是觉得波士顿好，想回来。问我波士顿情况如何，我说严重啊。他问超市开吗？超市开他就回去，飞机票，租房都不贵。好吧，服了，就不怕中途感染上吗？还说要来我这一起上自习。额…咱碰面至少也要等你2周以后吧，我想。</p><p>通过跟不少留学朋友聊天，基本发现了大家对待疫情的重视程度跟国内父母亲戚叮嘱的程度成正比。因为大家都在国外，国外的疫情应对情况，大家都了解，之前都不戴口罩，即使现在，大家带上口罩也照常出来透气，买菜啥的。如果我们没有国内情况作为参考，我们也会跟外国人的认知差不多，没那么’在乎‘。<br>国内的疫情情况，加上国内的父母不断的叮嘱，即使会很不耐烦，也会让我们的潜意识里提高对疫情的重视程度。我有几个朋友，父母每天联系的，他们一般都不会出门，除非不得已买菜。但更有一些朋友，这些朋友应该属于类似“放养”的家庭？（小时候的我很羡慕放养）。叮嘱有，但不多，或者家里人也都是那种心比较大的，他们就会经常出门。买菜，遛弯，跑步各色情况。</p><p>有一朋友，在家都戴着口罩（除了自己卧室），因为他的室友就是那种跑很远，还坐地铁买菜，经常出门的人。人跟人还是很大区别的，但不管怎么样，希望大家要保护好自己，也要顾忌到他人吧。</p><h3 id="20200509"><a href="#20200509" class="headerlink" title="20200509"></a>20200509</h3><p>今天不得了，做了一个美食的梦。可惜就是没吃上，一直在等人。等来了，就不记得后面梦的剧情了，或是醒了。<br>真是馋死我了，天天看抖音上分享的美食，让我更加想回国了。按下决心，回国后，一定要天天下馆子。如果更有机会全国旅游，要吃遍抖音上分享的美食。<br>这就是日有所思，也有所梦吧。</p><p>梦里的吃饭的地点，是小时候上小学的路上的一幢房子，那里有个小店，以前在那买过很多吃的和玩具。很是奇怪，好像很多梦里，总会梦到上学的那段路，总是黑黑的天，一眼看不到尽头的路。路边一座孤零零的小店铺，以及小店里很严肃的老爷爷。</p><p>仿佛那段路，就是我小时候，小小的人眼中，非常高大的整个世界一样。无论是好梦，还是恶梦。想来还是在那条路上的恶梦居多。小时候特别害怕空旷阴森感，所以即使到现在，也还会在梦里不时出现那种感觉。可是小时候过的还是蛮开心的啊。</p><p>前几天听了美国Fox新闻上讨论疫情对人做梦的影响。我当时想，我去，这也能当一期节目做，可真是闲的。还煞有介事的找来了几个人，一顿谈。不过当然是有影响了，我这梦到要吃好的，不就是一非常好的例子。</p><h3 id="20200510"><a href="#20200510" class="headerlink" title="20200510"></a>20200510</h3><p> 今天是平淡的一天，应该需要做点其他的事情了</p><h3 id="20200511"><a href="#20200511" class="headerlink" title="20200511"></a>20200511</h3><p> 今天开组会，上来我们会先唠唠嗑，例行问候下。问你和家里人如何了，大家一般都会说很好。但老板今天随口说了下他父亲得了肺炎。我们一听，这咋整，赶紧询问下情况如何。老板非常轻松的说，一点事没有，父亲自己去医院住了几天，什么症状也没有，现在快回家了。好吧，这还是第一次听到熟人说身边有人得了肺炎。但貌似不严重，这都很长时间了，才听老板提起，他也是正常工作，正常跟我们和其他人开会。看来他们真没有把肺炎放在眼里啊。可能在他们看来就是一场流感，是可以轻松治愈的。<br>我问他父亲是否经常去Gym，身体状态很好。他说，以前经常去，不过身体保持很好。果然，看来健康的身体，是抗疫的关键之一啊。</p><p>从昨天开始，有点上火，头有一点点难受，然后昨天又熬了夜，导致今天不知道怎能搞的，嗓子还有点不舒服。刚到的连花清瘟，正好吃上了几颗。应该不是肺炎，其他身体状态都挺好，正常工作休息。我这不怎么出门的，基本不可能得上，但一想我前几天出了躺门，还有室友经常点外卖…一会再吃些连花清瘟吧。</p><p>必须把落下的锻炼身体的任务也给拾起来了，至少2套跆拳道基本伸展（之前学过一点），至少30个俯卧撑，1个平板支撑。嗯，加油！</p><h3 id="20200512"><a href="#20200512" class="headerlink" title="20200512"></a>20200512</h3><p>昨天的一个新闻特别火了。川普记者会上，一个亚裔记者问了川普为什么国内死那么多人了，你还要跟国际比赛测试的人数，这重要吗？然后川普可能被激怒了，直接回答说：ask China（问中国）。亚裔记者，当场就不干了，你为什么针对我说。川普答：因为你问了一个nasty question（脏问题）. 然后川普直接结束记者会。</p><p>这是一个非常恶劣的事件，迅速出现在各大新闻头条上。对川普的批评不绝于耳。批评主要集中在川普的无礼和种族主义。对于种族相关的话题，美国国内是非常敏感的，也不愿意公开提及。就像川普，一个标准的种族主义者，也不敢公然直说，但言行还是出卖了他的内心。有句话说的好，之所以在乎，是因为这个问题还严重。只有在大家可以大胆谈及的时候，问题才不是问题了。</p><p>川普当总统，正是迎合了一帮美国种族主义者。美国人的教育一直以来都是，伟大的美国，世界最强大的美国，导致了他们的内里充满了一种自豪感。大部分的人对其他国家并没有那么感兴趣，他们了解到的国家，文化，基本都是道听途说，山寨的。这无疑是坐井观天，好在美国确实是地球上最强大的国家，还够他们观一会的。</p><p>现在媒体是特别不待见了川普，民主党更是对川普不遗余力的打击。但就像一个流氓，本身污点多多，多一件不多，少一件不少，死猪不怕开水烫的感觉。</p><p>这次疫情，让川普赶上了。媒体下，就会吹嘘，就会推卸责任的川普展露无遗。如果这还能当选下届的话，那只能说像川普这样的人，美国真的很多，应该会让很多移民伤心吧。</p><h3 id="20200513"><a href="#20200513" class="headerlink" title="20200513"></a>20200513</h3><p>不出门真的很难受，呆的头晕。搬过躺椅到窗口，努力呼吸外面的新鲜空气，开始写今天的记录。</p><p>待家里时间久了，生活逐渐变得有规律起来。从开始的极不适应，然后不断努力挣扎，到现在的习惯。这就是一个系统从开始的混乱，到逐渐自驱动收敛的一个过程。收敛过后的状态其实还是蛮好的。不用每天挣扎各种情况，只需要按部就班。定点吃饭，定点学习，随时休息，随时做自己想做的事情。效率明显变高了很多，思考也变得更加深入。（说实话，平时大家忙忙碌碌，到底有多少是有效利用呢）。</p><p>得到这样的效果其实是不易的。一是要没有人打扰。现在有时候感觉，没人打扰实在是太爽了。<br>要知道，系统在没有扰动或者这个扰动也是规律，可控的情况下，才能自我收敛。生活中，一些意外的扰动，常常造成系统的奔溃。或许不至于奔溃，但发散在所难免。这就解释为什么很多厉害的人物，总是在默默无闻一个人的时候，能够做出举世瞩目的成绩，反而到后面，成就越发渐微。</p><p>牛顿年轻时，伦敦发生瘟疫，他为了躲避瘟疫，在一个农场呆了很长时间，完成了他此生最重要的成就，也开启了现代物理学。之后的人生，成就不再明显。</p><p>当我们所说的系统逐渐稳定的时候，可以聚焦问题，很容易解决。而如果外界不断的给予新的刺激，很有可能就会无法聚焦，导致问题变得发散无解。</p><p>这里再举一个我喜欢的歌手，周董的例子。年轻时，无与伦比的才华横空出世。到现在，什么业务都涉足的周董，再也没有了往昔创造力。</p><p>这是一段难得的经历，尽管是一场人间悲剧。需要好好利用好这段经历吧，多学点东西。<br>在想如果情况马上变好了，怎么去维持这种系统收敛的状态。应该又要经历一阵子巨幅震动。收敛的过程肯定很难，而且我不是一个能特别会应付外界环境的人，外界的环境对我的扰动还是蛮大的。（不同的人，不一样事可能不一样，但我这个人，还有我要做的事可能都需要一个稳定的环境吧）。不管怎么说，也必须学会增加自己的robust(弹性)特性，要学会拒绝一些环境带来的扰动，这个也叫denoise（去噪）。</p><h3 id="20200514"><a href="#20200514" class="headerlink" title="20200514"></a>20200514</h3><p>现在每天听新闻，每过一阵子，必谈到中国。随着川普的返工政策的实施，以及未来越来越近的大选，对中国的不断甩锅，仿佛成了他唯一能做的事情。这也是为什么新闻都要谈到中国。主要集中在这几个问题上，一个是对中国的制裁讨论，一个是对中国数据真假的讨论，另外一个是对于中国防御措施的讨论。</p><p>不过他们普遍认为中国的数据不可靠，讨论很少。而对于中国的防御措施手段，更是了解甚少，总是从新闻中听到韩国怎么怎么样，我们是否应该怎么样，很少听到说从中国学习什么防疫措施。</p><p>而对中国的制裁新闻最为热烈。大家做上小板凳，吃着瓜想听听有什么新意的讨论。听来听去，都是那些：这是由中国传播出去的病毒，他们要为我们的损失负责；中方很多生物实验室之类的。这帮人所宣讲的内容全是蓬佩奥的那一套，他们需要一个声势，不管是不是事实，反正听到才能让有些信。</p><p>然而科学界的证据以及科学家却不忍心让这帮政客歪曲事实，颠倒是非。况且，现在最主要的事情是抗疫，抗疫啊。有的一线科学家更是痛心疾首，直接明言，川普将带领美国走向更加糟糕的疫情局面。</p><p>虽然疫情很严重，但老美仍然没有过于害怕这个疫情。怎么说，美国人的与危险相伴的能力要比我们强太多。他们成长的世界跟我们完全不同，有各种贫民区，也有各大黑帮，违法吸毒，家家有枪支，晚上不出门，还有一些流行病泛滥等等…这些都早已融入他们成长的血液之中。</p><p>不过说到底，还是不同的历史文化造就了不同的世界。你看欧美历史上，动不动就要决斗，动不动就是邻邦的战争，随时死人那种，荣誉大于生命（有没有点现在他们喊的自由大于生命的赶脚），城邦林立。而中国的历史上，相对他们要平稳很多，大部分是农民，安居乐业为主，不管谁当官，只要安稳就好，向往田园文明，发展礼制。</p><p>不管怎么说，现在的两大历史文明，欧美和中亚，肯定会不断的碰撞。我个人是喜欢一个更加“文明”，更加“理性化”的世界。但究竟哪个历史文明更加有利发展出这样的世界，还有待进一步观察。不过更有可能是会是相互融合，殊途同归那种吧。毕竟，人工智能的世界必将到来。</p><h3 id="20200515"><a href="#20200515" class="headerlink" title="20200515"></a>20200515</h3><p>这两天的天气是真心好啊。没隔离以前，太过于假忙碌，很少注意到美好的天气，更不会说跑出实验室去室外专门去享受阳光。可是现在隔离在家，一看到外面阳光灿烂，内心就躁动不安，特别想去外面溜达。</p><p>躁动不安的可不止我一个人。朝窗外望去，三三两两的人群出来散步。对面学校的标志那，甚至排起了队伍，好多等着拍照。更有甚者，在校园草地那躺在那嗮太阳，做游戏。几乎没怎么看到他们戴口罩。确实是，我试过，在外面戴上口罩，呼吸很不爽，有自己的口气，也感受不到外面空气的清新。拿下口罩，狠狠的呼吸一下，又赶紧戴上，怕那个万一。看来他们老外貌似没有这顾虑。</p><p>想想还是在窗口努力呼吸几口吧。你还别说，贼新鲜。专门把半躺椅放到窗户口下，就在这办公了。之前也放过，那时天气还很冷，呆不了多长时间。这才几天，忽然一夜夏风来。对面的大树从刚抽芽，到现在树叶包裹都快变成了一个球。</p><p>晚上突然下了场雨，开始小，后面一下倾盆而下，不过这雨下的很干净，利落，没多久就停了，仿佛就是为了宣誓夏天的即将到来。</p><h3 id="20200516"><a href="#20200516" class="headerlink" title="20200516"></a>20200516</h3><p>好惊喜，快睡觉了，突然听到CNN新闻上，播放钟南山爷爷的采访。他们把钟爷爷比喻成中国的福奇医生。在这次抗疫过程中，他们都有着至关重要的作用。遗憾的是，美国的钟南山，福奇医生却没能发挥出来。政府，川普并没有听取福奇医生的建议，而且后面被川普辞掉，非常遗憾，他只能在新闻媒体上表达对疫情的担忧，对政府政策的不满。</p><p>采访中说，虽然钟爷爷的建议没有被武汉当地政府采纳，但很快，中央政府听取了他的建议，很快措施就开展了。听到我们国家对钟爷爷，对科学家建议的重视，他们表现出短暂的沉默，大家都心照不宣了，美国这次防疫，川普政府很不行。科学家的建议也不听取，各种大意不在乎。现在还要进行复工，貌似只能是走一步看一步了。</p><p>这次疫情，还是有很多看的明白，科学家在川普政府那虽然人微言轻，但无一例外，都讲事实，不歪曲。这也无形中，也让部分人明白，川普就是在甩锅中国。希望媒体真能够少采访些政客，多采访些科学家。这才是国家的希望不是。</p><h3 id="20200517"><a href="#20200517" class="headerlink" title="20200517"></a>20200517</h3><p>生活按部就班，也越来越有状态。现在每天都能保证至少7个小时的学习。有的人说7个小时算什么？以前没有疫情的时候，那时候，8小时还真不算什么，有时候10小时也有。但即使10几个小时，也一直没有过“今天足够了，明天再干的”满足感。可以说8个小时基本是连续的，这样有的人会说会疲倦。会的，所以会在完成一个小任务刷几下抖音。接着完成下一个任务。不间断的学习思考，可以让工作学习更加高效。平时我们生活中，太多的被打断，导致很多情况下无法进入深度思考，效率自然而然比较低。这也是那么多人喜欢早起或者夜晚的学习的原因。</p><p>想起博士办公室坐我后面的一个中国朋友，他就是那种喜欢早起的人，4点钟起床，早早的来实验室干活，然后下午3，4点就回家了。之前办公室人不多，他还坐我边上，后面人多起来，他直接躲到了他自己的实验室，每天专注的干上7，8个小时，然后再去干自己喜欢干的事情。好多次看他在朋友圈发看完哪本书的分享。他优秀是很有理由的不是。</p><h3 id="20200518"><a href="#20200518" class="headerlink" title="20200518"></a>20200518</h3><p>今天麻州开始实施Reopen（重开）计划。一些教堂可以开放礼拜，但不能进行聚集活动，还有一些必要的经营也可以开放，比如说制造业和建造业等。<br>在过一周，一些办公室也可以开放，也就是说可以去上班了，但加了一条，说不包括波士顿。因为波士顿地区占据了疫情人数的大多数，也是容易聚集的区域。</p><p>快到夏天，人心浮动，坚持抗疫变得越来越常规化，也变得没有开始那会的紧张。会看到很多人出门运动，转悠。我发现自己也没那么害怕疫情了，出门也没觉得那么可怕。你看这种从众的心理真是要警惕啊。今天才后知后觉，也并不仅仅因为疫情不出门，还有一个重要原因就是我本来就是宅男啊。出门基本都是因为一个原因，一个承诺。现在这些都可以没有了，那有什么理由不在家呆着呢。</p><p>傍晚看着窗外的不多见的淡红彩的晚霞，感叹夏天真是来了。回想起在家时经常跟父母迎着晚霞去遛弯的时光，当时还想着他们耽误我看电视，打游戏，现在想想那才是值得珍惜怀念的闲暇光阴。想想也多年未回家了，奶奶的，有点想跟他们压马路了。</p><p>上次，老爸问我还会搞对象不，我一愣，笑着回答说，可能还真不会了。跟女生聊天太耗时间，而且自己也从来不想将就，这是一点也没有单身狗的觉悟啊。<br>单身太久，总是享受站在窗户口看风景，这也挺好的。不过想来，若还有一个人，催促我这个宅男出门，虽然我可能嘴上不乐意，但行动和内心还是会很服从的吧。</p><h3 id="20200519"><a href="#20200519" class="headerlink" title="20200519"></a>20200519</h3><p>抖音任命Disney原CEO担任新的TikTok的CEO，在国际化的路线上越走越远。短短几年，TikTok已然成大树。甚至一度要遭到美国政府的封杀。我出国前，基本没看过什么直播。那时候，大多数是那些游戏直播和美女直播类为主。之前还会想一个平常人怎么愿意把自己的生活给视频展示给别人呢，没想到，这股风迅速飞起来，录个抖音跟发个朋友圈似的。年轻人更是趋之若附，各种创意，各种百态都能从抖音里看到。</p><p>为了不让自己跟国内脱节，我安装了抖音，同时也安装了抖音国际版。对比两个版本，国内的抖音视频更加精良，更符合国人审美，它的制作现在越来越专业化，运营化。国外的版本，看得不是特别多，因为感觉大多数是就像是青少年的朋友圈，充斥着美国式的整蛊，视频制作不是很精良，更像是美国年轻人展现酷的方式。</p><p>刷抖音很容易陷入进去，明明知道你还有正事要做，可是就是一个接一个不停的划下去。之前我还以为国外没有什么精良的视频。最近也刷到了几个留美在外的博主，视频做的也还不错。强推一个叫“角角独角兽”的一个抖音，应该是我看到在美最好的视频播主了吧。还有几个都是留学生的播主，也还不错。他们能在兼顾学习的时候，拍摄大量的素材，剪辑，我是很佩服的。让我来，是做不了的。一是没有那么多精力，我这个人，不能同时做2件以上的事情，不然很容易整段垮掉。二是本身比较腼腆，让我去上镜估计比让猴子上镜还难。</p><p>总结下，国内版抖音更加成熟，无论是视频制作，还是运营变现，而且主流思想价值观比较正。国外版抖音正处于高速发展期，是年轻人的聚集地，但不是很成熟，这也是为什么引进做内容很厉害的Disney CEO担任CEO的原因吧。</p><h3 id="20200520"><a href="#20200520" class="headerlink" title="20200520"></a>20200520</h3><p>哇，今天是520，是个情侣秀恩爱的日子，真是让人糟心，坚决不刷朋友圈。而且这不头疼了一天，从前几天就开始了，今天疼的尤其厉害，不能够是感染了吧。可是查看症状，并没有头疼的症状。“应该是晚上睡觉太晚了吧“，这样安慰自己。</p><p>晚上一个女性朋友叫我帮她安空调，我没搭理，疫情期间，不晓得隔离吗？真跟老美似的，不在乎了啊。而且我头疼，万一，也不能传染了你不是。</p><h3 id="20200521"><a href="#20200521" class="headerlink" title="20200521"></a>20200521</h3><p>早上看到咱学校发邮件，说学校有医学院的学生在医院里服务。一下想起来，我壁球教练就是医学院的学生，赶紧问她是不是情况怎么样，是否安全，问她要不要口罩，可以提供给她100个左右的常规医用口罩（因为不常出门，所以国内姐姐邮过来和我自己买的都没怎么用）。她回我，她没有在医院继续干活了，因为只有她一个人是不付工钱的，所以3月份的时候，她就离开了，回家隔离学习，准备医学的各种证书考试。</p><p>她仿佛是怕我误解她没有志愿者精神，给我解释很多。但我还是很理解的。美国跟中国不一样，中国的医生护士，在这特殊的时期，就是祖国最伟大的人。他们的奉献精神被全国人民认可，赞许，他们的付出，自己也认为是很有值得的。而美国这边，医生护士也很不错，该履行职责操守都有，即使物资短缺，无防护上岗，政府无能。但并不能要求一个免费劳动力冒着巨大风险来奉献自己。</p><p>在美国，当医生护士很不容易，需要通过各种资格认证考试，我这个教练已经毕业了，还在挣扎于各种资格考试，去医院实习也只能是免费那种。所以每一个持证上岗的医生护士都应该是蛮厉害的。</p><p>她之后帮我也问了她的医生护士朋友是否有需要，问了一圈，说除了N95，普通的已经不需要了。我说这是好事也是坏事，好事是基本的防护有了，但坏事是重要物资还是很短缺。这将是一个持久战呐。</p><h3 id="20200522"><a href="#20200522" class="headerlink" title="20200522"></a>20200522</h3><p>不断听到关于疫苗的好消息，先是美国这边的接连传出疫苗实验结果很好的消息，后面国内也传出第一期实验结果，全都产生了抗体的好消息。虽然是好消息，可惜疫苗的上线时间，依然要等到好几个月后。对于美国的疫情，会是一个持久的战斗，我不知道还能不能呆屋里那么久了。估计到9月份，大家都复工，那时候又将是一个什么样的情况。大家一起”破罐破摔“？</p><p>听到这帮美国政客污蔑中国的游戏，已经听腻了，没什么花样。是你，就是你，就是你传播的，就是你要搞我。这次幸亏有科学家的石锤证据，不然这帮不要脸的西方政客更加张牙舞爪，肆无忌惮。</p><p>又来干涉中国内政，香港问题还要通过什么议案，关你屁事。想想这帮香港港独分子，也是够傻逼的，早就说过，你闹腾的话，国家会更加管制你的，这不两会就通过了国安法。</p><p>晚上，又跟朋友一起云看电影，看来周五晚上变成我们的例行电影日了。远在芝加哥的朋友抱怨说伊利诺伊州现在疫情比麻州还要严重，房东每天都正常上下班，一点没有防疫的意识。</p><h3 id="20200523"><a href="#20200523" class="headerlink" title="20200523"></a>20200523</h3><p>看Ronny show, 东南亚的疫情, </p><h3 id="20200524"><a href="#20200524" class="headerlink" title="20200524"></a>20200524</h3><p>晚上听了一个CNN记者的口述报告，关于疫情下的中国和美国。因为是报告，肯定不能跟那帮政治人物一样喷粪，所以有的陈述还算中肯，但也避免不了一些先天的刻板映象，比如中国的一党专治，没有自由云云。他的陈述，主要有这几方面，中国到底有没有瞒报信息或者信息透明；中国是怎么来抗战疫情的；面对国际，尤其是美国的”质疑“（污蔑），中国是怎么面对的。</p><p>对于中国有没有瞒报。记者很”中肯“的说：当地武汉政府肯定是有隐瞒疫情的，导致疫情控制不及时，但北京中央知不知道，有没有隐瞒，还不是特别清楚，但根据中国一贯的隐瞒信息，不是不可能。他这里着重提到了吹哨人李文亮医生的遭遇。<br>这里我想到了方方日记，国内的大势是一口否定，我是觉的有点过了。无论再光辉的胜利，总会伴随着一些黑暗，被写出来了，被记录出来了，甚至是道听途说写出来了，但我想方方应该是没有恶意的吧，她希望这些事情都不要发生，或者希望给予一些惊醒吧。我认识一个一心想留在美国的中国人，不知什么原因特别痛恨共产党，他是那种不希望中国共产党好，中国好的人，这种是很可恶的。但我们大部分人，说你不好，是自家人，是希望你变得更好。</p><p>不过也可以理解？在特殊时期，不希望有的声音动摇了民心。这永远是一个伟大老祖宗关于”度“的问题(国外叫trade off)。太多自由言论（free speech）, 容易滋生谣言，造成社会动荡，完全没有free speech，又易滋生黑暗，无法发现问题，进而发展进步。</p><p>关于中国怎么抗疫的，他陈述中，描述了中国的高科技抗疫，但有隐私和强制的一些问题。</p><p>关于两个国家互相blam,相比于cold war时候，跟苏联联合生产天花病毒的疫苗，现在两个超级大国却玩起了互相指责的游戏，这救不了任何人。<br>中国不在韬光养晦，开始比较激进了。<br>中国因为瞒报，晚了5，6天，但美国川普却明明知道的情况下，不重视，导致晚了5，6周。</p><h3 id="20200525"><a href="#20200525" class="headerlink" title="20200525"></a>20200525</h3><p>今天完成了一个目标，特别高兴</p><h3 id="20200526"><a href="#20200526" class="headerlink" title="20200526"></a>20200526</h3><p>做了一个黄瓜炒鸡肉<br>港独思考<br>特别想开个小酒馆<br>复工，海滩，年轻人死亡</p><h3 id="20200527"><a href="#20200527" class="headerlink" title="20200527"></a>20200527</h3><p>space X, NASA</p><h3 id="20200528"><a href="#20200528" class="headerlink" title="20200528"></a>20200528</h3><p>美国失去了10万人的生命</p><h3 id="20200529"><a href="#20200529" class="headerlink" title="20200529"></a>20200529</h3><p>最近，被警察暴力执法死亡的黑人，俨然再次点燃了黑人抗议种族主义的导火索。大家通过各种方式纪念Floyd，为他声张正义。那些个警察得到严惩，全国各地发起游行，纪念活动。甚至延续到了国外，世界各地出现了很多抗议游行。</p><p>美国的暴动越演越烈。示威者从开始游行，逐渐演变到暴动，破坏。很多商店超市被抢，房屋被砸，被烧。</p><h3 id="2020531"><a href="#2020531" class="headerlink" title="2020531"></a>2020531</h3><p>这几天陆续收到朋友和家人的微信，询问我在这边的情况。动乱一直在持续，而且越演越烈的倾向。<br>早上，姐发来微信，说貌似波士顿也发生暴乱，不是很安全，叫我别出门。我笑着说姐，你比我收到的信息还快。我这边消息还停留在前天波士顿很文明的游行活动。<br>不得不说，现在的技术世界的消息传播速度是真心快，尤其是关于中国和美国的新闻。我有时候一边刷抖音，一边听着美国CNN的广播，甚至都能听到相同的新闻报道。</p><p>所以朋友说想通过我了解了解美国，直接看抖音甚至了解的比我还多了，哈哈。<br>不过隔着屏幕，终归不等于体验。国内爸妈看到新闻，都非常担心。其实有时候，他们发来消息，我才知道发生了什么。对于我们平常的生活，对于不处于风暴中的我们，影响没有那么大。</p><p>但美国在向着不好的方向发展，世界也变的越来越动荡，其实前些年就开始了。未来科技的发展，跟生产关系的矛盾，会越来越尖锐，处于底层的劳动力变得可有可无，面对这部分人群怎么规划，怎么安置，有可能成为避免动荡其中一个关键因素吧。</p><p>可以想象，人工智能时代到来之前，人类的发展一定会经历一个个阵痛。加油吧！中国，加油世界！</p>]]></content>
      
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>疫情日记6,7月</title>
      <link href="/2018/06/08/%E7%96%AB%E6%83%85%E6%97%A5%E8%AE%B06,7%E6%9C%88/"/>
      <url>/2018/06/08/%E7%96%AB%E6%83%85%E6%97%A5%E8%AE%B06,7%E6%9C%88/</url>
      
        <content type="html"><![CDATA[<h2 id="202006"><a href="#202006" class="headerlink" title="202006"></a>202006</h2><h3 id="20200601"><a href="#20200601" class="headerlink" title="20200601"></a>20200601</h3><p>今天开始公司实习的第一天（也是在家办公）。终于可以有正当的理由，把大把的时间花在上面。之前把公司的任务总是排在科研之后，导致效率很低下。不过，科研也不能放下，不然一放下，再拿起就很难了。时间管理现在显得很重要。还是不太喜欢叫时间管理这个词，特别高大上的，特别繁琐（可能也是跟我很多次管理失败有关）。其实只要时间能够有效利用，就很好了。刻意去按时按点，规规矩矩，反而不美。</p><p>今天头还是疼。早上的时候，特意出门溜了溜，作为新月开始的第一天。早上基本没几个人，我走到校园里，拉下口罩，放肆的呼吸着新鲜空气。拍了几段视频，有机会剪辑成抖音视频。<br>下午的时候，头更疼了，跟一个非常需要注意力的枯燥的活也很有关系。一看不行了，赶紧躺下睡了会，平时我休息很短时间就起了，这次头很沉，很快睡着了。<br>”不会是得新冠了吧“，应该不是的，跟新冠症状还是不像，应该只是天气变化原因，导致的小感冒。</p><p>话说大家现在都麻木了。大街上，下午遛弯的人特别多。跑步的，野餐的，遛狗的，三三两两。值得欣慰的是，大多数还是戴着口罩。<br>就连小心翼翼的华人也麻木了。我的很多朋友该出门的出门，甚至有聚会打麻将的。疫情过了那么久，大家的神经早已经不像开始那样紧绷，生活还要继续。更重要的是，从众心理的作用。大家都这样，所以你也能这样。最近刷到一则抖音，说哈佛推荐的十本必读书，其中之一是《乌合之众》，好巧。</p><h3 id="20200602"><a href="#20200602" class="headerlink" title="20200602"></a>20200602</h3><p>刚天黑，窗外突然传来口号声，以及嘈杂声。朝外一瞧，原来有游行在我们们前的大街上。之前听说了好几次波士顿游行了，但一次也没经过门口的大道。这边没有商圈，只有学校和住宅居多，所以游行都不会很乐意来的。一般他们游行，都会在老市政府大厦的门口，或者是backbay那边波士顿最繁华的地段。这不听说，高端的prudential商圈前几天晚上<br>游行的时候，被人破坏了，被抢了好些东西。</p><p>窗外的游行，看起来还算文明，大家统一服装，看不清楚是否带了口罩，但安全距离肯定是很少人遵守。他们占用了整条街，队伍最后面的汽车不知道是不耐烦，还是说呼应他们的抗议，不断的按着喇叭。这条街是居民区，没啥人，你就看到他们喊了几句口号，就不怎么喊了。应该是要保存体力到繁华的Backbay那片区再喊吧。</p><h3 id="20200603"><a href="#20200603" class="headerlink" title="20200603"></a>20200603</h3><p>今天是交房租的日子（其实是我拖了2天），早上起来，做好防护。出门先去交完房租，然后迎着朝阳去校园里转悠了一圈。说是朝阳，其实已经升的蛮高的了，不过早上，大家貌似都不怎么出门，一到下午，他们就都出动了。所以这时候街上并没有几个人，不过校园里今天不少的施工人员，在整修校园。有戴口罩的，也有没戴的。他们算是最早一批麻州批准复工的人员了。这些工作的确很需要人工人力。到没人的地方，我就拨开点口罩，努力的呼吸几把新鲜空气，拍几段视频，没有人的校园，是真挺好看的。</p><p>最后绕一圈后，来到了校园学校标志那。之前每天站在窗户口，看别人在这里拍照，终于我也可以在这里拍了。可惜我没有模特，只能拍张空荡荡的标志了。</p><p>晚上吃了豌豆炸酱面，还真挺好吃的。现在各种面都可以做成快餐速食来卖了，味道还不错。是好怀念国内的美食。不对，这边的美食，也怀念，特想去中国城点个菜，吃个广东早茶。听说有的餐馆一直是开着的，不过只提供外卖服务。不过，我现在外卖都一次没点过，还是有一定风险的不是。话说，我室友每天都在点外卖啊，他感染上的话，我怎么着也防不住的吧，那点与不点，有啥区别。愁！</p><h3 id="2020604"><a href="#2020604" class="headerlink" title="2020604"></a>2020604</h3><p>这两天头终于不那么疼了。应该是感冒了，有时还是流一点鼻涕。今天终于可以补一补这些天拉下的日记了。笔记一放下，不知不觉就欠了10多篇。时间过的可真不走心，像吸粉似的一哧溜。</p><p>早上尽管带着耳塞，依然被窗外的车辆嘈杂声吵醒。以前没有隔离的时候，即使早上早已车水马龙，即使没有戴着耳塞，我依然能够睡着。可是现在，隔离期间，没有背景噪声，，什么声音都能突兀出来。楼下哥们停个车，打个电话我，都能听得一清二楚。尤其讨厌那种骑摩托车的，美国这边的摩托车跟国内还不一样，声音巨响无比那种。然后每次来的时候，总是到你附近，油门更一踩，声音变得贼大，就好像特意让你听到似的。”这帮傻逼“。</p><p>晚上吃完饭，跟我姐聊了会，问我连花清瘟是否到了。之前连花清瘟国内一般都是邮不过去的，只能通过特殊途径，就没让她邮。不过幸亏美国亚马逊上也能买到几盒。这几天感冒（应该是吧），正好吃了一板。现在状态应该恢复到之前了。</p><h3 id="20200605"><a href="#20200605" class="headerlink" title="20200605"></a>20200605</h3><p>早上醒来太早了，翻了会书，又睡了下。这一下睡到了10点多。起床就很头疼，下午开完会后，放下手中的东西，准备好好休息下。<br>睡了一下，果然好很多了。</p><p>晚上约了朋友看电影。《天堂电影院》。这部片子还是国内研究生那会看的吧，后面剧情都记不住了，但是是好电影，推荐给了朋友。电影关于主人公回忆自己在西西里岛的幼时和年轻时候的生活，讲述了亲情（与男二号不是父子，甚是父子）与爱情，以及人到中年回来寻找记忆的故事。看完，我和朋友得出结论，”初恋是最刻骨铭心的“。主人公跨越了大半生，还是一直忘不了初恋。里面有一个选择题，是选择离开小镇，走向成功还是选择爱情，很有可能一辈子拘禁在小镇。主人公毋庸置疑是想选择爱情，可是这个选择题他的”父亲“帮他做了。影片里，他也没有恨他，仍然很感激他为他做的一切，但他也后悔自己错过了记有女友地址的纸条。我问朋友，你会怎么选择。他们很快就回答了我，”爱情啊，成功反正我没有，爱情呢，不过也没遇着唉“，”哪样都可以啦！我不挑，只要能有“。<br>哈哈，是啊，我们‘凡人’很少有男一号的电影人生，只是生活而已，没有回溯，只能向前，一步一个脚印，有时精彩，有时平淡，来吧，接着就是。</p><h3 id="20200607"><a href="#20200607" class="headerlink" title="20200607"></a>20200607</h3><p>一周实习任务完成情况很好，导致周末两天有点小浪啊。也是头有点疼，而且一到晚上，仿佛有一根筋蹦在那，按住太阳穴才能缓解下，注意下劳逸结合，不能休息的时候就对着屏幕（实际上的得不到休息）。之前买的游戏机也很少玩了，玩一会头就很疼，奶奶的，买来以为休息换脑用的，休息个毛线了现在。</p><p>下傍晚实在忍不住，出去转了一圈。平时都是天气非常好的时候，会忍不住出去转上一圈，今天却是个阴天。可一旦动了这个念头，再也收不住了。在家里纠结了好几下，还是戴好防护出门。我只在校园里转了转，主要是没什么人。虽然没有明媚的阳光，可天显得还是非常干净。</p><p>听说今天校园里有一个游行集会，呼应这次的黑人运动 ”I can’t breath“。可是我下傍晚过来，却并没有看见，应该是错过了。这是我后来听说的，要是知道有集会，我可能就不出门了。学校这边发了好多封邮件，表示对这次运动的支持，抗议种族主义，甚至组织了多个相关活动。我看到的是线上的，不知道这次线下是不是学校发起的。</p><h3 id="20200608"><a href="#20200608" class="headerlink" title="20200608"></a>20200608</h3><p>看新闻说，美国的中国城区在所有区域是感染率最低的，对比中国城拥挤的人均占地面积，不得不赞。毕竟是华人，国内的耳濡目染，文化习惯等等，使大家的意识还是比外国人要前沿。惭愧的是，当时美国刚发生疫情的时候，就一直没去过中国城了。因为当时中国疫情严重，怕有些从国内回来的人。</p><p>之前一直在中国城一家理发店理发，算是波士顿最便宜的理发店了吧。是两个福建的老移民，在留学生圈很有名，每次去理的时候，都会有留学生在等着。我喜欢找那个阿姨理头发，另一个大爷看起来很专业，但在我忍不住好奇心找他理过一次之后，就没有然后了。即使要再等上一个人，也还是要等阿姨理。<br>我算是那种长相有’阿姨缘‘的人吧，有次跟阿姨闲聊，竟然说我长的”漂亮“，不过也不止听过好几个阿姨这么说了，心好累！一直是很怀疑阿姨的审美，跟现在的小姑娘的审美很不一样。我就从来没听过小姑娘跟我讲你很”帅“，更没有小姑娘跟我表白啥的。</p><p>中午吃饭的时候，出门看见室友剪头发了。好吧，他不跟我一样，自己动手，丰衣足食，他定是去了理发店了。这…理发店刚允许复工，到底是心大，不讲究啊。虽然他经常出门，但口罩还是戴的蛮好的。只是希望那个”万一“不要出现吧。</p><h3 id="20200609"><a href="#20200609" class="headerlink" title="20200609"></a>20200609</h3><p>开会，不谦虚的毛病，无聊，晚上无人的时候看书</p><h3 id="20200610"><a href="#20200610" class="headerlink" title="20200610"></a>20200610</h3><p>看得到里面的万维钢专栏解读《历史正确的一侧》这本书来理解西方，尤其是美国。对照这次的黑人运动，有感。</p><p>西方的思想支柱主要来自两个地方，一个是宗教人性，一个是雅典理性。<br>宗教不需要通过对复杂世界的认知学习，直接给了你一套宗教上帝标准，接近天堂上帝就可以了，而且突出了人的主导能力，这也就可以解释他们接受不了相对论的原因。<br>而雅典体系认为人需要通过学习去认知这个复杂的世界，试图用完全理性的方式去理解世界，这里没有人性的存在，只有理性，个人意义上的自由也是不存在的。</p><p>有此衍生出两大思潮，一个是科学，一个是自由和权利。<br>当然现今世界，没有哪个国家会完完全全继承某一个思想，走的都会结合的道路。作者说尤其是美国的建立，是这两种思想体系的集大成者。然而近些年来，个人主义逐渐有点走向极端。你可能听说过反智运动，就是一帮人不相信科学的人，甚至认为科学也是一种压迫，逼他们去相信，宁愿只相信自己的认知。还有很多人把主观感受看的太重，搞的很多人不敢说话（说出客观事实）。也时常会听到有的老外把”this is fact(这是事实)“老挂在嘴边，因为抬杠的人太多了。</p><p>这次美国的黑人运动，主流大家一边倒的支持黑人。但有用吗？说实话，不会那么有用，因为事实没变。一个国家把人分类，区别，其实就是在激化矛盾。你不能说黑人犯罪多，所以对黑人有偏见，也不能因为怕别人说你种族歧视而对他们有任何优待，而应该就是把他当成是一个犯罪的人对待。把人区别对待，以及各种刻板印象，形成了美国独特的身份政治。  </p><p>总是突出自己的身份标签，没有必要，理性的世界，没有自己一说。为什么说大多数科学家是最没有狭隘的国家主义，民族主义的一帮人。因为这跟需要理性的科学是背道而驰的。<br>有此延伸出科学主义的国家建设理念，但说实话，现阶段除非是一个专制的体制，不然很难实现完全的科学主义，很简单，因为不是所有的人都有相似的科学认知理念。<br>这就突出教育的重要性了，但那是洗脑吗？科学不是洗脑，科学是去认知世界，我们有一个共同的目标是理性的认知这个世界。这感觉是回到了起点–雅典体系的思想。但跟那会还不太一样，那时候，是人刚开始对自然认知有了个掌控感，但随着认知的深入，他们发现掌控不了，于是宗教体系思想‘大行其道’。但未来，科技的发展越来越深入，理性将取代人性成为进化的方向。<br>举个例子，电影里总是放AI人工智能取代人类，我喜欢解读成人类的完全理性化进化。</p><h3 id="20200611"><a href="#20200611" class="headerlink" title="20200611"></a>20200611</h3><p>做了一个噩梦，睡了3个小时不到醒了。当时应该打开笔记把噩梦写下来，反正现在是一点也记不住了，不过那种恶心的感觉依稀还有点。<br>被惊醒之后，有害怕，想到幸亏这套房子里还有室友，之前还嫌弃想一个人住这么大的房子，不然我真要爬起来去打开所有屋的灯了。更有种突如其来孤单，打开手机，打开久未打开的探探，刷一刷，仿佛配对成功就能解除孤单似的。可是，zero配对…（我去，不稀罕），还是真人见面好啊，啥时候能出门见人呐…</p><p>逼自己又继续睡觉，怎么也睡不着。窗外已经见亮，车辆声逐渐多起，带上耳塞，可是脑子的思绪已经停不下来。现在睡觉一般都会带着耳机听故事，不然很容易瞎想睡不着。愤然起床，打开电脑。嘿，头脑还是蛮清醒的的，工作了2个多小时，头才开始有点昏沉，借势赶紧爬上床补了几小时的觉。</p><p>时间过的太快了。不经意到下午了。吃完简单的中午饭，开始刷抖音。刷到一则新闻，一个黑人网红炮轰美国的这次黑人运动”I can’t breathe“，正在把一个罪犯塑造成英雄。太勇敢了姑娘，终于有人敢公开这么说了。大家都心里门清，只不过是大家都不敢说，或不愿意说罢了。美国的身份政治，正在戕害美国本身的自由。大家都拿身份说事，办事，反而不会改变身份。相反，身份政治让刻板印象变得真实，现实。黑人就是有很多criminal（罪犯），白人就是自视高人一等，亚裔就是只会工作，中餐，还有忍气吞声，拉丁裔就是脏活累活…它还可以分的再细很多。之前刷到的一则抖音，亚裔美国姑娘，大胆说：”sick of hearing black life matters, all life matters, not only black”(很讨厌说要把黑人的命当命，而不是所有的人命当命）。大家要争取的不是一个组群的幸福，是所有族群的幸福，尤其是美国这么个多移民种族的国家。</p><h3 id="20200612"><a href="#20200612" class="headerlink" title="20200612"></a>20200612</h3><p>今天中午开完会，就没怎么看书工作了。偷懒了一下。发现了一个喜剧剧集《Space Force(太空军)》。非常搞笑讽刺，由我喜欢的一个喜剧演员演的，之前看过他的另一部喜剧。这部剧是我在抖音上刷到，有人分享里面黑中国的搞笑片段。</p><p>这部剧看的时候不能较真，黑中国和俄罗斯，甚至印度都有。一气看完10集，里面充满了反讽似的自嘲，既黑别人，也黑自己。<br>列举几个片段。</p><p>1<br>印度比美国太空军提前发射了一个装备新型发动机的火箭，他们就怀疑太空军里有间谍，首先是那个俄罗斯人，在之后是作为二把手的华裔科学家，最后甚至一把手的白人gay科学家也遭怀疑。最后发现人印度就是比你快了一步。讽刺美国的自大，自我，甚至种族主义等等问题。</p><p>2<br>美国刚发射了一个卫星，被中国用卫星机械手给分拆了，跟个小孩搞破坏似的，然后就溜了。老美不干了，这里非常搞笑，老大坚持要实验用的太空猴出舱完成修复工作。可想而知，无厘头的指挥，必然导致无厘头的结局，太空猴最后被中国卫星给接收了。这是剧集刚开始没多久，就出现的剧情。美国现在的首位假想敌不是俄罗斯了，而是中国，他们预想中，中国就会被的很强大，甚至在太空中能够对美国搞破坏。美国的老大尊严被摩擦了。</p><p>3<br>即使老大尊严被摩擦了，剧集里的美国也想扮演一个正义者，好像在说”都是你们先来欺负我的“。返回月球，中国先登上了，然后宣称”静海“月球坑是中国的科研领土（想想也不可能，谁叫这部剧是人家拍的呢），不允许其他人进入。美国当然不干，然后进行了很多和平的尝试，联系科学家，联系军方（这里中国被黑的很惨），无果。那你无情，欺负人，别怪我无义了。部分美国军方要求武力威慑，夺取中国月球科研站。男主很纠结，这是要引起世界大战啊，于是联合科学家和宇航员去拆了中国科研站。没想到，中国也是要去拆了他们的科研站，最后两国的科研站就都互相被拆了，第一季终。<br>这里面，他们被描述成尊重科学家的建议，热爱和平，不愿意搞事，黑了中国，也讽刺了美国的自大与愚昧。<br>当今，美国是拥有世界上最好的科学智囊团，然这次疫情却表现的非常糟糕，科学家的建议没有被政府完全采纳。现在的种族问题也更加愈演愈烈。美国的问题需要变化，然后变化对非常成熟的美国体制，以及那些已经作为利益获得者而言，不容易，或许也不需要。</p><p>不管怎么说，这部作为一个喜剧，还是非常搞笑的，而且虽然是黑中国了，但从另一个侧面不正是表现了中国的强大嘛。</p><h3 id="2020613"><a href="#2020613" class="headerlink" title="2020613"></a>2020613</h3><p>杰伦新歌《Mojito》刷屏了啊。竟然有人说旋律抄袭，我是第一个不服，抄袭也是抄的自己的《迷迭香》吧，我莫名的听出了迷迭香的味道…尽管杰伦不复巅峰，但此歌一出，还是吊打那些年轻歌手。现在都说歌曲已经沦为视觉的配角了，大家不怎么听歌，特别是小孩，喜欢看动感的视频，奇奇怪怪的‘儿童说唱’…现在年轻的歌手我是真不知道几个。</p><p>除了几个特别有才的，如毛不易。最近也是经常听他的歌，感觉是那种平凡中不平凡的感觉，没有那么多高音，中国风清缓的调调，还挺适合学习的。或许是老了，对那种动次打次的歌，要求越来越高，一般的入不了耳了。</p><h3 id="20200614"><a href="#20200614" class="headerlink" title="20200614"></a>20200614</h3><p>北京出现反扑的新冠病历，开始又戒备起来。人是防住了，可是食品输入倒是露了。可是我却没怎么担心，相信祖国的措施和实力，短期内应该很快就能被控制住。跟这边美国对比，美国这边大家早已放松了警惕，你看游行一浪接着一浪的。病历也突破了200万人。”自由主义的胜利？“。</p><p>国内的油罐车爆炸，造成了10多个人死亡，爆炸范围很大。有的人可能在打王者荣耀，有的人可能在看剧，有的人可能是只是走在路上，可突然就失去的生命。谁能知道不幸会降临到自己身上呢？<br>告诫自己，时刻准备着，时刻活出自己，去做自己最想做的事情，直到那一刻到来临前，能够跟自己说没什么大遗憾，我认真的活过了。</p><p>下午研究下早餐食谱。那个网红早餐做法，吃了2个多月了吧，是真腻了。需要调调味了。所以查了下还有哪些做早餐的懒人做法。算是有了一种新的选项了吧，可是想着食欲为毛没那么高。<br>还是国内好，想吃包子扫一下，煎饼果子来一套，面条，生煎，肉饼，馄饨，白吉馍，肉夹馍，鸡蛋饼，手抓饼…不行，快流哈喇子了</p><h3 id="20200617"><a href="#20200617" class="headerlink" title="20200617"></a>20200617</h3><p>罪过，一不留神，又溜走了几天。这几天没有记录下来什么有意思的东西。这几天睡眠质量不是很好，不是做噩梦，就是4点多钟就醒，再者是早上路边的嘈杂声。有点给自己找借口的嫌疑啊。睡眠质量不高的代价，就是自控能力和理性能力的下降。每天的规划有点被打乱，很多事情跟不上了节奏。</p><p>我想这里探讨下，到底是身体控制了思想，还是思想掌控了身体。我的浅薄观点是这样的，思想其实是受限于物质的，也受限于身体。但人的身体具有很大的潜力和一定的冗余性，他会赋予你部分强大的能力，让你这部分能力去激发甚至恢复身体其他部分的能力。<br>比如说，你坚持冥想，赋予你一套处理机制–思想（其实是物质的调整组合机制），当身体健康稍微变得越界了，这部分处理机制是可以帮助你处理和预防，或者协助的。所以有时候，给自己正确的暗示，相信它，那么身体的物质会朝着那套相信的机制去运行，因为消耗的能量最小。你要不相信，那就要耗费大量的能量绕着道的去处理，可想而知结果并不会很好。</p><h3 id="20200618"><a href="#20200618" class="headerlink" title="20200618"></a>20200618</h3><p>无聊聊探探</p><h3 id="20200619"><a href="#20200619" class="headerlink" title="20200619"></a>20200619</h3><p>今天周五，美国的疫情依然很严重，但大家已经麻木，生活还要继续，各个州的复工计划也早已经实施。黑人运动搞的如火如荼，看到新闻说，对华人不利的什么ACA5法案在加州参议院劳工委员会通过。什么是ACA5法案，就是社会要”公平“的给与少数族裔及女性提供更多机会，尤其是黑人。方案对于黑人应该是很”公平“，但对于优秀的少数族裔，如”华裔“那是相当不利。大家最反对的理由就是，孩子上学录取不在是看成绩，能力，这时候还要看你的族裔占比。华裔是少数，但其中优秀的比例要比其他少数族裔多很多，如果方案通过，那就会大大削弱华裔优秀的孩子上名牌大学的比例。</p><p>记得类似的法案被提了很多次了吧，不知道之前是不是没通过，这次看这架势要通过了。作为华人，当然不希望这个方案通过，你黑人能力不行，怪我喽。作为黑人，不是我黑人能力不行，是这个社会不给机会，不公平，方案通过，我们就会有更多的话语权，不会被欺压。</p><p>之前说过种族政治是美国的特色政治。方案是通过了，但一点都不公平。这其实还是族裔争夺这有限的资源，但位于上层的人依然没有什么影响。说白了，就是你们这些少数族裔去争抢我们剩下的资源吧。美国的经济在增长，但普通民众的收入几十年却没有增长多少了，那么增长的钱去哪了呢。</p><p>美国的个人主义无疑是胜利的，但个人主义的弊端也越来越多。人作为一个群居动物，谋求群体利益最大化，应该是人类的本能。技术不是很发达的时候，个人主义能起个带头作用；技术发达的时候，个人的力量发挥到了极致，那就需要其他人的力量。无论是哪种，都是在特定时期保证整体的利益一直是最大化的。</p><p>这即是理性化的运作方式，是以得到群体利益最大值的一种运作方式。希望祖国越来越理性，希望世界越来越理性。</p><h3 id="20200621"><a href="#20200621" class="headerlink" title="20200621"></a>20200621</h3><p>父亲节，日全食</p><h3 id="20200622"><a href="#20200622" class="headerlink" title="20200622"></a>20200622</h3><p>今天开始跟一些startup的课程。对于我这种没有商科背景的人来说，还是蛮新鲜的。教你怎么面对创业中面对的一些情况，大佬们讲的都很不错。这是原本线下的课程，因为疫情的缘故只能是线上。线上其实效率更高，大家不用一大早赶到哪里，随便穿着裤衩就能听课，而且世界各地的人也都能够加入进来。</p><p>有意思的是，孵化机构特别看重diversity(多样性)，女性创业者的比重。看到入围这次比赛，女性创业者超过了60%占比，挺厉害的，一直认为女性在很多方面是具有优势的。所谓平等不是说在所有方面都要所谓的“平等”，而是在你该厉害的地方，就有平等的机会让你凸显出来，不管你是男性还是女性。刻意的强调性别，种族，其实很大程度就是一种歧视。</p><p>课上现在印象深刻的有这几点，比如设定清晰短期的目标，快速迭代，打造好自己的创业故事等。也讲了怎么做一个leader，但讲的很散，没有系统。晚上的正好看到万维钢专栏关于leader的专栏文章《九个工作谎言和真相》。大体意思就是人不是管的，设定具体目标和计划实质上是自上而下的管控。要跟下属讲意义，提供信息和帮助，相信他们自己能发挥他们的主动积极性。</p><p>这让我想起来跟导师的科研经历。导师很忙，一个人管理了非常的项目，但还能够游刃有余的切换，很让人佩服。跟他开会的时候，开始和结束的时候，他都会问，有什么我们可以帮助你的。这其实让我也发挥了很多的主观能动性，发现了很多新鲜的见解。我们一起讨论，一起制定下一步的研究路线，实施起来快速很多。只是现在实习，又把刚有眉目的研究方向放下，感觉有点对不住他。</p><h3 id="20200626"><a href="#20200626" class="headerlink" title="20200626"></a>20200626</h3><p>端午节, 正愁没有粽子吃。室友的朋友送过来几个粽子，他顺便给了我2个。 </p><p>粽子是北方的粽子，甜口的。吃的不是粽子，倒是乡愁。  </p><p>我该算是北方的人吧。从小吃甜口的，咸粽子在我小学5年级以前想都没有想过。 </p><p>为什么是5年级，因为那年，从南边搬过来一户人家，给我们家送了几个粽子。然后打开了我的新世界，原来还有用肉包裹的，咸味的粽子存在，而且竟然可以这么好吃，更不是自己臆想的恶心味道。  </p><p>后来就对咸粽子情有独钟了，可惜我们家只做甜粽子，也就是自己大了，会去超市买些来吃。BTW咸蛋黄粽子也是我的最爱。</p><p>端午总会赛龙舟，在美国这边，也是有的。华人很多组织都有一个划船的队伍。每到节日，查尔斯河上的赛龙舟绝对是一道亮丽的风景线。</p><p>可惜，今年是没了。保命年，祝愿大家平平安安度过今年就好。</p><h3 id="20200627"><a href="#20200627" class="headerlink" title="20200627"></a>20200627</h3><p>美国一大学教授公然宣称，如果黑人有亚洲人那么优秀，就不会存在种族歧视了。</p><p>厉害，佩服，这话也敢说。先不说对与不对，在这种时候，这种环境下，说这话明显是要断送自己的职业生涯啊。</p><p>之前就有一个教授反对学生闹事，被学校停职，被facebook入职调查，愤然离开，任职了国内的小米（是吧？还是腾讯）</p><p>现在美国搞政治的人貌似有点多，学生动不动也出去游个行，开个大会。政治正确的秀太多，为了政治而政治。</p><p>也许还时常看到的一些无脑议员，为了博出位，说了很多傻逼的话。况且傻逼话，还是有很多傻逼会听，会信的。</p><p>再来说教授的观点正确。怎么说呢，我只能说是片面的。一个人优秀有否，很大程度上取决于你的成长环境。而黑人的环境不是很好。如果有一条轻松的犯罪道路和一条艰难无比的学习道路，你会怎么选。</p><p>很同情黑人的遭遇，但这样的遭遇更不应该是归于一个种族的问题。想解决问题，也不是通过压榨其他族裔来解决的。根本还在于贫与富的矛盾。</p><p>但改变是太难了，上层的人获取了绝大多数财富，底层的人怎么活。而且美国作为一个移民国家，想要发达就需要靠高素质的移民，那本国的”矮矬穷“就要放弃吗？资本主义的基本法则告诉你，是这样的。 </p><h3 id="20200628"><a href="#20200628" class="headerlink" title="20200628"></a>20200628</h3><p>今天给自己理了理头发，两边推掉，上面不动，哎，看，精神多了。</p><p>现在不出门，时常对头发有两股冲动。一是留长发，因为觉得酷。二是想全部推掉，因为打理实在是太麻烦。</p><p>可惜，留长发失败，忍受不了那个过程，咔咔咔，推掉。还问过我留长头发的老板，他说要忍住那个阶段就好了。</p><p>取了个中间值吧，就推掉让自己最受不了的两边和后面，顶上的就留着。这飞机头造型也火了好多年了吧，不知道以后发型还能怎么搞，就这么点地方，花样貌似还挺多。</p><p>下午下了好大的雨啊，我很喜欢。</p><p>看到有分析说，喜欢下雨天的人，不是内向就是抑郁。</p><p>觉得跟小时候很有关，我的很多美好记忆都跟下雨有关吧。小时候我喜欢玩泥巴，下雨了才能玩。也喜欢呆呆的坐在门口，看大雨如何肆虐，闻雨水冲击而起的泥土味。</p><p>最重要的一点是，每到下雨天，我就不用做作业了。就像农忙的人，下雨天只能呆在屋里无所事事。仿佛想说，”呐，不是我偷懒，是天公不作美“。</p><p>屁话，作业屋里什么时候都能做，跟农忙能一样嘛。不一样，但感觉是一样的。我可以毫无歉疚的停下手里的作业，走到外面，就那么无所事事。</p><p>今天外面的雨下的还蛮大的，正好给大家躁动的心浇一浇水。</p><p>一声惊雷，吓破你们出门风骚的心。在外的人，也急急忙忙朝家里赶。</p><p>天地间，仿佛一切都将停滞了，你要做的就是安安静静的做自己，不要有那么多幺蛾子了。</p><h2 id="202007"><a href="#202007" class="headerlink" title="202007"></a>202007</h2><h3 id="20200727"><a href="#20200727" class="headerlink" title="20200727"></a>20200727</h3><p>父母好多年没见</p><h3 id="20200725"><a href="#20200725" class="headerlink" title="20200725"></a>20200725</h3><p>又解锁了一个菜：黄焖鸡</p><p>这就是街舞！</p><h3 id="20200723"><a href="#20200723" class="headerlink" title="20200723"></a>20200723</h3><p>听说FBI调查博士生</p><h3 id="20200722"><a href="#20200722" class="headerlink" title="20200722"></a>20200722</h3><p>美国的霸权主义zhenxinouxindaowol。  </p><p>早上起来刷手机，看到老爸给我发的消息: 最近美国针对留学生的盘查，骚扰事件频繁发生，大使馆提醒大家注意安全。</p><p>再发刷到一条消息: 美国强制中国驻休斯顿总领事馆3日内关闭走人，原因是美国想在中国武汉有一个外交豁免（武汉美国领馆人员不用进行隔离措施）。</p><p>非常正规也是正确的操作，老美你当成是挑衅你了吗？天呐，这届政府是真心服了。</p><p>想起之前看的那个《太空署》的美剧，那帮高官的无知自大竟然讽刺的真真的。而里面的男主人公为了美国，也为了全人类的共同利益认知，采取措施前都会很克制。而男主人公的认知，看来现任美国政府是达不到。</p><h3 id="20200721"><a href="#20200721" class="headerlink" title="20200721"></a>20200721</h3><p>今天跟朋友出门转了一圈（这是不能给老父亲看到，不然该天天打电话斥责了）。</p><p>戴口罩，保持正常的社交距离。绕着学校绕了一圈，没想到操场上这么多人。网球场地也是全满的。</p><p>朋友说，国内是不可能发生的，不过国外大家都已经非常习惯了。</p><p>又谈起，现在回国也很麻烦了，都要提前做完检测才能回国，问起我哪里可以做检测。我说学校开学后，在校内就有一个检测点。</p><p>不敢想象到时候学生都回来时，像什么样了。本来就不大的校园，1半学生回来可能都显得拥挤。</p><p>检测到一个就可能有一窝了吧，呸呸呸，乌鸦嘴！</p><h3 id="20200719"><a href="#20200719" class="headerlink" title="20200719"></a>20200719</h3><p>不知道是不是在家待久了，时间一点都不经得住过，眼前的世界也看起来模模糊糊的，仿佛烟雾缭绕。上一秒看到的事情，在记忆里却感觉好久远的事情，模糊且旧。</p><p>戴上眼镜，试图使眼前的事物变得清晰，仿佛眼前清晰了，脑子里的也就变得清晰。</p><p>每天都跟自己说要修行冥想，利用在家这段时间，把之前的喧嚣与浮躁给滤掉，可每天还是有点“忙忙碌碌”。</p><p>不仅仅是忙工作，也忙着玩…</p><p>昨天跟一朋友聊天，又聊到她吃饭贼慢的属性。无论别人怎么催，怎么嘲，她总是吃的慢慢悠悠的，一小口，一小口，给边上的我们给急的…</p><p>我问你就不着急吗，不想快速吃完做其他事情吗？她说基本没有过很急的情况。</p><p>想起来很久之前读的一本书，叫慢动作冥想。就是让你的动作慢下来，思维慢下来，关注到当下每分每秒，不做其他胡思乱想。看来我这个朋友虽然不懂得什么是慢动作冥想，但有点先天悟道的意思哈。</p><h3 id="20200718"><a href="#20200718" class="headerlink" title="20200718"></a>20200718</h3><p>中午接了个电话，一个离开波士顿的印度朋友突然顺道造访。</p><p>好长时间没见，挺高兴他能想着我。但同时我又不得不担心，他会不会对疫情不在乎，会不会邀请我去哪吃饭，或者想来家里坐坐…</p><p>不过我是想多了，大老远看见他戴着口罩过来了。没法很热情的打招呼，于是我们隔空击拳。</p><p>隔着一米多远，互相聊了聊近来的情况，并约好下次疫情过后，再来Boston到时候一定好好聚下。</p><p>他算是来美国之后的第一个外国朋友，我们之间格外的客气，我请了他一次，他又回请我一次。</p><p>虽说现在印度跟中国关系又不是很好，但单从我接触的一些印度人来看，还是很友好，客气的。</p><h3 id="20200716"><a href="#20200716" class="headerlink" title="20200716"></a>20200716</h3><p>美在父看来，洪水猛兽般<br>聊华裔教授怎么压榨学生</p><h3 id="20200713"><a href="#20200713" class="headerlink" title="20200713"></a>20200713</h3><p>美国国内也不尽全是一帮说瞎话的‘政治家’。不过说实话，说瞎话，容易出位，有曝光率。</p><p>很多专家也在反省这次疫情，尤其是反省跟中国的关系。美国这次的疫情期间出现的各种问题，再对比中国，不得不让一些专家思考这是为什么。</p><p>肯定第一个想到是中国的体制。基本上没有西方学者会让认为中国一党专政好的，但他们通过中国的发展，又不得不得出一些中国体制的好处。他们应该怎么也想不明白，在几百年前，他们就实验过并放弃的‘集体制’，为什么会在中国有点成功的意思。</p><p>我想他们不敢想，也不敢说，就留待时间来验证吧。</p><p>其中记者问了一个专家的问题很有趣：如果你是个将出生的孩子，你想出生在美国还是中国。专家说，如果你出生在10%以下的底层，那么不用怀疑，肯定是中国。要是美国，大部分会犯罪，过着很不健康的生活，寿命也很短。但如果你出生在10%的上层，那么美国你将活的更舒适。</p><h3 id="20200711"><a href="#20200711" class="headerlink" title="20200711"></a>20200711</h3><p>今天出门邮寄返税表（真是拖到了最后，从3月拖到了7月），近几个月以来的第一次出远门。</p><p>街上人不是很多，大部分人戴着口罩，不过仍然有些不戴口罩。</p><p>那个大转角处，也没有了往日的车水马龙。</p><p>路过波士顿交响乐大厅，虽然早上没人是正常，但在疫情下的衬托下，望着几个月前的音乐海报，还是显得十分萧瑟</p><p>我最喜欢的一个居酒屋，头也没回的直接路过，因为还没开门…</p><h3 id="20200709"><a href="#20200709" class="headerlink" title="20200709"></a>20200709</h3><p>疫情继续失控</p><h3 id="20207008"><a href="#20207008" class="headerlink" title="20207008"></a>20207008</h3><p>今年我们老板新收了一个国内的学生，之前一直跟我联系，原本准备9月份过来，但是因为疫情的缘故，9月份肯定是过不来了。</p><p>今天跟他聊了一下，发现他似乎有点不想来了。虽然我很希望他过来，但却十分理解他想法的转变。</p><p>美国的疫情表现十分糟糕，加上Trump gov. 的操作十分的low，就是朝中国泼脏水，导致疫情期间，中美关系更加恶劣。</p><p>就比如说我的家人，都非常希望我回国，什么留学学历，资历都比不上生命重要，而且美国的表现让他们很失望，这个学历不要也罢。</p><p>美国这次疫情的应对，不仅仅让本国人失望，也让中国国内十分的失望，说好的“美”国呢。可以预想，以后国外留学肯定不会像以前那么‘香’。</p><p>晚上跟老板也稍微聊了下，把一些他的担心跟老板说了一声。老板很痛心的跟我说，现在的情况是疯狂的，竟然迫使一个小孩（新收的学生确实岁数不大）去考虑学习以外的很多的事情（这里指美国签证，对他的研究背景调查等等）。</p><p>我也抱怨了几句川普，他说感同身受：我们应该把精力放在让世界变得更好，尽我们所能，而不是如此这般。</p><p>我说【在这场疫情中，让我们看到了科学家的担当，永远说真话，尽最大可能去拯救生命，并且能够跟科学家合作共赢，这才是榜样】。</p><p>老板回复一个大大的赞</p><h3 id="20200706"><a href="#20200706" class="headerlink" title="20200706"></a>20200706</h3><p>川普政府又作妖，如果你秋季只上网课的话，必须滚回国去上，美国不让你呆。</p><p>政策一出，美国高校一片哗然。很多高校已经宣布了秋季网课计划，这番操作直接让高校不得不把国会给告上了法庭。现有哈佛和MIT联合声明，后有高校不断加入。</p><p>荒唐的一笔是不是，老美的操作真心秀逗。难道都离开，还能减少疫情增长不成，缓解经济压力？</p><p>老美的这番操作着实寒了很多人的心。身边的朋友不断表达了以后会回国的打算，甚至看到朋友圈有拿到在美offer，也决定回国的同学。</p><p>虽然美国梦仍然是大部分国家移民追求的，但对于日益繁华的中国民众来说，美国梦将不再美矣，是一段开拓视野的经历。</p><h3 id="20200704"><a href="#20200704" class="headerlink" title="20200704"></a>20200704</h3><p>今天是美国的独立日。晚上据说有烟花。每年这个时候，都会跟几个朋友去查尔斯河边去观看烟花表演。河两边几乎看得见的草地上都坐着人。</p><p>最近虽然美国的病历增长又出现反弹，但麻州这边还是表现不错，并没有出现其他州的反弹现象。麻州的素质还是可以的。</p><p>不过今年也只能是线上看烟火表演了。对于喜欢聚会，唠嗑的老美，很扫兴的。我敢说，如果有消息放出，允许查尔斯河边观看烟火，那估计依然是人山人海。</p><hr><p>无意间又听到一个黑中国的新闻。一个是自称为共产主义死难者纪念基金会中国研究高级研究员的人（说白了就是西方财团雇来黑中国的一个组织）发表了一个报告，说报告来自新疆当地的政府网站,但现在已经下线了（也就是没有证据的道听胡说）。说中国共产党在该地区”实施种族文化灭绝”的政策，然后列举了手段，说国际必须采取措施惩治中国云云。</p><p>一个国家如果有人搞分裂，搞破坏，你不压制，改变，还任由其发展不成？就像香港问题，不管，还继续让香港走向混乱，继续让西方浑水摸鱼，暗度陈仓不成？当然，反人类，不人道的事情，也是坚决不能做的。有同情心是很好，就怕被利益利用的同情。</p><h3 id="20200703"><a href="#20200703" class="headerlink" title="20200703"></a>20200703</h3><p>不知道是美国待久了，还是自己变老了，逐渐对这个世界变得悲观起来。</p><p>曾几何时，算是一个乐观主义者，总是相信人类的发展一定在未来会越来越好。然而此时此刻，我开始变得忧心忡忡。</p><p>一场世界大疫情，让世界措不及防。首先人的生命在大自然面前，仍然很弱小，人类走过了几十万年的进化，不得不说是奇迹。</p><p>费米悖论是关于外星人存在的假说理论：根据宇宙惊人的年龄和庞大的星体数量，意味着应该有很多地外文明的存在，却仍然找不到一点证据之间的矛盾的一个理论。</p><p>其中一个设想假说是，地球是特殊的。</p><p>这个很有可能的，比如说我们其实是在一个”人“的模拟宇宙里。</p><p>比如说宇宙各种灾难，很难有文明能发展到一个星际文明的高度。</p><p>联想到地球上出现过的灾难，恐龙灭绝最为出名。人类的存亡，或许也是那些不可预见的灾难。</p><p>我喜欢存在即合理这个说法。现在存在的，必定是符合当下合理的，即使是一些”不合理“的事情。可惜，这样的合理，在能够灭绝人类的灾难面前，变得一文不值，对人类来说可没有合理可言。</p><p>灾难的最终一刻来临，是不可预见的，但灾难前的迹象，或许是可寻的。科学家做了很多这些相关的研究，比如说病毒的爆发，国际也建立了世卫组织，CDC来防止这些类似的事件发生。但仍然做的不够，面对大威胁，仍显无力。</p><p>人类的忧患意识也仍然很薄弱，温水煮青蛙，谁他妈能确定真会发生。全球气候变暖，北极冰川融化，亚马逊大火，澳大利亚大火，Ebola，COVID-19病毒，各地地震，厄尔尼诺，…</p><p>在这些大灾大难面前，悲观的情绪正是来自人类自己的漠视和不作为。人类有太多自己的发展问题需要解决，哪管得了所谓的环境问题，经济问题才是关键。各种既得利益集团，更是很难说去牺牲自己的利益去服务其他人。</p><p>现在的世界很奇怪，很多动荡根源都是利益集团的利益驱使。民主理想是很丰满，但太容易被利用。当权者如果是一个关注自身的利益者，民主就很容易被他们左右。</p><p>最近看到报道，长寿不再是科幻，很有可能在接下来的几十年里变成现实。可我们准备好了吗？面对这些利益的诱惑，世界的发展究竟会是什么样的呢。世界进一步的两级分化？我更相信，不会。但这个过程将是非常痛苦的吧。</p><p>我甚至悲观的认为，会发生战争。被利益裹挟的民主，被污蔑化的体制，上演利益的终极博弈，最终升级到战争。</p><h3 id="20200702"><a href="#20200702" class="headerlink" title="20200702"></a>20200702</h3><p>老妈又发来一个视频，一个穿着白大褂的医生，给大家普及熬夜的危害，几点到几点不睡，伤哪个器官。</p><p>我随手去抖音找了一个辟谣视频，给老妈转过去。老妈一会回复，你怎么知道你这个视频不是谣言。</p><p>呦，老妈竟然还有了批判性思考（抬杠）。不过我就是希望她审示下自己看到的是否就是正确的，目的算达到了吧。</p><p>现在网络是越来越快，信息传递也越来越方便，大家接触信息的密度更越拉越大。面对这么多的信息，该如何分辨一些谣言信息就显得很关键。</p><p>不管是谁都可以发表一些言论，很多言论并不会对你造成影响，比如这个早睡视频。有可能就是一个医生，在他的认知里这个就是对的。你听完之后，去执行也没啥问题。但还有一些谣言就有可能会伤害到你，比如现在抖音上比较多的直播忽悠带货，声称对你的健康或者啥啥啥有帮助的。</p><p>怎么说，你要像我老妈那样，持着一个怀疑态度对待你看到的所有视频，即使是谁谁权威说的。另一方面，多看些相关视频或者文字，现在网络如此发达，利用好搜索，总会找到你想佐证或怀疑的。再者，给自己的相信打分，权威科学家的言语，一般能打个9分（10分制），依次往下，鉴别别有用心的人。</p><h3 id="20200701"><a href="#20200701" class="headerlink" title="20200701"></a>20200701</h3><p>说来滑稽的事情，美国的疫情越发的严重，但国内还在为了是否戴口罩而争论不休。</p><p>先有川普坚决不戴口罩，树立了一个错误典范，后有各种所谓捍卫”人权“的群体个人大放厥词。</p><p>我最喜欢美国钟南山，福奇医生。一位科学捍卫者，游离于美国的政客与傻逼之间，仍然能够坚持该坚持的。面对傻逼，苦笑下，耸耸肩，多少无奈尽在不言中。</p>]]></content>
      
      
      <categories>
          
          <category> 大话东游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 每日一吐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine learning with python(3)</title>
      <link href="/2018/06/04/machine-learning-with-python-3/"/>
      <url>/2018/06/04/machine-learning-with-python-3/</url>
      
        <content type="html"><![CDATA[<p>系列1主要讲了python环境，软件安装，机器学习比较重要的库，还有python, numpy, pandas, mattplotlib的crash course. <a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-1/">machine learning with python 1</a></p><p>系列2主要讲了数据来了，怎么理解数据。主要是一些统计手段，可视化。<a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-2/">machine learning with python 2</a></p><p>本系列3，主要讲prepare data for machine leanring. 数据塞进机器学习算法前，对data 还需要处理什么。</p><h2 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h2><h3 id="scale优点"><a href="#scale优点" class="headerlink" title="scale优点"></a>scale优点</h3><p>关于data为什么要scale(归一化）。有以下几个原因：</p><ol><li>归一化可以加快梯度下降求最优解的速度；</li><li>归一化有可能提高精度（如KNN等算法）</li></ol><h3 id="why"><a href="#why" class="headerlink" title="why?"></a>why?</h3><ul><li>对于说可以加快速度，可以看这Boston 房屋价格变动的这个例子；（在吴恩达的课程中有很经典的案例，需要达人整理的吴恩达笔记可以留邮箱给我，发给你）<br>下图举了一个boston房价的例子，如果刻度不同，得到的loss function的等高线图，<br><img src="/images/machine-learning-with-python-3-1.png" alt="Boston price cost function"><br>我们看到左边的等高线是狭长的，右边是偏圆形的。在左图中，一个scale是0<del>2000，一个scale是1</del>5。这对于Gredient descent下降速度是很有影响的。</li><li>提高精度。是因为一些算法需要计算距离的时候，如果某一个特征值很大，那么算法就会取决于这个特征值，影响算法精度。scale可以避免这种异常值的出现。</li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ol><li>概率模型不需要归一化，因为模型不关心变量的取值，只关心变量的分布和变量的条件概率；</li><li>SVM、线性回归之类的最优化问题需要归一化，是否归一化主要在于是否关心变量取值；</li><li>神经网络需要标准化处理，一般变量的取值在-1到1之间，这样做是为了弱化某些变量的值较大而对模型产生影响。</li><li>在K近邻算法中，如果不对解释变量进行标准化，那么具有小数量级的解释变量的影响就会微乎其微。</li></ol><ul><li>From:<a href="https://blog.csdn.net/zenghaitao0128/article/details/78361038" target="_blank" rel="noopener">https://blog.csdn.net/zenghaitao0128/article/details/78361038</a></li></ul><h3 id="几种scale的方法"><a href="#几种scale的方法" class="headerlink" title="几种scale的方法"></a>几种scale的方法</h3><ul><li><p>MinMaxScaler<br>$$[x^{‘}= \frac{x-min(x)}{max(x)-min(x)}]$$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; MinMaxScaler(feature_range&#x3D;(0, 1))</span><br><span class="line">rescaledX &#x3D; scaler.fit_transform(X)</span><br><span class="line"># summarize transformed, 输出位数小数点后3位</span><br><span class="line">data set_printoptions(precision&#x3D;3)</span><br></pre></td></tr></table></figure></li><li><p>StandardScaler<br>$$[x^{‘}= \frac{x-u}{\sigma }]$$<br>$[u]$是平均值，$[\sigma]$是标准差，经过处理后，数据符合标准正态分布。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; StandardScaler().fit(X)</span><br><span class="line">rescaledX &#x3D; scaler.transform(X)</span><br></pre></td></tr></table></figure></li><li><p>Normalizer（范化）<br>主要有两种范化 L1, L2， sklearn中默认是L2<br>$$x^{‘}=\frac{x}{\left | x \right |}$$<br>说一下做范化的好处：适合在稀疏的数据中应用，比如1000维，只有几维是是非0的。这时候可以应用范化。 这里分母是||x||=square(x1^2+x2^2+…)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; Normalizer().fit(X)</span><br><span class="line">normalizedX &#x3D; scaler.transform(X)</span><br></pre></td></tr></table></figure></li><li><p>Binarize<br>有一个阈值（threshold)，如果有值超过这个值赋1，小于该值为0.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">binarizer &#x3D; Binarizer(threshold&#x3D;0.0).fit(X)</span><br><span class="line">binaryX &#x3D; binarizer.transform(X)</span><br></pre></td></tr></table></figure></li></ul><p>Mark下: 这几种scale方法，前两个比较好理解，但范化不是很理解，为什么能应用到稀疏的数据中，怎么应用，有待进一步查资料。</p><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>这个地方<strong>mark</strong>一下 ，数学公式比较多。短期内还没法一下全部搞懂。我先把大概给弄上来。</p><blockquote><p>数据决定了机器学习的上限，而算法只是尽可能逼近这个上限</p></blockquote><p>这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括<strong>特征构建、特征提取、特征选择</strong>三个部分。</p><p>特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p><p>reference: <a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a></p><p>这张图特别好，来自：<a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibao/p/6252280.html</a></p><p><img src="/images/machine-learning-with-python-3-2.png" alt="一图概览特征工程"></p><p>补充：关于维度灾难的补充：<a href="https://www.jianshu.com/p/d7aec8b41356" target="_blank" rel="noopener">https://www.jianshu.com/p/d7aec8b41356</a><br>有时候维度太多，会造成维度灾难，具体看链接。所以进行降维很有必要。</p><h3 id="特征构建"><a href="#特征构建" class="headerlink" title="特征构建"></a>特征构建</h3><p>特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。除此之外，属性分割和结合是特征构建时常使用的方法。结构性的表格数据，可以尝试组合二个、三个不同的属性构造新的特征，如果存在时间相关属性，可以划出不同的时间窗口，得到同一属性在不同时间下的特征值，也可以把一个属性分解或切分，例如将数据中的日期字段按照季度和周期后者一天的上午、下午和晚上去构建特征。总之特征构建是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。From:<a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a></p><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p><ul><li>特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li><li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li></ul><p>根据特征选择的形式又可以将特征选择方法分为3种：</p><ol><li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li><li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li><li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li></ol><p>我们使用sklearn中的feature_selection库来进行特征选择</p><h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><h5 id="皮尔森相关系数法-Pearson-Correlation"><a href="#皮尔森相关系数法-Pearson-Correlation" class="headerlink" title="皮尔森相关系数法(Pearson Correlation)"></a>皮尔森相关系数法(Pearson Correlation)</h5><h5 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h5><h5 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h5><h5 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h5><h4 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h4><h5 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h5><h4 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h4><h5 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h5><h5 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h5><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><h4 id="主成分分析法（PCA）"><a href="#主成分分析法（PCA）" class="headerlink" title="主成分分析法（PCA）"></a>主成分分析法（PCA）</h4><p>分析PCA很好的文章：<a href="https://www.cnblogs.com/hadoop2015/p/7419087.html" target="_blank" rel="noopener">https://www.cnblogs.com/hadoop2015/p/7419087.html</a><br>PCA是从特征的角度协方差角度： 求出协方差矩阵的特征值和特征向量，然后将特征向量按特征值的大小排序取出前K行组成矩阵P（这个P就是我们对角化协方差矩阵的时所使用的P, 具体的可以看看矩阵对角化的过程）， 这个P就是一组正交变化基， 然后将原始的矩阵X，左乘P，也就是将X变换到P组成的正交基中，然后PX＝Y就是降维后的矩阵。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; array[:,0:8]</span><br><span class="line">Y &#x3D; array[:,8]</span><br><span class="line"># feature extraction</span><br><span class="line">pca &#x3D; PCA(n_components&#x3D;3)</span><br><span class="line">fit &#x3D; pca.fit(X)</span><br><span class="line">x_new &#x3D; fit.transform(x)</span><br></pre></td></tr></table></figure><h4 id="线性判别分析法（LDA）"><a href="#线性判别分析法（LDA）" class="headerlink" title="线性判别分析法（LDA）"></a>线性判别分析法（LDA）</h4><p>LDA则是在已知样本的类标注， 希望投影到新的基后使得不同的类别之间的数据点的距离更大，同一类别的数据点更紧凑。</p><blockquote><p>特征工程更多参考：<br><a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibao/p/6252280.html</a><br><a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">https://www.cnblogs.com/wxquare/p/5484636.html</a><br><a href="http://dataunion.org/14072.html" target="_blank" rel="noopener">http://dataunion.org/14072.html</a><br><a href="https://blog.csdn.net/Dream_angel_Z/article/details/49388733" target="_blank" rel="noopener">https://blog.csdn.net/Dream_angel_Z/article/details/49388733</a></p></blockquote><h2 id="数据集的重新划分"><a href="#数据集的重新划分" class="headerlink" title="数据集的重新划分"></a>数据集的重新划分</h2><h3 id="KFold"><a href="#KFold" class="headerlink" title="KFold"></a>KFold</h3><p><a href="https://blog.csdn.net/FontThrone/article/details/79220127" target="_blank" rel="noopener">Sklearn中的CV与KFold详解</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#Stratified k-fold:实现了分层交叉切分*</span><br><span class="line">from sklearn.model_selection import StratifiedKFold</span><br><span class="line">X &#x3D; np.array([[1, 2, 3, 4],</span><br><span class="line">              [11, 12, 13, 14],</span><br><span class="line">              [21, 22, 23, 24],</span><br><span class="line">              [31, 32, 33, 34],</span><br><span class="line">              [41, 42, 43, 44],</span><br><span class="line">              [51, 52, 53, 54],</span><br><span class="line">              [61, 62, 63, 64],</span><br><span class="line">              [71, 72, 73, 74]])</span><br><span class="line"></span><br><span class="line">y &#x3D; np.array([1, 1, 0, 0, 1, 1, 0, 0])</span><br><span class="line"></span><br><span class="line">stratified_folder &#x3D; StratifiedKFold(n_splits&#x3D;4, random_state&#x3D;0, shuffle&#x3D;False)</span><br><span class="line">for train_index, test_index in stratified_folder.split(X, y):</span><br><span class="line">    print(&quot;Stratified Train Index:&quot;, train_index)</span><br><span class="line">    print(&quot;Stratified Test Index:&quot;, test_index)</span><br><span class="line">    print(&quot;Stratified y_train:&quot;, y[train_index])</span><br><span class="line">    print(&quot;Stratified y_test:&quot;, y[test_index],&#39;\n&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#结果：</span><br><span class="line">Stratified Train Index: [1 3 4 5 6 7]</span><br><span class="line">Stratified Test Index: [0 2]</span><br><span class="line">Stratified y_train: [1 0 1 1 0 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 2 4 5 6 7]</span><br><span class="line">Stratified Test Index: [1 3]</span><br><span class="line">Stratified y_train: [1 0 1 1 0 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 1 2 3 5 7]</span><br><span class="line">Stratified Test Index: [4 6]</span><br><span class="line">Stratified y_train: [1 1 0 0 1 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 1 2 3 4 6]</span><br><span class="line">Stratified Test Index: [5 7]</span><br><span class="line">Stratified y_train: [1 1 0 0 1 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning(2)</title>
      <link href="/2018/05/29/Deep-Learning-2/"/>
      <url>/2018/05/29/Deep-Learning-2/</url>
      
        <content type="html"><![CDATA[<p>First Introduces the Keras, and then give a summary of different architecture of Deep learning, such as CNN.</p><h2 id="Keras"><a href="#Keras" class="headerlink" title="Keras"></a>Keras</h2><p>why use keras? easy, support tensorflow, theano, CNTK.</p><p>How to build a Neural Network in Keras? There are some core concepts need know.</p><h3 id="Sequential-Model"><a href="#Sequential-Model" class="headerlink" title="Sequential Model"></a>Sequential Model</h3><p>The <a href="https://keras.io/models/sequential/" target="_blank" rel="noopener">keras.models.Sequential</a> class is a wrapper for the neural network model that treats the network as a sequence of layers. It implements the Keras model interface with common methods like compile(), fit(), and evaluate() that are used to train and run the model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">#Create the Sequential model</span><br><span class="line">model &#x3D; Sequential()</span><br></pre></td></tr></table></figure><h3 id="Layers"><a href="#Layers" class="headerlink" title="Layers"></a>Layers</h3><p>The Keras Layer class provides a common interface for a variety of standard neural network layers. There are fully connected layers, max pool layers, activation layers, and more. You can add a layer to a model using the model’s add() method. For example, a simple model with a single hidden layer might look like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers.core import Dense, Activation</span><br><span class="line"></span><br><span class="line"># X has shape (num_rows, num_cols), where the training data are stored</span><br><span class="line"># as row vectors</span><br><span class="line">X &#x3D; np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line"># y must have an output vector for each input vector</span><br><span class="line">y &#x3D; np.array([[0], [0], [0], [1]], dtype&#x3D;np.float32)</span><br><span class="line"></span><br><span class="line"># Create the Sequential model</span><br><span class="line">model &#x3D; Sequential()</span><br><span class="line"></span><br><span class="line"># 1st Layer - Add an input layer of 32 nodes with the same input shape as</span><br><span class="line"># the training samples in X</span><br><span class="line">model.add(Dense(32, input_dim&#x3D;X.shape[1]))</span><br><span class="line"></span><br><span class="line"># Add a softmax activation layer</span><br><span class="line">model.add(Activation(&#39;softmax&#39;))</span><br><span class="line"></span><br><span class="line"># 2nd Layer - Add a fully connected output layer</span><br><span class="line">model.add(Dense(1))</span><br><span class="line"></span><br><span class="line"># Add a sigmoid activation layer</span><br><span class="line">model.add(Activation(&#39;sigmoid&#39;))</span><br></pre></td></tr></table></figure><p>More layers decription we can check <a href="https://keras.io/layers/core/" target="_blank" rel="noopener">Keras Layers</a></p><h4 id="Dense"><a href="#Dense" class="headerlink" title="Dense"></a>Dense</h4><p>A dense layer represents a matrix vector multiplication, is used to change the dimensions of your vector.<br>Further Reading: <a href="https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer" target="_blank" rel="noopener">https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer</a></p><h4 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h4><p>Commonly, used in CNN. It transforms your tridmenesional tensor into a monodimensional tensor.<br>Furthere Reading: <a href="https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network" target="_blank" rel="noopener">https://www.quora.com/What-is-the-meaning-of-flattening-step-in-a-convolutional-neural-network</a></p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>Dropout is a regularization technique, which aims to reduce the complexity of the model with the goal to prevent overfitting.<br>Further Reading: <a href="https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer" target="_blank" rel="noopener">https://www.quora.com/In-Keras-what-is-a-dense-and-a-dropout-layer</a></p><h3 id="compile-model"><a href="#compile-model" class="headerlink" title="compile model"></a>compile model</h3><p>Once we have our model built, we need to compile it before it can be run. Compiling the Keras model calls the backend (tensorflow, theano, etc.) and binds the optimizer, loss function, and other parameters required before the model can be run on any input data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss&#x3D;&quot;categorical_crossentropy&quot;, optimizer&#x3D;&quot;adam&quot;, metrics &#x3D; [&quot;accuracy&quot;])</span><br></pre></td></tr></table></figure><p>We can see the resulting model architecture with the following command:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><h3 id="Fit-Model"><a href="#Fit-Model" class="headerlink" title="Fit Model"></a>Fit Model</h3><p>We have deﬁned our model and compiled it ready for eﬃcient computation. Now it is time to execute the model on some data. We can train or ﬁt our model on our loaded data by calling the fit() function on the model.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Fit the model</span><br><span class="line">model.fit(X, Y, epochs&#x3D;150, batch_size&#x3D;10)</span><br></pre></td></tr></table></figure><h3 id="Evaluate-model"><a href="#Evaluate-model" class="headerlink" title="Evaluate model"></a>Evaluate model</h3><p>We have trained our neural network on the entire dataset and we can evaluate the performance of the network on the same dataset. We use the evaluate function. It returns the loss value &amp; metrics values for the model in test mode.<br>The attribute <em>model.metrics_names</em> will give you the display labels for the scalar outputs.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># evaluate the model</span><br><span class="line">scores &#x3D; model.evaluate(X, Y)</span><br><span class="line">print(&quot;\n%s: %.2f%%&quot; % (model.metrics_names[1], scores[1]*100))</span><br></pre></td></tr></table></figure><h3 id="Predict-Model"><a href="#Predict-Model" class="headerlink" title="Predict Model"></a>Predict Model</h3><p><a href="https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/" target="_blank" rel="noopener">How to Make Predictions with Keras</a><br>This tutorial is great for different kind of prediction, no matter class predict, or regression predict.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict(self, x, batch_size&#x3D;None, verbose&#x3D;0, steps&#x3D;None)</span><br></pre></td></tr></table></figure><p>Generates output predictions for the input samples.</p><p>Further Reading: <a href="https://keras.io/models/sequential/" target="_blank" rel="noopener">Keras Documentation</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_classes(x)</span><br></pre></td></tr></table></figure><h2 id="MLP-Multi-Layer-Perceptron"><a href="#MLP-Multi-Layer-Perceptron" class="headerlink" title="MLP(Multi Layer Perceptron)"></a>MLP(Multi Layer Perceptron)</h2><p>The MLP is full connected layers.<br>Todo</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>Todo: descirption</p><h3 id="CNN-in-keras"><a href="#CNN-in-keras" class="headerlink" title="CNN in keras"></a>CNN in keras</h3><p>There are three types of layers in a Convolutional Neural Network:</p><ol><li><p>Convolutional Layers.</p><ul><li>Padding<img src="/images/deeplearning_padding.jpg" width = 70% height = 70% div align=center />[Deep learning.ai--Padding](https://www.coursera.org/lecture/convolutional-neural-networks/padding-o7CWi)</li></ul></li></ol><ol start="2"><li><p>Pooling Layers.</p><iframe width="700" height="380" src="https://www.youtube.com/embed/OkkIZNs7Cyc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></li><li><p>Fully-Connected Layers.</p></li></ol><p>output_size =1+ (input_size+2*padding-kernel_size)/stride</p><p>Further Reading: <a href="https://keras.io/layers/convolutional/#conv2d" target="_blank" rel="noopener">Keras Documentation</a></p><h3 id="1D-2D-3D-CNN-difference"><a href="#1D-2D-3D-CNN-difference" class="headerlink" title="1D,2D,3D CNN difference"></a>1D,2D,3D CNN difference</h3><p>Further Reading: <a href="https://www.cnblogs.com/szxspark/p/8445406.html" target="_blank" rel="noopener">Chinese version</a>,<br><a href="https://stackoverflow.com/questions/42883547/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn" target="_blank" rel="noopener">English Version</a></p><h2 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h2><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><blockquote><p>Announcement: Most of the content I summaried is from Keras documentation, the blog <a href="https://machinelearningmastery.com/deep-learning-with-python/" target="_blank" rel="noopener">Deep learning</a> and Udacity machine learning course.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning(1)</title>
      <link href="/2018/05/27/Deep-Learning-1/"/>
      <url>/2018/05/27/Deep-Learning-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>neural networks are a class of machine learning algorithms to model complex patterns in datasets using multiple hidden layers and non-linear activation functions.</p><p>It contains an input, passes it through multiple layers of hidden neurons(deep learning contains much more layers), and outputs a prediction representing the combined input of all the neurons.</p><p>![1](/images/neural network1.png)</p><p>Neural networks are trained iteratively using optimization techniques like <strong>gradient descent</strong>. After every training, an error metric is calculated based on the difference between prediction and target.</p><p>the derivatives of this error metric are calculated and propagated back through the network using a technique called <strong>backpropagation</strong>. Each neuron’s weights are the adjusted relative to how much they contributed to the total error. This process is repeated iteratively until the network error drops below an acceptable threshold.</p><h3 id="Perceptrons"><a href="#Perceptrons" class="headerlink" title="Perceptrons"></a>Perceptrons</h3><p>A Perceptron takes a group of weighted inputs, if the date points is in right side, then go through the activation function, output the classification.</p><p>![4](/images/neural network4.png)</p><p>![5](/images/neural network5.png)</p><h3 id="Neural-network-and-Linear-regression"><a href="#Neural-network-and-Linear-regression" class="headerlink" title="Neural network and Linear regression"></a>Neural network and Linear regression</h3><p>It is very simple to classify the data in the figure 2 below. we just need to find the line that can best classify them. Maybe, if the data points are more chaos, we adjust need more lines to help classify. However, this is not the difference between NN and LR. NN can directly classify the Non-linear regions, how we do this, we use the activation function. Discuss below.<br>![2](/images/neural network2.png)<br>![3](/images/neural network3.png)</p><h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><p>What is activation functions?<br>Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer. Popular activation functions include <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" target="_blank" rel="noopener">relu</a> and <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" target="_blank" rel="noopener">sigmoid</a>.<br>Sigmoid almost give up by most deep learning architecture. relu is very popular now.<br><a href="">softmax</a> is very important when output the multiple classes.</p><p>Why we use activation function?</p><ul><li><p><strong>Non-linear</strong> as we say above, for non-linear dataset(e.g. x^2, sin, log), if we use multiple lines to classify, it is linear regression, not neural network. Activation functions model the dataset relationships that we need a non-linear prediction equation.</p></li><li><p><strong>Continuously differentiable</strong> – do you remember how we adjust the weights of the networks we talked above. We use the gradient descent, gradient descent need the output to have a nice slope so we can compute error derivatives with respect to weights. if the output is discrete values, we can’t proceed.</p></li><li><p><strong>Fixed range</strong> – activation function typically squash the input data into a narrow range that makes training the model more stable and efficient.</p></li></ul><p>Why we don’t use sigmoid function nowadays? That’s because after multiple layers, the gradient is vanishing. We will never get the lowest error. So the relu function and its <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu" target="_blank" rel="noopener">variants</a> become popular.</p><p>reference: <a href="https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f" target="_blank" rel="noopener">1</a>, <a href="http://www.sohu.com/a/145367458_468740" target="_blank" rel="noopener">2</a>, <a href="http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#activation-functions" target="_blank" rel="noopener">3</a></p><p>Found great resources: <a href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" target="_blank" rel="noopener">Choosing the right Activation Function</a></p><p><em>Add 12/21/2018</em></p><p>sigmoid function<br>$$ $$</p><p>tanh function</p><p>relu function</p><p>when we want to the dW = dJ/dW</p><h3 id="Error-function-loss-function"><a href="#Error-function-loss-function" class="headerlink" title="Error function(loss function)"></a>Error function(loss function)</h3><p>A loss function, or cost function, is a wrapper around our model’s predict function that tells us “how good” the model is at making predictions for a given set of parameters. The loss function has its own curve and its own derivatives. The slope of this curve tells us how to change our parameters to make the model more accurate! We use the model to make predictions. We use the cost function to update our parameters. Our cost function can take a variety of forms as there are many different cost functions available. Popular loss functions include: MSE (L2) and Cross-entropy Loss. <a href="http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#loss-functions" target="_blank" rel="noopener">from here</a></p><h4 id="Cross-entropy"><a href="#Cross-entropy" class="headerlink" title="Cross-entropy"></a>Cross-entropy</h4><p>First, we need take a look at maximum likelihood. 4 points in the figure. Take two classify lines as an example. Multiply each data point’s probability, and find which lines can get the maximum likelihood.<br>![6](/images/neural network6.png)  </p><p>However, the probability is so small, if we have lots of data, the probability will be near 0. So how can we change it to make more easy to distinguish. we will use the log function.   $-log(p)$<br>For binary classification, the cross-entropy can be calculated as:<br>$$[-(ylog(p)+(1-y)log(1-p))]$$<br>For multiple classification, we calculate a separate loss for each class label per observation and sum the result<br>$$-sum_{i=1}^{n} \sum_{j=1}^{m}y_{ij}ln(p_{ij})$$</p><p>Last problem, how we maximize the probabilities, which means minimize the cross entropy.<br>In order to minimize the error function, we need to take some derivatives. $[\frac{\partial E}{\partial w_j}]$, $[\frac{\partial E}{\partial b}]$</p><p><strong>Gradient Descent Step</strong> Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:<br>$${w_j}’\leftarrow w_j - a\left [ \frac{\partial E}{\partial w_j}\right ]$$<br>similarly, updates the bias:<br>$${b}’\leftarrow b - a\left [ \frac{\partial E}{\partial b}\right ]$$<br>a is the learning rate, is a constant.</p><h3 id="Nerual-Network-Architecture"><a href="#Nerual-Network-Architecture" class="headerlink" title="Nerual Network Architecture"></a>Nerual Network Architecture</h3><p>How we combine several perceptrons into a third, more complicated one?<br>As figure 3, two lines help to classify the dataset. Simply as this and add the function of activation to change this linear to nonlinear classification.<br>![7](/images/neural network7.png)</p><p>More detail:   </p><iframe width="854" height="480" src="https://www.youtube.com/embed/Boy3zHVrWB4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><p>For deep learning, it contains more layers and more nodes than we discuss above.<br>![8](/images/neural network8.png)![9](/images/neural network9.png)</p><h3 id="Feedforward-and-Backpropagation"><a href="#Feedforward-and-Backpropagation" class="headerlink" title="Feedforward and Backpropagation"></a>Feedforward and Backpropagation</h3><h3 id="Train-Optimization"><a href="#Train-Optimization" class="headerlink" title="Train Optimization"></a>Train Optimization</h3><p>Why we use Optimization?<br>there are many things that can fail when train out model. For example, our architecture can be poorly chosen, our data can be noisy and our model could maybe taking years to run.</p><h4 id="Epoch-and-Batch"><a href="#Epoch-and-Batch" class="headerlink" title="Epoch and Batch"></a>Epoch and Batch</h4><p>One epoch means all data go one iterations, batch means divide the dataset into several parts.<br>For example, if I have 1000 data points, the batach_size is 100, then I need 10 times iterations to accomplish 1 times epoch.<br>Why we will have batch size, because train all dataset one time maybe time consuming and lay burden on computer. But it not means increasing batch_size is a good idea. Choose a appropriate paraments is a very important part in deep learning.<br>The same with choose epoch parament.</p><h4 id="Overfitting-and-Underfitting"><a href="#Overfitting-and-Underfitting" class="headerlink" title="Overfitting and Underfitting"></a>Overfitting and Underfitting</h4><p>overfitting is you train too much, gain high variance. Underfitting verse is you train less, gain high bias.<br>![10](/images/neural network10.png)</p><p>Overfitting and Underfitting is a tradeoff, we can draw the error by model complexity graph.<br>![11](/images/neural network11.png)<br>In this graph, we should choose appropriate number of epochs, in case our test error decrease at first then go up again.</p><h4 id="Earlystoppoing"><a href="#Earlystoppoing" class="headerlink" title="Earlystoppoing"></a>Earlystoppoing</h4><p>We can see from last figure, there is a lowest point of val_loss, we should stop at that point. You can see the documentation in <a href="https://keras.io/callback/" target="_blank" rel="noopener">keras document</a>, they use val_loss as monitor to see if the loss is increasing or decreasing. Patience is the number of epochs with no improvement after which training will be stopped.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#in keras</span><br><span class="line">earlyStopping&#x3D;keras.callbacks.EarlyStopping(monitor&#x3D;&#39;val_loss&#39;, patience&#x3D;60, verbose&#x3D;0, mode&#x3D;&#39;auto&#39;)</span><br></pre></td></tr></table></figure><h4 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h4><p>from the figure 17, we see that if the step keep the same, it’s hard to get the lowest point. So, he rule of thumb, if you model is not working, decrease the learning rate. The rule: if steep, take long steps; if plain, small step.<br>![17](/images/neural network17.png)</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>Think of the question?  split the two points.<br>![12](/images/neural network12.png)<br>As we talked above, the probability of the points will say that solution 2 will have smaller error.<br>![13](/images/neural network13.png)<br>However, the smaller error is not good for gradient descent(too sure), just as BertrAIND Russell said:</p><blockquote><p>The whole problem with artificial intelligence is that bad models are so certain of themselves, and good models so full of doubts.</p></blockquote><p>![14](/images/neural network14.png)</p><p><em>Large coefficients will led to overfitting, so how we deal with it?</em> We will <strong>punish the big coefficients</strong>. This method is called <strong>Regularization</strong>.<br>There are two ways: L1, L2.<br>L1 tends to end up with sparse vectors. That means small weights will tend to go to zero. So if we want to reduce the number of weights and end up with a small set. L1 is also good for feature selection, when there are hundreds of features, L1 can help us select which ones are important.<br>L2 on the other hand, it tries to maintain all the weights homogeneously small. This one normally better for training models.<br>![15](/images/neural network15.png)</p><h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>This is method that in the process of training, no all the nodes are joined at one time.<br>This is a method can prevent overfitting.</p><h4 id="Local-Minima-Momentum"><a href="#Local-Minima-Momentum" class="headerlink" title="Local Minima(Momentum)"></a>Local Minima(Momentum)</h4><p>When we do gradient descent, it sometimes will stuck in local minima. How to solve this problem.</p><p>One is to randomly start. It starts from a few different random places and do gradient descend form all of them. This will increse the probability that we will get to the minimum, at least good local minimum.</p><p>Another is called momentum, just as shown below, the step happened long before will matter less than the ones that happened recently, it will gets us over the hump.<br>![16](/images/neural network16.png)</p><p>There are lots of optimizers, it is very important to help us find the smallest error and get a good model.<br>Check this <a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">blog</a>, it introduces lots of optimizers.<br>And the optimizers in <a href="https://keras.io/optimizers/" target="_blank" rel="noopener">keras</a>.</p><p>To be continue…</p><blockquote><p>Announcment: Most of the content I summary are from Udacity NanoDegree Class–Machine Learning and this cheatsheet website,<a href="http://ml-cheatsheet.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Machine Learning Cheatsheet</a>.</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine learning with python(2)</title>
      <link href="/2018/05/18/machine-learning-with-python-2/"/>
      <url>/2018/05/18/machine-learning-with-python-2/</url>
      
        <content type="html"><![CDATA[<p>系列整理第2篇，主要包括load data的三种方式，几种理解数据的统计手段，可视化数据。</p><h2 id="load-data"><a href="#load-data" class="headerlink" title="load data"></a>load data</h2><p>一般机器学习处理的数据是csv的格式，一般包括file header, comments(用#表示)，delimiter(即逗号)，quotes(数值有引用时，用双引号)</p><h3 id="Load-csv-with-python"><a href="#Load-csv-with-python" class="headerlink" title="Load csv with python"></a>Load csv with python</h3><p>load出来的数据先变成list,再转成array，输出给sklearn</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Load CSV Using Python Standard Library</span><br><span class="line">import csv</span><br><span class="line">import numpy</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">raw_data &#x3D; open(filename, &#39;rt&#39;)</span><br><span class="line">reader &#x3D; csv.reader(raw_data, delimiter&#x3D;&#39;,&#39;, quoting&#x3D;csv.QUOTE_NONE)</span><br><span class="line">x &#x3D; list(reader)</span><br><span class="line">data &#x3D; numpy.array(x).astype(&#39;float&#39;)</span><br><span class="line">print(data.shape)</span><br><span class="line"># out</span><br><span class="line">(768, 9)</span><br></pre></td></tr></table></figure><h3 id="Load-csv-with-numpy"><a href="#Load-csv-with-numpy" class="headerlink" title="Load csv with numpy"></a>Load csv with numpy</h3><p>用numpy.loadtxt(),出来的直接是array，但load出来的数据是没有header row</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Load CSV using NumPy</span><br><span class="line">from numpy import loadtxt</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">raw_data &#x3D; open(filename, &#39;rt&#39;)</span><br><span class="line">data &#x3D; loadtxt(raw_data, delimiter&#x3D;&quot;,&quot;)</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># Load CSV from URL using NumPy</span><br><span class="line">from numpy import loadtxt</span><br><span class="line">from urllib.request import urlopen</span><br><span class="line">url &#x3D; &#39;https:&#x2F;&#x2F;goo.gl&#x2F;bDdBiA&#39;</span><br><span class="line">raw_data &#x3D; urlopen(url)</span><br><span class="line">dataset &#x3D; loadtxt(raw_data, delimiter&#x3D;&quot;,&quot;)</span><br><span class="line">print(dataset.shape)</span><br></pre></td></tr></table></figure><h3 id="Load-csv-with-pandas"><a href="#Load-csv-with-pandas" class="headerlink" title="Load csv with pandas"></a>Load csv with pandas</h3><p>终于说到最常用的load data的方式，1是因为简单，2是因为方便后续的数据理解可视化。直接用 pandas.read_csv()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from urllib.request import urlopen</span><br><span class="line"># Load CSV using Pandas</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names) # 把names&#x3D;col_label赋值给数据</span><br><span class="line">print(data.shape)</span><br></pre></td></tr></table></figure><h2 id="理解你的数据with统计描述"><a href="#理解你的数据with统计描述" class="headerlink" title="理解你的数据with统计描述"></a>理解你的数据with统计描述</h2><h3 id="数据一瞥"><a href="#数据一瞥" class="headerlink" title="数据一瞥"></a>数据一瞥</h3><p>利用data.head(), default 是5行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># View first 20 rows</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">peek &#x3D; data.head(20) #20行</span><br><span class="line">print(peek)</span><br></pre></td></tr></table></figure><h3 id="数据维度"><a href="#数据维度" class="headerlink" title="数据维度"></a>数据维度</h3><p>用data.shape</p><h3 id="每个属性的数据类型"><a href="#每个属性的数据类型" class="headerlink" title="每个属性的数据类型"></a>每个属性的数据类型</h3><p>用 date.dtype</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Data Types for Each Attribute</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names) types &#x3D; data.dtypes print(types)</span><br><span class="line"># out</span><br><span class="line">preg int64</span><br><span class="line">plas int64</span><br><span class="line">pres int64</span><br><span class="line">skin int64</span><br><span class="line">test int64</span><br><span class="line">mass float64</span><br><span class="line">pedi float64</span><br><span class="line">age int64</span><br><span class="line">class int64</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><h3 id="统计描述"><a href="#统计描述" class="headerlink" title="统计描述"></a>统计描述</h3><p>用data.describe()<br>描述了8个统计属性：</p><ul><li>count(计数),</li><li>mean(平均数)，</li><li>Standard Deviation(标准差),</li><li>Minimum Value(最小值),</li><li>25th Percentile(第25百分位),</li><li>50th Percentile (Median中位数).,</li><li>75th Percentile(第75百分位),</li><li>Maximum Value(最大数)</li></ul><h3 id="Class-Distribution-类分布"><a href="#Class-Distribution-类分布" class="headerlink" title="Class Distribution(类分布)"></a>Class Distribution(类分布)</h3><p>有一些属性的值可以明显的分为几类（例如0，1），可以通过如下的方法快速的知道属性的class分布情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Class Distribution</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">class_counts &#x3D; data.groupby(&#39;class&#39;).size()</span><br><span class="line">print(class_counts)</span><br><span class="line"># out 看出class 那一列可以分为0，1类别，各有500和268行</span><br><span class="line">class</span><br><span class="line">0 500</span><br><span class="line">1 268</span><br></pre></td></tr></table></figure><h3 id="属性之间相关性-correlation"><a href="#属性之间相关性-correlation" class="headerlink" title="属性之间相关性(correlation)"></a>属性之间相关性(correlation)</h3><p>相关性是指两个变量之间的相关性。<br>一般使用 <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" target="_blank" rel="noopener">Pearson’s Correlation coefficient</a>, 假设属性是正太分布的（这里不是很懂，待查）。<br>0 表示没有相关性，1 和 -1 表示完全相关和负相关。<br>在机器学习中，弱相关性对于机器学习训练结果好，而强相关性很不利于线性和逻辑回归等算法。有可能去掉其中其中属性列再训练。<br>利用Pandas DataFrame中data.corr() 可以直接计算相关系数矩阵。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Pairwise Pearson correlations</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from pandas import set_option</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">set_option(&#39;display.width&#39;, 100)  #屏幕横向最多显示100个字符</span><br><span class="line">set_option(&#39;precision&#39;, 3) #小数点后显示3位</span><br><span class="line">correlations &#x3D; data.corr(method&#x3D;&#39;pearson&#39;)</span><br><span class="line">print(correlations)</span><br></pre></td></tr></table></figure><p><a href="https://www.cnblogs.com/yesuuu/p/6100714.html" target="_blank" rel="noopener">set_option 函数可以参考</a></p><p><img src="/images/%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9F%A9%E9%98%B5.png" alt="得到的相关性矩阵"></p><h3 id="Skew-of-Univariate-Distribution-偏态分布"><a href="#Skew-of-Univariate-Distribution-偏态分布" class="headerlink" title="Skew of Univariate Distribution(偏态分布)"></a>Skew of Univariate Distribution(偏态分布)</h3><p>很多机器学习算法假设数据变量是高斯分布的。但有一些变量分布倾斜或者squash在一个方向等(中文叫<a href="https://baike.baidu.com/item/%E5%81%8F%E6%80%81%E7%B3%BB%E6%95%B0/10793795?fr=aladdin" target="_blank" rel="noopener">偏态分布</a>)，所以可以纠正它的倾斜性从而提高模型准确度，一般尝试取对数，缩小差异。</p><p>0表示不偏，越远离0表示越偏斜 中位数和平均数之差的三次方 除以 标准差三次方</p><p>通过Pandas dataframe中函数 skew()知道分布</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># Skew for each attribute</span><br><span class="line">from pandas import read_csv</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">skew &#x3D; data.skew() print(skew)</span><br><span class="line"># out</span><br><span class="line">preg 0.901674</span><br><span class="line">plas 0.173754</span><br><span class="line">pres -1.843608</span><br><span class="line">skin 0.109372</span><br><span class="line">test 2.272251</span><br><span class="line">mass -0.428982</span><br><span class="line">pedi 1.919911</span><br><span class="line">age 1.129597</span><br><span class="line">class 0.635017</span><br></pre></td></tr></table></figure><h3 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h3><ul><li>查看数据。统计描述并不是全部，停下来看看数据，想一想。</li><li>ask why. 问一些和主题相关的问题</li><li>写下想到ideas，以及后面可以尝试的想法</li></ul><h2 id="更懂你的数据-可视化"><a href="#更懂你的数据-可视化" class="headerlink" title="更懂你的数据-可视化"></a>更懂你的数据-可视化</h2><h3 id="Univariate-plots-单变量画图"><a href="#Univariate-plots-单变量画图" class="headerlink" title="Univariate plots(单变量画图)"></a>Univariate plots(单变量画图)</h3><h4 id="Histograms-直方图"><a href="#Histograms-直方图" class="headerlink" title="Histograms(直方图)"></a>Histograms(直方图)</h4><p>最快的方式可以让你看到每个变量的分布情况。y轴是count。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.hist()</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>![Histograms of each attribute](/images/Histograms of each attribute.png)</p><p>可以看出大概他们的分布，例如age,pedi,preg,test他们大概是指数分布，而mass,pres，plas可能是高斯分布。class 一下可以分为两类</p><h4 id="Density-Plots-密度图"><a href="#Density-Plots-密度图" class="headerlink" title="Density Plots(密度图)"></a>Density Plots(密度图)</h4><p>密度图比直方图更能表示出数据分布的特点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.plot(kind&#x3D;&#39;density&#39;, subplots&#x3D;True, layout&#x3D;(3,3), sharex&#x3D;False) pyplot.show()</span><br></pre></td></tr></table></figure><p>![Density plots of each attribute](/images/Density plots of each attribute.png)</p><h4 id="Box-and-Whisker-Plots-箱线图"><a href="#Box-and-Whisker-Plots-箱线图" class="headerlink" title="Box and Whisker Plots(箱线图)"></a>Box and Whisker Plots(箱线图)</h4><p><a href="http://wiki.mbalib.com/wiki/%E7%AE%B1%E7%BA%BF%E5%9B%BE" target="_blank" rel="noopener">箱线图</a> 画出Q1（25分位），Q2（中位数），Q3（75分位），下边界（Q1-1.5<em>(Q3-Q1)）,上边界（Q3+1.5</em>(Q3-Q1)）</p><p>为什么要画箱线图：</p><ul><li>1直观识别异常值。超过上下边界的认为是异常值；</li><li>2看出数据分布的skew和尾重</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">data.plot(kind&#x3D;&#39;box&#39;, subplots&#x3D;True, layout&#x3D;(3,3), sharex&#x3D;False, sharey&#x3D;False)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>![Box and whisker plots of each attribute](/images/Box and whisker plots of each attribute.png)<br>一下可以看出age,test，skin的数据向smaller values偏斜</p><h3 id="Multivariate-Plots-多变量画图"><a href="#Multivariate-Plots-多变量画图" class="headerlink" title="Multivariate Plots(多变量画图)"></a>Multivariate Plots(多变量画图)</h3><p>主要是反应是不同属性间的interaction关系</p><h4 id="Correlation-Matrix-Plot"><a href="#Correlation-Matrix-Plot" class="headerlink" title="Correlation Matrix Plot"></a>Correlation Matrix Plot</h4><p>可以参考上面关于相关性的解释</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Correlation Matrix Plot</span><br><span class="line">from matplotlib import pyplot</span><br><span class="line">from pandas import read_csv</span><br><span class="line">import numpy</span><br><span class="line">filename &#x3D; &#39;pima-indians-diabetes.data.csv&#39;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">correlations &#x3D; data.corr() # plot correlation matrix</span><br><span class="line">fig &#x3D; pyplot.figure()  # init一个画布</span><br><span class="line">ax &#x3D; fig.add_subplot(111) # 将画布分割成1行1列，图像画在第一块</span><br><span class="line">cax &#x3D; ax.matshow(correlations, vmin&#x3D;-1, vmax&#x3D;1) # matshow是画出矩阵图</span><br><span class="line">fig.colorbar(cax) #给出colorbar</span><br><span class="line">ticks &#x3D; numpy.arange(0,9,1) # tick是刻度的意思</span><br><span class="line">ax.set_xticks(ticks) #设置x刻度范围，步长</span><br><span class="line">ax.set_yticks(ticks)</span><br><span class="line">ax.set_xticklabels(names) #x刻度用names值来表示</span><br><span class="line">ax.set_yticklabels(names)</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>跟多关于绘画的<a href="https://matplotlib.org/index.html" target="_blank" rel="noopener">参考</a></p><p>也可以不需要set_xticks，去除ax.set_xticks一下内容，之后图片显示没有label了</p><p><img src="/images/Correlation matrix plot1.png" width="50%" height="50%"><img src="/images/Correlation matrix plot2.png" width="50%" height="50%"></p><p>其实现在还有一种更好看的画图库<a href="http://seaborn.pydata.org/generated/seaborn.heatmap.html?highlight=heat#seaborn.heatmap" target="_blank" rel="noopener">seaborn</a>来表示热点图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import seaborn as sns</span><br><span class="line">cr &#x3D; data.corr()</span><br><span class="line"># plot a heatmap of correlation</span><br><span class="line">ax1 &#x3D; sns.heatmap(cr, center &#x3D; 0)</span><br><span class="line">display(cr)</span><br></pre></td></tr></table></figure><p>![seaborn example](/images/seaborn example.png)</p><h4 id="Scatter-Plot-Matrix-散点图"><a href="#Scatter-Plot-Matrix-散点图" class="headerlink" title="Scatter Plot Matrix(散点图)"></a>Scatter Plot Matrix(散点图)</h4><p>pd.scatter_matrix()</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib import pyplot</span><br><span class="line">from pandas import read_csv</span><br><span class="line">from pandas.plotting import scatter_matrix</span><br><span class="line">filename &#x3D; &quot;pima-indians-diabetes.data.csv&quot;</span><br><span class="line">names &#x3D; [&#39;preg&#39;, &#39;plas&#39;, &#39;pres&#39;, &#39;skin&#39;, &#39;test&#39;, &#39;mass&#39;, &#39;pedi&#39;, &#39;age&#39;, &#39;class&#39;]</span><br><span class="line">data &#x3D; read_csv(filename, names&#x3D;names)</span><br><span class="line">scatter_matrix(data) # if设置diagonal &#x3D;&#39;kde&#39;,对角线会显示密度图</span><br><span class="line">pyplot.show()</span><br></pre></td></tr></table></figure><p>![Scatter plot matrix of the data](/images/Scatter plot matrix of the data.png)</p><h3 id="More-links"><a href="#More-links" class="headerlink" title="More links"></a>More links</h3><p><a href="https://blog.csdn.net/qq_34264472/article/details/53814653" target="_blank" rel="noopener">seaborn tutorial</a><br><a href="https://blog.csdn.net/ali197294332/article/details/51694141" target="_blank" rel="noopener">matplotlib tutorial</a><br><a href="https://github.com/JonOnEarth/ML-NANO/blob/master/customer_segments/customer_segments.ipynb" target="_blank" rel="noopener">我的github项目</a></p>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>machine learning with python(1)</title>
      <link href="/2018/05/15/machine-learning-with-python-1/"/>
      <url>/2018/05/15/machine-learning-with-python-1/</url>
      
        <content type="html"><![CDATA[<p>想来机器学习学习也快1年了。一年来，囫囵吞枣，上完了Udacity的机器学习课程，拿到了证书，做了几个项目。但总感觉自己学的不够扎实，学了后面忘了前面，非常有必要来复习整理一下。尤其现在跟Professor做了一个机器学习应用，更加凸显了自己数学方面的薄弱。</p><p>想了好几种方式来扎实自己的机器学习。开始想用Udacity复习的，发现再翻看视频很费时间，暂且作为补充方式。首选这位大神的书，用他的书来复习，能事半功倍。强势安利：<a href="https://machinelearningmastery.com/products/" target="_blank" rel="noopener">machinelearningmastery.com</a>。以下基本都是书中的笔记。</p><p>这次机器学习的几个重点：<br>1, python怎么处理数据<br>2, 算法总结</p><h2 id="Python-Ecosystem-for-Machine-Learning"><a href="#Python-Ecosystem-for-Machine-Learning" class="headerlink" title="Python Ecosystem for Machine Learning"></a>Python Ecosystem for Machine Learning</h2><h3 id="两个Python软件："><a href="#两个Python软件：" class="headerlink" title="两个Python软件："></a>两个Python软件：</h3><p>推荐新手安装这两个python软件(我是win10)：  </p><ul><li>Anaconda 读“安娜康德”，大蟒蛇的意思。数据处理的很多包都已经集成，安装省事，不需要你配置各种环境。</li><li>winPython 绿色版本，不用安装，下载即用。集成了各种软件，各种库。</li><li>新手开始最好用软件里的jupyter notebook，别提多方便。附上jupyter的一个<a href="https://www.datacamp.com/community/blog/jupyter-notebook-cheat-sheet" target="_blank" rel="noopener">小抄</a></li></ul><h3 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h3><ul><li>Numpy：让数据变成arrays格式，使数据适用于机器学习算法</li><li>Pandas: load, organize and anlyze 数据, 更好的理解数据</li><li>Matpoltlib: 可视化数据</li></ul><h3 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h3><p>需要Scipy的环境支持。有很多的机器学习算法，分类，回归，聚类等。</p><h2 id="Crash-course"><a href="#Crash-course" class="headerlink" title="Crash course"></a>Crash course</h2><h3 id="Crash-course-of-python"><a href="#Crash-course-of-python" class="headerlink" title="Crash course of python"></a>Crash course of python</h3><p>参考<a href="https://jononearth.com/%E5%AD%A6%E7%BC%96%E7%A8%8B/python-cheatsheet1/">python小抄</a>  </p><h3 id="Numpy-Crash-Course"><a href="#Numpy-Crash-Course" class="headerlink" title="Numpy Crash Course"></a>Numpy Crash Course</h3><p>参考<a href="https://jononearth.com/%E5%AD%A6%E7%BC%96%E7%A8%8B/Numpy-%E5%B0%8F%E6%8A%84/">numpy小抄</a></p><h4 id="create-array"><a href="#create-array" class="headerlink" title="create array"></a>create array</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># define an array</span><br><span class="line">import numpy</span><br><span class="line">mylist &#x3D; [1, 2, 3]</span><br><span class="line">myarray &#x3D; numpy.array(mylist)</span><br><span class="line">print(myarray)</span><br><span class="line">print(myarray.shape)</span><br><span class="line"># out</span><br><span class="line">[1 2 3]</span><br><span class="line">(3,)</span><br></pre></td></tr></table></figure><h4 id="获取-Data"><a href="#获取-Data" class="headerlink" title="获取 Data"></a>获取 Data</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># access values</span><br><span class="line">import numpy</span><br><span class="line">mylist &#x3D; [[1, 2, 3], [3, 4, 5]]</span><br><span class="line">myarray &#x3D; numpy.array(mylist)</span><br><span class="line">print(myarray)</span><br><span class="line">print(myarray.shape)</span><br><span class="line">print(&quot;First row: %s&quot; % myarray[0])</span><br><span class="line">print(&quot;Last row: %s&quot; % myarray[-1])</span><br><span class="line">print(&quot;Specific row and col: %s&quot; % myarray[0, 2])</span><br><span class="line">print(&quot;Whole col: %s&quot; % myarray[:, 2])</span><br><span class="line"># out</span><br><span class="line">[[1 2 3] [3 4 5]]</span><br><span class="line">(2, 3)</span><br><span class="line">First row: [1 2 3]</span><br><span class="line">Last row: [3 4 5]</span><br><span class="line">Specific row and col: 3</span><br><span class="line">Whole col: [3 5]</span><br></pre></td></tr></table></figure><h4 id="Arithmetic"><a href="#Arithmetic" class="headerlink" title="Arithmetic"></a>Arithmetic</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># arithmetic</span><br><span class="line">import numpy</span><br><span class="line">myarray1 &#x3D; numpy.array([2, 2, 2])</span><br><span class="line">myarray2 &#x3D; numpy.array([3, 3, 3])</span><br><span class="line">print(&quot;Addition: %s&quot; % (myarray1 + myarray2))</span><br><span class="line">print(&quot;Multiplication: %s&quot; % (myarray1 * myarray2))</span><br><span class="line"># out</span><br><span class="line">Addition: [5 5 5]</span><br><span class="line">Multiplication: [6 6 6]</span><br></pre></td></tr></table></figure><h3 id="Matplotlib-Crash-Course"><a href="#Matplotlib-Crash-Course" class="headerlink" title="Matplotlib Crash Course"></a>Matplotlib Crash Course</h3><ul><li>call the function with data (such as .plot)</li><li>call many functions to setup the properties of the plot(labels and colors)</li><li>make the plot visible(e.g. .show)</li></ul><h4 id="直线图"><a href="#直线图" class="headerlink" title="直线图"></a>直线图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># basic line plot 一维</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy</span><br><span class="line">myarray &#x3D; numpy.array([1, 2, 3])</span><br><span class="line">plt.plot(myarray)</span><br><span class="line">plt.xlabel(&#39;some x axis&#39;)</span><br><span class="line">plt.ylabel(&#39;some y axis&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="散点图"><a href="#散点图" class="headerlink" title="散点图"></a>散点图</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># basic scatter plot</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy</span><br><span class="line">x &#x3D; numpy.array([1, 2, 3])</span><br><span class="line">y &#x3D; numpy.array([2, 4, 6])</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line">plt.xlabel(&#39;some x axis&#39;)</span><br><span class="line">plt.ylabel(&#39;some y axis&#39;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h4 id="subplots"><a href="#subplots" class="headerlink" title="subplots"></a>subplots</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, axes &#x3D; plt.subplots(2, 2, subplot_kw&#x3D;dict(polar&#x3D;True))# fig还是指一个整体图</span><br><span class="line">axes[0, 0].plot(x, y) # axes表示每个图的集合</span><br><span class="line">axes[1, 1].scatter(x, y)</span><br></pre></td></tr></table></figure><h4 id="python-画图"><a href="#python-画图" class="headerlink" title="python 画图"></a>python 画图</h4><ul><li>怎么画更好看：<a href="https://matplotlib.org/3.2.0/gallery/style_sheets/style_sheets_reference.html#sphx-glr-gallery-style-sheets-style-sheets-reference-py" target="_blank" rel="noopener">style sheets</a>, <a href="https://matplotlib.org/3.2.0/gallery/index.html#style-sheets" target="_blank" rel="noopener">other style sheets</a></li><li>plotly: </li></ul><h3 id="Pandas-Crash-Course"><a href="#Pandas-Crash-Course" class="headerlink" title="Pandas Crash Course"></a>Pandas Crash Course</h3><p>参考<a href="https://jononearth.com/%E5%AD%A6%E7%BC%96%E7%A8%8B/Pands%E5%B0%8F%E6%8A%84/">pandas小抄</a></p><h4 id="series"><a href="#series" class="headerlink" title="series"></a>series</h4><p>一个series是1维的array + index，每个row上面加了label</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># series</span><br><span class="line">import numpy</span><br><span class="line">import pandas</span><br><span class="line">myarray &#x3D; numpy.array([1, 2, 3])</span><br><span class="line">rownames &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]</span><br><span class="line">myseries &#x3D; pandas.Series(myarray, index&#x3D;rownames)</span><br><span class="line">print(myseries)</span><br><span class="line"># out</span><br><span class="line">a 1</span><br><span class="line">b 2</span><br><span class="line">c 3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">print(myseries[0])</span><br><span class="line">print(myseries[&#39;a&#39;])</span><br><span class="line">1</span><br><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><p>Data frame是多维array + row_label + col_label</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># dataframe</span><br><span class="line">import numpy</span><br><span class="line">import pandas</span><br><span class="line">myarray &#x3D; numpy.array([[1, 2, 3], [4, 5, 6]])</span><br><span class="line">rownames &#x3D; [&#39;a&#39;, &#39;b&#39;]</span><br><span class="line">colnames &#x3D; [&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span><br><span class="line">mydataframe &#x3D; pandas.DataFrame(myarray, index&#x3D;rownames, columns&#x3D;colnames)</span><br><span class="line">print(mydataframe)</span><br><span class="line"># out</span><br><span class="line">  one two three</span><br><span class="line">a 1   2    3</span><br><span class="line">b 4   5    6</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;method 1:&quot;)</span><br><span class="line">print(&quot;one column:\n%s&quot; % mydataframe[&#39;one&#39;])</span><br><span class="line">print(&quot;method 2:&quot;)</span><br><span class="line">print(&quot;one column:\n%s&quot; % mydataframe.one)</span><br><span class="line"># out</span><br><span class="line">method 1:</span><br><span class="line">one column:</span><br><span class="line">a 1</span><br><span class="line">b 4</span><br><span class="line">method 2:</span><br><span class="line">one column:</span><br><span class="line">a 1</span><br><span class="line">b 4</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 机械公敌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas小抄</title>
      <link href="/2018/05/10/Pands%E5%B0%8F%E6%8A%84/"/>
      <url>/2018/05/10/Pands%E5%B0%8F%E6%8A%84/</url>
      
        <content type="html"><![CDATA[<p>Pandas 在数据中学中是常用的库。 整理了一些好用方便的小抄，随时可以查看。</p><h2 id="Pandas小抄"><a href="#Pandas小抄" class="headerlink" title="Pandas小抄"></a>Pandas小抄</h2><ul><li><p><a href="https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/pandas/pandas.ipynb" target="_blank" rel="noopener">来源1 notebook版</a><br><img src="/images/pandas%E5%B0%8F%E6%8A%841.jpg" alt=""></p></li><li><p><a href="https://www.datacamp.com/community/blog/python-pandas-cheat-sheet#gs.S4P4T=U" target="_blank" rel="noopener">来源2</a><br><img src="/images/pandas%E5%B0%8F%E6%8A%842.jpg" alt=""></p></li><li><p><a href="http://datasciencefree.com/pandas.pdf" target="_blank" rel="noopener">来源3</a><br><img src="/images/pandas%E5%B0%8F%E6%8A%843.jpg" alt=""></p></li></ul><h2 id="my-note"><a href="#my-note" class="headerlink" title="my note"></a>my note</h2><h3 id="重要概念1：Series"><a href="#重要概念1：Series" class="headerlink" title="重要概念1：Series"></a>重要概念1：Series</h3><ul><li>series 有 index<br>表示方式：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ser_1 &#x3D; Series([1,5,3])</span><br><span class="line">ser_1</span><br><span class="line">&gt;&gt;&gt; 0   1</span><br><span class="line">&gt;&gt;&gt; 1   5</span><br><span class="line">&gt;&gt;&gt; 2   3</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ser_1.values</span><br><span class="line">&gt;&gt;&gt;array([1,5,3])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ser_2 &#x3D; Series([1,1,2,,-3], index &#x3D; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]</span><br><span class="line">ser_2</span><br><span class="line">&gt;&gt;&gt;a   1</span><br><span class="line">&gt;&gt;&gt;b   1</span><br><span class="line">&gt;&gt;&gt;c   2</span><br><span class="line">&gt;&gt;&gt;d   -3</span><br></pre></td></tr></table></figure></li><li>create a series by passing in a dict:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dict_1 &#x3D; &#123;&#39;foo&#39; : 100, &#39;bar&#39; : 200, &#39;baz&#39; : 300&#125;</span><br><span class="line">ser_3 &#x3D; Series(dict_1)</span><br><span class="line">ser3</span><br><span class="line">&gt;&gt;&gt;bar  200</span><br><span class="line">&gt;&gt;&gt;baz  300</span><br><span class="line">&gt;&gt;&gt;foo  100</span><br><span class="line">&gt;&gt;&gt;dtype: int64</span><br></pre></td></tr></table></figure></li></ul><h3 id="重要概念2：DataFrame"><a href="#重要概念2：DataFrame" class="headerlink" title="重要概念2：DataFrame"></a>重要概念2：DataFrame</h3><ul><li>Dict to DataFrame   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dict_1 &#x3D; &#123;&#39;state&#39;:[&#39;VA&#39;,&#39;VA&#39;,&#39;VA&#39;,&#39;MA&#39;,&#39;MA&#39;],</span><br><span class="line">          &#39;year&#39;:[2012,2013,2014,2014,2015],</span><br><span class="line">          &#39;pop&#39;:[5,5,5,3,2]&#125;</span><br><span class="line">df_1 &#x3D; DataFrame(dict_1)</span><br></pre></td></tr></table></figure></li></ul><table><thead><tr><th align="left"></th><th align="left">pop</th><th align="left">state</th><th align="left">year</th></tr></thead><tbody><tr><td align="left">0</td><td align="left">5</td><td align="left">VA</td><td align="left">2012</td></tr><tr><td align="left">1</td><td align="left">5</td><td align="left">VA</td><td align="left">2013</td></tr><tr><td align="left">2</td><td align="left">5</td><td align="left">VA</td><td align="left">2014</td></tr><tr><td align="left">3</td><td align="left">3</td><td align="left">MA</td><td align="left">2014</td></tr><tr><td align="left">4</td><td align="left">2</td><td align="left">MA</td><td align="left">2015</td></tr></tbody></table><ul><li>返回Series值  </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df_1[&#39;state&#39;]</span><br><span class="line">&gt;&gt;&gt;0 VA</span><br><span class="line">&gt;&gt;&gt;1 VA</span><br><span class="line">&gt;&gt;&gt;2 VA</span><br><span class="line">&gt;&gt;&gt;3 MA</span><br><span class="line">&gt;&gt;&gt;4 MA</span><br><span class="line">&gt;&gt;&gt;Name: state, dtype: object</span><br></pre></td></tr></table></figure><ul><li>取一行值</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_1.ix[0]</span><br><span class="line">&gt;&gt;&gt;pop 5</span><br><span class="line">&gt;&gt;&gt;state VA</span><br><span class="line">&gt;&gt;&gt;year 2012</span><br></pre></td></tr></table></figure><h3 id="去掉NaN值（todo）"><a href="#去掉NaN值（todo）" class="headerlink" title="去掉NaN值（todo）"></a>去掉NaN值（todo）</h3><h3 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h3><ul><li>get dataset csv</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_1 &#x3D; pd.read_csv(&quot;..&#x2F;data&#x2F;jon.csv&quot;)</span><br></pre></td></tr></table></figure><ul><li>list the first 5 rows of the DataFrame</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_1.head()</span><br></pre></td></tr></table></figure><ul><li>create a copy of the CSV file, encoded in UTF-8 and hiding the index and header labels:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df_1.to_csv(&#39;..&#x2F;data&#x2F;jon.csv,</span><br><span class="line">            encoding&#x3D;&#39;utf-8&#39;,</span><br><span class="line">            index&#x3D;False,</span><br><span class="line">            header&#x3D;False)</span><br></pre></td></tr></table></figure><h3 id="df里的数据与数比大小后-输出"><a href="#df里的数据与数比大小后-输出" class="headerlink" title="df里的数据与数比大小后 输出"></a>df里的数据与数比大小后 输出</h3><p>df_ &gt; 5表示 df里面大于5的数输出true，df_[true]输出，<strong>小于5的输出NaN</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_[df_ &gt; 5]</span><br></pre></td></tr></table></figure><h3 id="合并数据"><a href="#合并数据" class="headerlink" title="合并数据"></a>合并数据</h3><p>pd.concat([],axis=0, ignore_index=true,…)</p><h3 id="重复值的处理"><a href="#重复值的处理" class="headerlink" title="重复值的处理"></a>重复值的处理</h3><p>Python数据分析-数据处理-重复值处理<br><a href="https://blog.csdn.net/maxwell315/article/details/75639190" target="_blank" rel="noopener">https://blog.csdn.net/maxwell315/article/details/75639190</a></p><p>duplicated<br>drop_duplicated</p><h3 id="读取mat格式文件"><a href="#读取mat格式文件" class="headerlink" title="读取mat格式文件"></a>读取mat格式文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mport pandas as pd</span><br><span class="line">import scipy</span><br><span class="line">from scipy import io</span><br><span class="line">features_struct &#x3D; scipy.io.loadmat(&#39;E:&#x2F;workspacelxr&#x2F;contem&#x2F;data.mat&#39;)</span><br><span class="line">features &#x3D; features_struct[&#39;data&#39;]</span><br><span class="line">dfdata &#x3D; pd.DataFrame(features)</span><br><span class="line">datapath1 &#x3D; &#39;E:&#x2F;workspacelxr&#x2F;contem&#x2F;data.txt&#39;</span><br><span class="line">dfdata.to_csv(datapath1, index&#x3D;False)</span><br><span class="line">---------------------</span><br><span class="line">作者：Luisa_M</span><br><span class="line">来源：CSDN</span><br><span class="line">原文：https:&#x2F;&#x2F;blog.csdn.net&#x2F;zebralxr&#x2F;article&#x2F;details&#x2F;78254192</span><br><span class="line">版权声明：本文为博主原创文章，转载请附上博文链接！</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Numpy 小抄</title>
      <link href="/2018/05/09/Numpy-%E5%B0%8F%E6%8A%84/"/>
      <url>/2018/05/09/Numpy-%E5%B0%8F%E6%8A%84/</url>
      
        <content type="html"><![CDATA[<p>NumPy is the library that gives Python its ability to work with data at speed. Originally, launched in 1995 as ‘Numeric,’ NumPy is the foundation on which many important Python data science libraries are built, including Pandas, SciPy and scikit-learn.</p><p>先给出几个很好的numpy 小抄，以后再整理遇到的一些numpy相关问题：</p><h2 id="Numpy小抄"><a href="#Numpy小抄" class="headerlink" title="Numpy小抄"></a>Numpy小抄</h2><ul><li><p><img src="/images/numpy_%E5%B0%8F%E6%8A%842.jpg" alt="[来源1](https://www.dataquest.io/blog/numpy-cheat-sheet/)"></p></li><li><p><img src="/images/numpy_notebook.png" alt="[来源2：Notebook版](https://github.com/donnemartin/data-science-ipython-notebooks/blob/master/numpy/numpy.ipynb)"></p></li></ul><h2 id="numpy重要性质"><a href="#numpy重要性质" class="headerlink" title="numpy重要性质"></a>numpy重要性质</h2><h3 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h3><p>就是说矩阵运算的时候， a.shape = (3,2), b.shape=(3,1), a+b 时，b会自动复制成shape = (3,2),进行运算。</p><p>参考： <a href="https://www.coursera.org/learn/neural-networks-deep-learning/notebook/Zh0CU/python-basics-with-numpy-optional" target="_blank" rel="noopener">coursera deeplearning.ai</a>,</p><h2 id="My-note"><a href="#My-note" class="headerlink" title="My note"></a>My note</h2><table><thead><tr><th align="left">函数</th><th align="left">解释</th><th align="left">例子</th></tr></thead><tbody><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.around.html#numpy.around" target="_blank" rel="noopener">numpy.round(a, decimals=0, out=None)</a> 与around函数相似</td><td align="left">Round an array to the given number of [decimals].</td><td align="left">np.around([0.37, 1.64], decimals=1)  结果：array([ 0.4,  1.6])</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.floor.html" target="_blank" rel="noopener">np.floor()</a></td><td align="left">返回标量的最大整数</td><td align="left">&gt;&gt;&gt; a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])  (&gt;&gt;&gt; np.floor(a) 结果：array([-2., -2., -1.,  0.,  1.,  1.,  2.]))</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html" target="_blank" rel="noopener">np.column_stack((x, y))</a></td><td align="left">把两个1D的array变成一个2D array</td><td align="left">&gt;&gt;&gt; a = np.array((1,2,3)) &gt;&gt;&gt; b = np.array((2,3,4)) &gt;&gt;&gt; np.column_stack((a,b)) 结果：array([[1, 2],[2, 3],[3, 4]])</td></tr><tr><td align="left">numpy.arange(0,9,1)</td><td align="left">表示0到9之间的数，step是1</td><td align="left">输出是0，1，2…到8</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linspace.html" target="_blank" rel="noopener">np.linspace(start, stop, num,endpoint)</a></td><td align="left">start to stop之间有num个数</td><td align="left">&gt;&gt;&gt; np.linspace(2.0, 3.0, num=5, endpoint=False) out: array([ 2. ,  2.2,  2.4,  2.6,  2.8])</td></tr><tr><td align="left">np.float_()</td><td align="left">list变array, float</td><td align="left"></td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.unique.html" target="_blank" rel="noopener">np.unique()</a></td><td align="left">算array里有多少个count的时候可以用一下</td><td align="left"></td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html" target="_blank" rel="noopener">numpy.random.rand(a,b)</a></td><td align="left">转化成a行b列0,1之间的任意数</td><td align="left">np.random.rand(3,2) out: array([[ 0.14022471,  0.96360618], [ 0.37601032,  0.25528411], [ 0.49313049,  0.94909878]]) #random</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html" target="_blank" rel="noopener">np.sum()</a></td><td align="left">axis=1,一行向量自己里面相加，axis=0,一列里面自己相加</td><td align="left">&gt;&gt;&gt; np.sum([[0, 1], [0, 5]]), np.sum([[0, 1], [0, 5]], axis=0) np.sum([[0, 1], [0, 5]], axis=1) output: 6, array([0,6]), array([1, 5])</td></tr><tr><td align="left"><a href="https://blog.csdn.net/hqh131360239/article/details/79061535" target="_blank" rel="noopener">np.linalg.norm()</a></td><td align="left">求范数,默认2范数，axis=1,一行向量自己范2，axis=0,一列列列向量范2</td><td align="left"></td></tr><tr><td align="left">np.row_stack()</td><td align="left">矩阵增加行</td><td align="left">a = np.array([[4, 4,], [5, 5]])  c = np.row_stack((a, [8,9]))  output: array([[4,4],[5,5],[8,9]])</td></tr><tr><td align="left">np.column_stack()</td><td align="left">矩阵增加行</td><td align="left">a = np.array([[4, 4,], [5, 5]])  c = np.column_stack((a, [8,9]))  output: array([[4,4,8],[5,5,9])</td></tr><tr><td align="left"><a href="https://jingyan.baidu.com/article/9113f81b2291802b3214c706.html" target="_blank" rel="noopener">np.all()</a></td><td align="left">假如我们想要知道矩阵a和矩阵b中所有对应元素是否相等，我们需要使用all方法</td><td align="left"></td></tr><tr><td align="left">np.any()</td><td align="left">假如我们想要知道矩阵a和矩阵b中对应元素是否有一个相等，我们需要使用any方法</td><td align="left"></td></tr><tr><td align="left"><a href="https://blog.csdn.net/yangyuwen_yang/article/details/79193770" target="_blank" rel="noopener">np.unique()</a>，<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html" target="_blank" rel="noopener">link2</a></td><td align="left">重复的值，取唯一值并且从小到大排列</td><td align="left">a, s= np.unique(A, return_index=True)</td></tr><tr><td align="left"><a href="https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.delete.html" target="_blank" rel="noopener">np.delete(arr,1,axis=0)</a></td><td align="left">axis = 0 mean delete vertically</td><td align="left"></td></tr><tr><td align="left"><a href="https://www.programiz.com/python-programming/assert-statement" target="_blank" rel="noopener">assert(a.shape == (1,5))</a></td><td align="left">Assertions are simply boolean expressions that checks if the conditions return true or not. If it is true, the program does nothing and move to the next line of code. However, if it’s false, the program stops and throws an error.</td><td align="left"></td></tr><tr><td align="left">numpy.sum(a, axis=None, dtype=None, out=None, keepdims=False)</td><td align="left"><a href="https://stackoverflow.com/questions/39441517/in-numpy-sum-there-is-parameter-called-keepdims-what-does-it-do" target="_blank" rel="noopener">keepdims</a>的重要性</td><td align="left"></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
            <tag> numpy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python 小抄</title>
      <link href="/2018/05/08/python-cheatsheet1/"/>
      <url>/2018/05/08/python-cheatsheet1/</url>
      
        <content type="html"><![CDATA[<table><thead><tr><th align="left">列表</th><th align="left">元组</th><th align="left">集合</th><th align="left">字典</th></tr></thead><tbody><tr><td align="left">英文</td><td align="left">list</td><td align="left">tuple</td><td align="left">set</td></tr><tr><td align="left">可否读写</td><td align="left">读写</td><td align="left">只读</td><td align="left">读写</td></tr><tr><td align="left">可否重复</td><td align="left">是</td><td align="left">是</td><td align="left">否</td></tr><tr><td align="left">存储方式</td><td align="left">值</td><td align="left">值</td><td align="left">键(不能重复)</td></tr><tr><td align="left">是否有序</td><td align="left">有序</td><td align="left">有序</td><td align="left">无序</td></tr><tr><td align="left">初始化</td><td align="left">[1,’a’]</td><td align="left">(‘a’, 1)</td><td align="left">set([1,2]) 或 {1,2}</td></tr><tr><td align="left">添加</td><td align="left">append</td><td align="left">只读</td><td align="left">add</td></tr><tr><td align="left">读元素</td><td align="left">l[2:]</td><td align="left">t[0]</td><td align="left">无</td></tr></tbody></table><table><thead><tr><th align="left">字符串’string’</th><th align="left">列表[lists]</th><th align="left">元组(Tuples)</th><th align="left">字典{dictionaries}</th></tr></thead><tbody><tr><td align="left">字符串连接：firstname=’jon’, lastname=’earth’, fullname = firstname + lastname</td><td align="left">创建一个列表：planets = [‘Earth’, ‘Mars’, ‘Jupiter’]</td><td align="left">元组类似lists,但其中元素不能被操作：dimensions=(1920, 1080)</td><td align="left">一个简单的字典 alien = {‘color’:’green’,’planet’:’Mars’}</td></tr><tr><td align="left"></td><td align="left">获取列表元素：home = planets[0]</td><td align="left">dimensions[0]=1920</td><td align="left">取得字典中的keyvalue,如 alien[‘color’]  输出 green</td></tr><tr><td align="left"></td><td align="left">列表切片 planets.append(‘Saturn’)</td><td align="left"></td><td align="left">加入一个新的键值(key为x_positon, value为0)  alien[‘x_position’] = 0</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">遍历所有的键值(key-value)  for name, number in fav_numbers.items():</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">遍历所有的“键”(key)  for name in fav_numbers.keys()：</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left">遍历所有的“值”(value)  for number in fav_numbers.values()：</td></tr></tbody></table><h2 id="一些很有用的参考资料"><a href="#一些很有用的参考资料" class="headerlink" title="一些很有用的参考资料"></a>一些很有用的参考资料</h2><ul><li><a href="http://pythontutor.com/" target="_blank" rel="noopener">VISUALIZE CODE AND GET LIVE HELP</a><br>可视化你的code, 帮助你理解</li><li><a href="https://www.cnblogs.com/soaringEveryday/p/5044007.html" target="_blank" rel="noopener">python中 list, tuple，dict, set的用法</a></li><li><a href="https://blog.csdn.net/lilongsy/article/details/70895753" target="_blank" rel="noopener">Python列表、元组、集合、字典的区别和相互转换</a></li><li><a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PythonForDataScience.pdf" target="_blank" rel="noopener">Lists小抄和Numpy</a></li></ul><hr><h2 id="set"><a href="#set" class="headerlink" title="set"></a>set</h2><p>set就像是把Dict中的key抽出来了一样，类似于一个List，但是内容又不能重复，通过调用set()方法创建</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set([3,6,3]) or</span><br><span class="line">   &#123;3,6,3&#125;</span><br></pre></td></tr></table></figure><ul><li>嵌套tuple<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">s &#x3D; set([(&#39;Adam&#39;, 95), (&#39;Lisa&#39;, 85), (&#39;Bart&#39;, 59)])</span><br><span class="line">#tuple</span><br><span class="line">for x in s:</span><br><span class="line">    print x[0],&#39;:&#39;,x[1]</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">Lisa : 85</span><br><span class="line">Adam : 95</span><br><span class="line">Bart : 59</span><br></pre></td></tr></table></figure></li><li>通过add和remove来添加、删除元素（保持不重复），添加元素时，用set的add()方法：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; set([1, 2, 3])</span><br><span class="line">&gt;&gt;&gt; s.add(4)</span><br><span class="line">&gt;&gt;&gt; print s</span><br><span class="line">set([1, 2, 3, 4])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; s &#x3D; set([1, 2, 3, 4])</span><br><span class="line">&gt;&gt;&gt; s.remove(4)</span><br><span class="line">&gt;&gt;&gt; print s</span><br><span class="line">set([1, 2, 3])</span><br></pre></td></tr></table></figure></li></ul><h2 id="列表lists"><a href="#列表lists" class="headerlink" title="列表lists"></a>列表lists</h2><ul><li>lists中嵌套lists<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my_list2 &#x3D; [[4,5,6,7], [3,4,5,6]]</span><br><span class="line">my_list2[1][:2]</span><br></pre></td></tr></table></figure></li></ul><h3 id="Numpy-与-list-简单对比"><a href="#Numpy-与-list-简单对比" class="headerlink" title="Numpy 与 list 简单对比"></a>Numpy 与 list 简单对比</h3><ul><li>Numpy Arrarys<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_list &#x3D; [1, 2, 3, 4]</span><br><span class="line">my_array &#x3D; np.array(my_list)</span><br><span class="line">my_2darray &#x3D; np.array([[1,2,3],[4,5,6]])</span><br></pre></td></tr></table></figure></li><li>取其中的元素<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Subset</span><br><span class="line">my_array[1]  # select item at index 1</span><br><span class="line">&gt;&gt;&gt; 2</span><br><span class="line"></span><br><span class="line"># slice</span><br><span class="line">my_array[0:2]</span><br><span class="line">&gt;&gt;&gt;array([1, 2])  # selcet items at index 0 and 1</span><br><span class="line"></span><br><span class="line"># Subset 2D Numpy arrays</span><br><span class="line">my_2darray[:,0]</span><br><span class="line">&gt;&gt;&gt;arrary([1,4])  # my_2darray[rows, columns]</span><br></pre></td></tr></table></figure></li></ul><h2 id="字典-Dictionaries"><a href="#字典-Dictionaries" class="headerlink" title="字典 Dictionaries"></a>字典 Dictionaries</h2><ul><li>使用key来访问value 2种方法<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alien_0 &#x3D; &#123;&#39;color&#39;: &#39;green&#39;, &#39;points&#39;:5&#125;</span><br><span class="line">print(alien_0[&#39;color&#39;])</span><br><span class="line">print(alien_0[&#39;points&#39;])</span><br><span class="line"></span><br><span class="line">alien_color &#x3D; alien_0.get(&#39;color&#39;)</span><br></pre></td></tr></table></figure></li><li>增加一个键值对<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">alien_0 &#x3D; &#123;&#39;color&#39;: &#39;green&#39;, &#39;points&#39;: 5&#125;</span><br><span class="line">alien_0[&#39;x&#39;] &#x3D; 0</span><br><span class="line">alien_0[&#39;y&#39;] &#x3D; 25</span><br><span class="line">alien_0[&#39;speed&#39;] &#x3D; 1.5</span><br></pre></td></tr></table></figure></li><li>删除一个键值对<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del alien_0[&#39;points&#39;]</span><br></pre></td></tr></table></figure></li><li>Dict的合并<ul><li>方法1：可以用dict函数<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d1 &#x3D; &#123;&#39;mike&#39;:12, &#39;jack&#39;:19&#125;</span><br><span class="line">d2 &#x3D; &#123;&#39;jone&#39;:22, &#39;ivy&#39;:17&#125;</span><br><span class="line">dMerge &#x3D; dict(d1.items() + d2.items())</span><br><span class="line">print dMerge</span><br><span class="line">&gt;&gt;&gt;&#123;&#39;mike&#39;: 12, &#39;jack&#39;: 19, &#39;jone&#39;: 22, &#39;ivy&#39;: 17&#125;</span><br></pre></td></tr></table></figure></li><li>方法2：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dMerge2 &#x3D; dict(d1, **d2)</span><br><span class="line">print dMerge2</span><br><span class="line">&gt;&gt;&gt;&#123;&#39;mike&#39;: 12, &#39;jack&#39;: 19, &#39;jone&#39;: 22, &#39;ivy&#39;: 17&#125;</span><br></pre></td></tr></table></figure></li><li>方法2比方法1速度快很多，方法2等同于：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dMerge3 &#x3D; dict(d1)</span><br><span class="line">dMerge3.update(d2)</span><br><span class="line">print dMerge</span><br><span class="line">&gt;&gt;&gt;&#123;&#39;mike&#39;: 12, &#39;jack&#39;: 19, &#39;jone&#39;: 22, &#39;ivy&#39;: 17&#125;</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="嵌套"><a href="#嵌套" class="headerlink" title="嵌套"></a>嵌套</h2><ol><li><p>字典嵌套在列表中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Start with anempty list.</span><br><span class="line">users &#x3D; []</span><br><span class="line"># Make a new user, and add them to the list.</span><br><span class="line">new_user &#x3D; &#123;    </span><br><span class="line">  &#39;last&#39;: &#39;fermi&#39;,   </span><br><span class="line">  &#39;first&#39;: &#39;enrico&#39;,  </span><br><span class="line">  &#39;username&#39;: &#39;efermi&#39;,</span><br><span class="line">   &#125;</span><br><span class="line">users.append(new_user)</span><br><span class="line"></span><br><span class="line"> # Make another new user, and add them as well.</span><br><span class="line">new_user &#x3D; &#123;   </span><br><span class="line">   &#39;last&#39;: &#39;curie&#39;,  </span><br><span class="line">  &#39;first&#39;: &#39;marie&#39;,   </span><br><span class="line">  &#39;username&#39;: &#39;mcurie&#39;,    &#125;</span><br><span class="line">users.append(new_user)</span><br><span class="line"></span><br><span class="line"> # Show all information about each user.</span><br><span class="line">for user_dict in users:    </span><br><span class="line">    for k, v in user_dict.items():</span><br><span class="line">        print(k + &quot;: &quot; + v)   </span><br><span class="line">    print(&quot;\n&quot;)</span><br></pre></td></tr></table></figure></li><li><p>列表嵌套在字典中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fav_languages &#x3D; &#123;</span><br><span class="line">      &#39;jen&#39;: [&#39;python&#39;, &#39;ruby&#39;],</span><br><span class="line">      &#39;sarah&#39;: [&#39;c&#39;],  </span><br><span class="line">      &#39;edward&#39;: [&#39;ruby&#39;, &#39;go&#39;],</span><br><span class="line">      &#39;phil&#39;: [&#39;python&#39;, &#39;haskell&#39;],</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # Show all responses for each person.</span><br><span class="line">    for name, langs in fav_languages.items():</span><br><span class="line">         print(name + &quot;: &quot;)</span><br><span class="line">         for lang in langs:</span><br><span class="line">             print(&quot;-&quot; + lang)</span><br></pre></td></tr></table></figure></li><li><p>字典嵌套在字典中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">users &#x3D; &#123;</span><br><span class="line">  &#39;aeinstein&#39;: &#123;</span><br><span class="line">        &#39;first&#39;: &#39;albert&#39;,</span><br><span class="line">        &#39;last&#39;: &#39;einstein&#39;,</span><br><span class="line">        &#39;location&#39;: &#39;princeton&#39;,&#125;,</span><br><span class="line">  &#39;mcurie&#39;: &#123;</span><br><span class="line">        &#39;first&#39;: &#39;marie&#39;，</span><br><span class="line">        &#39;last&#39;: &#39;curie&#39;,  </span><br><span class="line">        &#39;location&#39;: &#39;paris&#39;,&#125;,</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"> for username, user_dict in users.items():</span><br><span class="line">     print(&quot;\nUsername: &quot; + username)</span><br><span class="line">     full_name &#x3D; user_dict[&#39;first&#39;] + &quot; &quot;</span><br><span class="line">     full_name +&#x3D; user_dict[&#39;last&#39;]</span><br><span class="line">     location &#x3D; user_dict[&#39;location&#39;]</span><br><span class="line">     print(&quot;\tFull name: &quot; + full_name.title())</span><br><span class="line">     print(&quot;\tLocation: &quot; + location.title())</span><br></pre></td></tr></table></figure></li><li><p>元组tuple嵌套在列表list中</p></li><li><p>list嵌套在tuple中</p></li></ol><h2 id="类class"><a href="#类class" class="headerlink" title="类class"></a>类class</h2><p>todo</p><h2 id="文件读取和写入"><a href="#文件读取和写入" class="headerlink" title="文件读取和写入"></a>文件读取和写入</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f_path &#x3D; &quot;C:\Users\ehmatthes\books\alice.txt&quot;</span><br><span class="line"></span><br><span class="line">with open(f_path) as f_obj:</span><br><span class="line">  lines &#x3D; f_obj.readlines()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">filename &#x3D; &#39;programming.txt&#39;</span><br><span class="line"># overwirte文件</span><br><span class="line">with open(filename, &#39;w&#39;) as f:</span><br><span class="line">   f.write(&quot;I love programming!&quot;)</span><br><span class="line"># 添加到文件末尾</span><br><span class="line">with open(filename, &#39;w&#39;) as f:</span><br><span class="line">f.write(&quot;I love programming!&quot;)</span><br></pre></td></tr></table></figure><h2 id="python前面加星号和双星号"><a href="#python前面加星号和双星号" class="headerlink" title="python前面加星号和双星号"></a>python前面加星号和双星号</h2><h2 id="python-除法"><a href="#python-除法" class="headerlink" title="python 除法"></a>python 除法</h2><p><a href="https://blog.csdn.net/GarfieldEr007/article/details/51304068" target="_blank" rel="noopener">Python中的除法 整除 非整除</a></p><h2 id="self-in-python"><a href="#self-in-python" class="headerlink" title="self in python"></a>self in python</h2><p>首先明确的是self只有在类的方法中才会有，独立的函数或方法是不必带有self的。self在定义类的方法时是必须有的，虽然在调用时不必传入相应的参数。</p><p>self名称不是必须的，在python中self不是关键词，你可以定义成a或b或其它名字都可以,但是约定成俗（为了和其他编程语言统一，减少理解难度），不要搞另类，大家会不明白的。</p><p>下例中将self改为myname一样没有错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    def _init_(myname,name):</span><br><span class="line">        myname.name&#x3D;name</span><br><span class="line">    def sayhello(myname):</span><br><span class="line">        print &#39;My name is:&#39;,myname.name</span><br><span class="line">p&#x3D;Person(&#39;Bill&#39;)</span><br><span class="line">print p</span><br></pre></td></tr></table></figure><p>self指的是类实例对象本身(注意：不是类本身)。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Person:</span><br><span class="line">    def _init_(self,name):</span><br><span class="line">        self.name&#x3D;name</span><br><span class="line">    def sayhello(self):</span><br><span class="line">        print &#39;My name is:&#39;,self.name</span><br><span class="line">p1&#x3D;Person(&#39;Bill&#39;)</span><br><span class="line">p2 &#x3D; Person(&#39;Apple&#39;)</span><br><span class="line">print p1</span><br></pre></td></tr></table></figure><p>如果self指向类本身，那么当有多个实例对象时，self指向哪一个呢？</p><p>总结:</p><ul><li>self在定义时需要定义，但是在调用时会自动传入。</li><li>self的名字并不是规定死的，但是最好还是按照约定是用self</li><li>self总是指调用时的类的实例。</li></ul><p>转载自：<a href="https://www.cnblogs.com/chownjy/p/8663024.html" target="_blank" rel="noopener">Python类中的self到底是干啥的</a></p><h2 id="python-数据类型转换-dtype"><a href="#python-数据类型转换-dtype" class="headerlink" title="python 数据类型转换 dtype"></a>python 数据类型转换 dtype</h2><p>b = a.astype(int)<br><a href="https://www.cnblogs.com/hhh5460/p/5129032.html" target="_blank" rel="noopener">https://www.cnblogs.com/hhh5460/p/5129032.html</a></p><h2 id="python一些函数"><a href="#python一些函数" class="headerlink" title="python一些函数"></a>python一些函数</h2><table><thead><tr><th align="left">函数</th><th align="left">解释</th><th align="left">例子</th></tr></thead><tbody><tr><td align="left"><a href="http://www.runoob.com/python/python-func-zip.html" target="_blank" rel="noopener">zip(a,b)</a></td><td align="left">把两个list中对应的元素打包成元组，返回list</td><td align="left">a = [1,2,3],b = [4,5,6], zipped = zip(a,b)     # 打包为元组的列表 out: [(1, 4), (2, 5), (3, 6)]</td></tr><tr><td align="left"><a href="https://www.yiibai.com/python/number_randrange.html" target="_blank" rel="noopener">randrange(1,100,2)</a></td><td align="left">randomly select  an odd number between 0~100</td><td align="left">randrange ([start,] stop [,step])</td></tr><tr><td align="left"><a href="http://www.revotu.com/matrix-transpose-in-python.html" target="_blank" rel="noopener">Python矩阵转置方法</a></td><td align="left">先用zip并行迭代每一个列表元素，然后再用map将结果中的元组转成列表。</td><td align="left">list(map(list, zip(*matrix)))</td></tr><tr><td align="left">list.append()</td><td align="left"></td><td align="left">list1 = [‘C++’, ‘Java’, ‘Python’], list1.append(‘C#’), print (“updated list : “, list1) output: pdated list :  [‘C++’, ‘Java’, ‘Python’, ‘C#’]</td></tr><tr><td align="left"><a href="https://blog.csdn.net/weixin_35955795/article/details/52448345" target="_blank" rel="noopener">matrix.count(3)</a></td><td align="left">count() 方法用于统计某个元素在列表中出现的次数</td><td align="left"></td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> 黑客帝国 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 数据科学 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
