<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>Deep Learning(1) | JonOnEarth</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Deep Learning(1)</h1><a id="logo" href="/.">JonOnEarth</a><p class="description">加油中国</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/DailyNote/"><i class="fa fa-user"> DailyNote</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Deep Learning(1)</h1><div class="post-meta">May 27, 2018<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/">机械公敌</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/05/27/Deep-Learning-1/" href="/2018/05/27/Deep-Learning-1/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Network"><span class="toc-number">1.</span> <span class="toc-text">Neural Network</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Perceptrons"><span class="toc-number">1.1.</span> <span class="toc-text">Perceptrons</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-network-and-Linear-regression"><span class="toc-number">1.2.</span> <span class="toc-text">Neural network and Linear regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Activation-Functions"><span class="toc-number">1.3.</span> <span class="toc-text">Activation Functions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Error-function-loss-function"><span class="toc-number">1.4.</span> <span class="toc-text">Error function(loss function)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Cross-entropy"><span class="toc-number">1.4.1.</span> <span class="toc-text">Cross-entropy</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Nerual-Network-Architecture"><span class="toc-number">1.5.</span> <span class="toc-text">Nerual Network Architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feedforward-and-Backpropagation"><span class="toc-number">1.6.</span> <span class="toc-text">Feedforward and Backpropagation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Train-Optimization"><span class="toc-number">1.7.</span> <span class="toc-text">Train Optimization</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Epoch-and-Batch"><span class="toc-number">1.7.1.</span> <span class="toc-text">Epoch and Batch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Overfitting-and-Underfitting"><span class="toc-number">1.7.2.</span> <span class="toc-text">Overfitting and Underfitting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Earlystoppoing"><span class="toc-number">1.7.3.</span> <span class="toc-text">Earlystoppoing</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Learning-rate-decay"><span class="toc-number">1.7.4.</span> <span class="toc-text">Learning rate decay</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Regularization"><span class="toc-number">1.7.5.</span> <span class="toc-text">Regularization</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Dropout"><span class="toc-number">1.7.6.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Local-Minima-Momentum"><span class="toc-number">1.7.7.</span> <span class="toc-text">Local Minima(Momentum)</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>neural networks are a class of machine learning algorithms to model complex patterns in datasets using multiple hidden layers and non-linear activation functions.</p>
<p>It contains an input, passes it through multiple layers of hidden neurons(deep learning contains much more layers), and outputs a prediction representing the combined input of all the neurons.</p>
<p>![1](/images/neural network1.png)</p>
<p>Neural networks are trained iteratively using optimization techniques like <strong>gradient descent</strong>. After every training, an error metric is calculated based on the difference between prediction and target.</p>
<p>the derivatives of this error metric are calculated and propagated back through the network using a technique called <strong>backpropagation</strong>. Each neuron’s weights are the adjusted relative to how much they contributed to the total error. This process is repeated iteratively until the network error drops below an acceptable threshold.</p>
<h3 id="Perceptrons"><a href="#Perceptrons" class="headerlink" title="Perceptrons"></a>Perceptrons</h3><p>A Perceptron takes a group of weighted inputs, if the date points is in right side, then go through the activation function, output the classification.</p>
<p>![4](/images/neural network4.png)</p>
<p>![5](/images/neural network5.png)</p>
<h3 id="Neural-network-and-Linear-regression"><a href="#Neural-network-and-Linear-regression" class="headerlink" title="Neural network and Linear regression"></a>Neural network and Linear regression</h3><p>It is very simple to classify the data in the figure 2 below. we just need to find the line that can best classify them. Maybe, if the data points are more chaos, we adjust need more lines to help classify. However, this is not the difference between NN and LR. NN can directly classify the Non-linear regions, how we do this, we use the activation function. Discuss below.<br>![2](/images/neural network2.png)<br>![3](/images/neural network3.png)</p>
<h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><p>What is activation functions?<br>Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer. Popular activation functions include <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" target="_blank" rel="noopener">relu</a> and <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html" target="_blank" rel="noopener">sigmoid</a>.<br>Sigmoid almost give up by most deep learning architecture. relu is very popular now.<br><a href="">softmax</a> is very important when output the multiple classes.</p>
<p>Why we use activation function?</p>
<ul>
<li><p><strong>Non-linear</strong> as we say above, for non-linear dataset(e.g. x^2, sin, log), if we use multiple lines to classify, it is linear regression, not neural network. Activation functions model the dataset relationships that we need a non-linear prediction equation.</p>
</li>
<li><p><strong>Continuously differentiable</strong> – do you remember how we adjust the weights of the networks we talked above. We use the gradient descent, gradient descent need the output to have a nice slope so we can compute error derivatives with respect to weights. if the output is discrete values, we can’t proceed.</p>
</li>
<li><p><strong>Fixed range</strong> – activation function typically squash the input data into a narrow range that makes training the model more stable and efficient.</p>
</li>
</ul>
<p>Why we don’t use sigmoid function nowadays? That’s because after multiple layers, the gradient is vanishing. We will never get the lowest error. So the relu function and its <a href="http://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#leakyrelu" target="_blank" rel="noopener">variants</a> become popular.</p>
<p>reference: <a href="https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f" target="_blank" rel="noopener">1</a>, <a href="http://www.sohu.com/a/145367458_468740" target="_blank" rel="noopener">2</a>, <a href="http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#activation-functions" target="_blank" rel="noopener">3</a></p>
<p>Found great resources: <a href="https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/" target="_blank" rel="noopener">Choosing the right Activation Function</a></p>
<p><em>Add 12/21/2018</em></p>
<p>sigmoid function<br>$$ $$</p>
<p>tanh function</p>
<p>relu function</p>
<p>when we want to the dW = dJ/dW</p>
<h3 id="Error-function-loss-function"><a href="#Error-function-loss-function" class="headerlink" title="Error function(loss function)"></a>Error function(loss function)</h3><p>A loss function, or cost function, is a wrapper around our model’s predict function that tells us “how good” the model is at making predictions for a given set of parameters. The loss function has its own curve and its own derivatives. The slope of this curve tells us how to change our parameters to make the model more accurate! We use the model to make predictions. We use the cost function to update our parameters. Our cost function can take a variety of forms as there are many different cost functions available. Popular loss functions include: MSE (L2) and Cross-entropy Loss. <a href="http://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html#loss-functions" target="_blank" rel="noopener">from here</a></p>
<h4 id="Cross-entropy"><a href="#Cross-entropy" class="headerlink" title="Cross-entropy"></a>Cross-entropy</h4><p>First, we need take a look at maximum likelihood. 4 points in the figure. Take two classify lines as an example. Multiply each data point’s probability, and find which lines can get the maximum likelihood.<br>![6](/images/neural network6.png)  </p>
<p>However, the probability is so small, if we have lots of data, the probability will be near 0. So how can we change it to make more easy to distinguish. we will use the log function.   $-log(p)$<br>For binary classification, the cross-entropy can be calculated as:<br>$$[-(ylog(p)+(1-y)log(1-p))]$$<br>For multiple classification, we calculate a separate loss for each class label per observation and sum the result<br>$$-sum_{i=1}^{n} \sum_{j=1}^{m}y_{ij}ln(p_{ij})$$</p>
<p>Last problem, how we maximize the probabilities, which means minimize the cross entropy.<br>In order to minimize the error function, we need to take some derivatives. $[\frac{\partial E}{\partial w_j}]$, $[\frac{\partial E}{\partial b}]$</p>
<p><strong>Gradient Descent Step</strong> Therefore, since the gradient descent step simply consists in subtracting a multiple of the gradient of the error function at every point, then this updates the weights in the following way:<br>$${w_j}’\leftarrow w_j - a\left [ \frac{\partial E}{\partial w_j}\right ]$$<br>similarly, updates the bias:<br>$${b}’\leftarrow b - a\left [ \frac{\partial E}{\partial b}\right ]$$<br>a is the learning rate, is a constant.</p>
<h3 id="Nerual-Network-Architecture"><a href="#Nerual-Network-Architecture" class="headerlink" title="Nerual Network Architecture"></a>Nerual Network Architecture</h3><p>How we combine several perceptrons into a third, more complicated one?<br>As figure 3, two lines help to classify the dataset. Simply as this and add the function of activation to change this linear to nonlinear classification.<br>![7](/images/neural network7.png)</p>
<p>More detail:   </p>
<iframe width="854" height="480" src="https://www.youtube.com/embed/Boy3zHVrWB4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

<p>For deep learning, it contains more layers and more nodes than we discuss above.<br>![8](/images/neural network8.png)![9](/images/neural network9.png)</p>
<h3 id="Feedforward-and-Backpropagation"><a href="#Feedforward-and-Backpropagation" class="headerlink" title="Feedforward and Backpropagation"></a>Feedforward and Backpropagation</h3><h3 id="Train-Optimization"><a href="#Train-Optimization" class="headerlink" title="Train Optimization"></a>Train Optimization</h3><p>Why we use Optimization?<br>there are many things that can fail when train out model. For example, our architecture can be poorly chosen, our data can be noisy and our model could maybe taking years to run.</p>
<h4 id="Epoch-and-Batch"><a href="#Epoch-and-Batch" class="headerlink" title="Epoch and Batch"></a>Epoch and Batch</h4><p>One epoch means all data go one iterations, batch means divide the dataset into several parts.<br>For example, if I have 1000 data points, the batach_size is 100, then I need 10 times iterations to accomplish 1 times epoch.<br>Why we will have batch size, because train all dataset one time maybe time consuming and lay burden on computer. But it not means increasing batch_size is a good idea. Choose a appropriate paraments is a very important part in deep learning.<br>The same with choose epoch parament.</p>
<h4 id="Overfitting-and-Underfitting"><a href="#Overfitting-and-Underfitting" class="headerlink" title="Overfitting and Underfitting"></a>Overfitting and Underfitting</h4><p>overfitting is you train too much, gain high variance. Underfitting verse is you train less, gain high bias.<br>![10](/images/neural network10.png)</p>
<p>Overfitting and Underfitting is a tradeoff, we can draw the error by model complexity graph.<br>![11](/images/neural network11.png)<br>In this graph, we should choose appropriate number of epochs, in case our test error decrease at first then go up again.</p>
<h4 id="Earlystoppoing"><a href="#Earlystoppoing" class="headerlink" title="Earlystoppoing"></a>Earlystoppoing</h4><p>We can see from last figure, there is a lowest point of val_loss, we should stop at that point. You can see the documentation in <a href="https://keras.io/callback/" target="_blank" rel="noopener">keras document</a>, they use val_loss as monitor to see if the loss is increasing or decreasing. Patience is the number of epochs with no improvement after which training will be stopped.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#in keras</span><br><span class="line">earlyStopping&#x3D;keras.callbacks.EarlyStopping(monitor&#x3D;&#39;val_loss&#39;, patience&#x3D;60, verbose&#x3D;0, mode&#x3D;&#39;auto&#39;)</span><br></pre></td></tr></table></figure>

<h4 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h4><p>from the figure 17, we see that if the step keep the same, it’s hard to get the lowest point. So, he rule of thumb, if you model is not working, decrease the learning rate. The rule: if steep, take long steps; if plain, small step.<br>![17](/images/neural network17.png)</p>
<h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><p>Think of the question?  split the two points.<br>![12](/images/neural network12.png)<br>As we talked above, the probability of the points will say that solution 2 will have smaller error.<br>![13](/images/neural network13.png)<br>However, the smaller error is not good for gradient descent(too sure), just as BertrAIND Russell said:</p>
<blockquote>
<p>The whole problem with artificial intelligence is that bad models are so certain of themselves, and good models so full of doubts.</p>
</blockquote>
<p>![14](/images/neural network14.png)</p>
<p><em>Large coefficients will led to overfitting, so how we deal with it?</em> We will <strong>punish the big coefficients</strong>. This method is called <strong>Regularization</strong>.<br>There are two ways: L1, L2.<br>L1 tends to end up with sparse vectors. That means small weights will tend to go to zero. So if we want to reduce the number of weights and end up with a small set. L1 is also good for feature selection, when there are hundreds of features, L1 can help us select which ones are important.<br>L2 on the other hand, it tries to maintain all the weights homogeneously small. This one normally better for training models.<br>![15](/images/neural network15.png)</p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>This is method that in the process of training, no all the nodes are joined at one time.<br>This is a method can prevent overfitting.</p>
<h4 id="Local-Minima-Momentum"><a href="#Local-Minima-Momentum" class="headerlink" title="Local Minima(Momentum)"></a>Local Minima(Momentum)</h4><p>When we do gradient descent, it sometimes will stuck in local minima. How to solve this problem.</p>
<p>One is to randomly start. It starts from a few different random places and do gradient descend form all of them. This will increse the probability that we will get to the minimum, at least good local minimum.</p>
<p>Another is called momentum, just as shown below, the step happened long before will matter less than the ones that happened recently, it will gets us over the hump.<br>![16](/images/neural network16.png)</p>
<p>There are lots of optimizers, it is very important to help us find the smallest error and get a good model.<br>Check this <a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">blog</a>, it introduces lots of optimizers.<br>And the optimizers in <a href="https://keras.io/optimizers/" target="_blank" rel="noopener">keras</a>.</p>
<p>To be continue…</p>
<blockquote>
<p>Announcment: Most of the content I summary are from Udacity NanoDegree Class–Machine Learning and this cheatsheet website,<a href="http://ml-cheatsheet.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Machine Learning Cheatsheet</a>.</p>
</blockquote>
</div><div class="tags"><a href="/tags/deep-learning/">deep learning</a><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post-nav"><a class="pre" href="/2018/05/29/Deep-Learning-2/">Deep Learning(2)</a><a class="next" href="/2018/05/18/machine-learning-with-python-2/">machine learning with python(2)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://jononearth.com/2018/05/27/Deep-Learning-1/';
    this.page.identifier = '2018/05/27/Deep-Learning-1/';
    this.page.title = 'Deep Learning(1)';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//jononearth.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//jononearth.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://jononearth.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/code/" style="font-size: 15px;">code</a> <a href="/tags/Data-structure/" style="font-size: 15px;">Data structure</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/ensmeble/" style="font-size: 15px;">ensmeble</a> <a href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" style="font-size: 15px;">经典算法</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" style="font-size: 15px;">数据科学</a> <a href="/tags/numpy/" style="font-size: 15px;">numpy</a> <a href="/tags/%E5%BB%BA%E7%AB%99/" style="font-size: 15px;">建站</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 15px;">面试题</a> <a href="/tags/%E8%B0%83%E5%8F%82/" style="font-size: 15px;">调参</a> <a href="/tags/%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D/" style="font-size: 15px;">室内定位</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/" style="font-size: 15px;">每日一吐</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E8%AF%9D%E4%B8%9C%E6%B8%B8/">大话东游</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AF%86%E5%AE%A4%E9%80%83%E8%84%B1/">密室逃脱</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/">机械公敌</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/">极客时间</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/">黑客帝国</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/05/02/2-3%E6%9C%88%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/">2,3月每日一吐</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/05/02/%E7%96%AB%E6%83%85%E4%B8%8B%E7%9A%84%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90202004/">2,3月每日一吐</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/02/15/hello-world/">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/01/2019%E6%80%BB%E7%BB%93%E5%A4%A7%E4%BC%9A/">2019总结大会</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/30/%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98/">建站备忘</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/2018-05-07-machine-leaning-index/">Machine Learning 资源汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E4%B9%8B%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/">Indoor Positioning Resources Document</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/01/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90201812/">每日一吐201812</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/01/2019%E6%96%B0%E5%B9%B4flag/">2019新年flag</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/">机器学习知识点</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//jononearth.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">JonOnEarth.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>