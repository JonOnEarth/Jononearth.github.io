<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>machine learning with python(3) | JonOnEarth</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><meta name="generator" content="Hexo 4.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">machine learning with python(3)</h1><a id="logo" href="/.">JonOnEarth</a><p class="description">加油中国</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/DailyNote/"><i class="fa fa-user"> DailyNote</i></a><a href="/about/"><i class="fa fa-user"> About</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">machine learning with python(3)</h1><div class="post-meta">Jun 4, 2018<span> | </span><span class="category"><a href="/categories/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/">机械公敌</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a class="disqus-comment-count" data-disqus-identifier="2018/06/04/machine-learning-with-python-3/" href="/2018/06/04/machine-learning-with-python-3/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Scale"><span class="toc-number">1.</span> <span class="toc-text">Scale</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#scale优点"><span class="toc-number">1.1.</span> <span class="toc-text">scale优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#why"><span class="toc-number">1.2.</span> <span class="toc-text">why?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#总结："><span class="toc-number">1.3.</span> <span class="toc-text">总结：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#几种scale的方法"><span class="toc-number">1.4.</span> <span class="toc-text">几种scale的方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征工程"><span class="toc-number">2.</span> <span class="toc-text">特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#特征构建"><span class="toc-number">2.1.</span> <span class="toc-text">特征构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征选择"><span class="toc-number">2.2.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Filter"><span class="toc-number">2.2.1.</span> <span class="toc-text">Filter</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#皮尔森相关系数法-Pearson-Correlation"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">皮尔森相关系数法(Pearson Correlation)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#方差选择法"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">方差选择法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#卡方检验"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">卡方检验</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#互信息法"><span class="toc-number">2.2.1.4.</span> <span class="toc-text">互信息法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Wrapper"><span class="toc-number">2.2.2.</span> <span class="toc-text">Wrapper</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#递归特征消除法"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">递归特征消除法</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Embedded"><span class="toc-number">2.2.3.</span> <span class="toc-text">Embedded</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#基于惩罚项的特征选择法"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">基于惩罚项的特征选择法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#基于树模型的特征选择法"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">基于树模型的特征选择法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征提取"><span class="toc-number">2.3.</span> <span class="toc-text">特征提取</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#主成分分析法（PCA）"><span class="toc-number">2.3.1.</span> <span class="toc-text">主成分分析法（PCA）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#线性判别分析法（LDA）"><span class="toc-number">2.3.2.</span> <span class="toc-text">线性判别分析法（LDA）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据集的重新划分"><span class="toc-number">3.</span> <span class="toc-text">数据集的重新划分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#KFold"><span class="toc-number">3.1.</span> <span class="toc-text">KFold</span></a></li></ol></li></ol></div></div><div class="post-content"><p>系列1主要讲了python环境，软件安装，机器学习比较重要的库，还有python, numpy, pandas, mattplotlib的crash course. <a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-1/">machine learning with python 1</a></p>
<p>系列2主要讲了数据来了，怎么理解数据。主要是一些统计手段，可视化。<a href="https://jononearth.com/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/machine-learning-with-python-2/">machine learning with python 2</a></p>
<p>本系列3，主要讲prepare data for machine leanring. 数据塞进机器学习算法前，对data 还需要处理什么。</p>
<h2 id="Scale"><a href="#Scale" class="headerlink" title="Scale"></a>Scale</h2><h3 id="scale优点"><a href="#scale优点" class="headerlink" title="scale优点"></a>scale优点</h3><p>关于data为什么要scale(归一化）。有以下几个原因：</p>
<ol>
<li>归一化可以加快梯度下降求最优解的速度；</li>
<li>归一化有可能提高精度（如KNN等算法）</li>
</ol>
<h3 id="why"><a href="#why" class="headerlink" title="why?"></a>why?</h3><ul>
<li>对于说可以加快速度，可以看这Boston 房屋价格变动的这个例子；（在吴恩达的课程中有很经典的案例，需要达人整理的吴恩达笔记可以留邮箱给我，发给你）<br>下图举了一个boston房价的例子，如果刻度不同，得到的loss function的等高线图，<br><img src="/images/machine-learning-with-python-3-1.png" alt="Boston price cost function"><br>我们看到左边的等高线是狭长的，右边是偏圆形的。在左图中，一个scale是0<del>2000，一个scale是1</del>5。这对于Gredient descent下降速度是很有影响的。</li>
<li>提高精度。是因为一些算法需要计算距离的时候，如果某一个特征值很大，那么算法就会取决于这个特征值，影响算法精度。scale可以避免这种异常值的出现。</li>
</ul>
<h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ol>
<li>概率模型不需要归一化，因为模型不关心变量的取值，只关心变量的分布和变量的条件概率；</li>
<li>SVM、线性回归之类的最优化问题需要归一化，是否归一化主要在于是否关心变量取值；</li>
<li>神经网络需要标准化处理，一般变量的取值在-1到1之间，这样做是为了弱化某些变量的值较大而对模型产生影响。</li>
<li>在K近邻算法中，如果不对解释变量进行标准化，那么具有小数量级的解释变量的影响就会微乎其微。</li>
</ol>
<ul>
<li>From:<a href="https://blog.csdn.net/zenghaitao0128/article/details/78361038" target="_blank" rel="noopener">https://blog.csdn.net/zenghaitao0128/article/details/78361038</a></li>
</ul>
<h3 id="几种scale的方法"><a href="#几种scale的方法" class="headerlink" title="几种scale的方法"></a>几种scale的方法</h3><ul>
<li><p>MinMaxScaler<br>$$[x^{‘}= \frac{x-min(x)}{max(x)-min(x)}]$$</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; MinMaxScaler(feature_range&#x3D;(0, 1))</span><br><span class="line">rescaledX &#x3D; scaler.fit_transform(X)</span><br><span class="line"># summarize transformed, 输出位数小数点后3位</span><br><span class="line">data set_printoptions(precision&#x3D;3)</span><br></pre></td></tr></table></figure>
</li>
<li><p>StandardScaler<br>$$[x^{‘}= \frac{x-u}{\sigma }]$$<br>$[u]$是平均值，$[\sigma]$是标准差，经过处理后，数据符合标准正态分布。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; StandardScaler().fit(X)</span><br><span class="line">rescaledX &#x3D; scaler.transform(X)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Normalizer（范化）<br>主要有两种范化 L1, L2， sklearn中默认是L2<br>$$x^{‘}=\frac{x}{\left | x \right |}$$<br>说一下做范化的好处：适合在稀疏的数据中应用，比如1000维，只有几维是是非0的。这时候可以应用范化。 这里分母是||x||=square(x1^2+x2^2+…)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler &#x3D; Normalizer().fit(X)</span><br><span class="line">normalizedX &#x3D; scaler.transform(X)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Binarize<br>有一个阈值（threshold)，如果有值超过这个值赋1，小于该值为0.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">binarizer &#x3D; Binarizer(threshold&#x3D;0.0).fit(X)</span><br><span class="line">binaryX &#x3D; binarizer.transform(X)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>Mark下: 这几种scale方法，前两个比较好理解，但范化不是很理解，为什么能应用到稀疏的数据中，怎么应用，有待进一步查资料。</p>
<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><p>这个地方<strong>mark</strong>一下 ，数学公式比较多。短期内还没法一下全部搞懂。我先把大概给弄上来。</p>
<blockquote>
<p>数据决定了机器学习的上限，而算法只是尽可能逼近这个上限</p>
</blockquote>
<p>这里的数据指的就是经过特征工程得到的数据。特征工程指的是把原始数据转变为模型的训练数据的过程，它的目的就是获取更好的训练数据特征，使得机器学习模型逼近这个上限。特征工程能使得模型的性能得到提升，有时甚至在简单的模型上也能取得不错的效果。特征工程在机器学习中占有非常重要的作用，一般认为括<strong>特征构建、特征提取、特征选择</strong>三个部分。</p>
<p>特征构建比较麻烦，需要一定的经验。 特征提取与特征选择都是为了从原始特征中找出最有效的特征。它们之间的区别是特征提取强调通过特征转换的方式得到一组具有明显物理或统计意义的特征；而特征选择是从特征集合中挑选一组具有明显物理或统计意义的特征子集。两者都能帮助减少特征的维度、数据冗余，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。</p>
<p>reference: <a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a></p>
<p>这张图特别好，来自：<a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibao/p/6252280.html</a></p>
<p><img src="/images/machine-learning-with-python-3-2.png" alt="一图概览特征工程"></p>
<p>补充：关于维度灾难的补充：<a href="https://www.jianshu.com/p/d7aec8b41356" target="_blank" rel="noopener">https://www.jianshu.com/p/d7aec8b41356</a><br>有时候维度太多，会造成维度灾难，具体看链接。所以进行降维很有必要。</p>
<h3 id="特征构建"><a href="#特征构建" class="headerlink" title="特征构建"></a>特征构建</h3><p>特征构建是指从原始数据中人工的找出一些具有物理意义的特征。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。除此之外，属性分割和结合是特征构建时常使用的方法。结构性的表格数据，可以尝试组合二个、三个不同的属性构造新的特征，如果存在时间相关属性，可以划出不同的时间窗口，得到同一属性在不同时间下的特征值，也可以把一个属性分解或切分，例如将数据中的日期字段按照季度和周期后者一天的上午、下午和晚上去构建特征。总之特征构建是个非常麻烦的问题，书里面也很少提到具体的方法，需要对问题有比较深入的理解。From:<a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">机器学习之特征工程</a></p>
<h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：</p>
<ul>
<li>特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。</li>
<li>特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。</li>
</ul>
<p>根据特征选择的形式又可以将特征选择方法分为3种：</p>
<ol>
<li>Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。</li>
<li>Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。</li>
<li>Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。</li>
</ol>
<p>我们使用sklearn中的feature_selection库来进行特征选择</p>
<h4 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h4><h5 id="皮尔森相关系数法-Pearson-Correlation"><a href="#皮尔森相关系数法-Pearson-Correlation" class="headerlink" title="皮尔森相关系数法(Pearson Correlation)"></a>皮尔森相关系数法(Pearson Correlation)</h5><h5 id="方差选择法"><a href="#方差选择法" class="headerlink" title="方差选择法"></a>方差选择法</h5><h5 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h5><h5 id="互信息法"><a href="#互信息法" class="headerlink" title="互信息法"></a>互信息法</h5><h4 id="Wrapper"><a href="#Wrapper" class="headerlink" title="Wrapper"></a>Wrapper</h4><h5 id="递归特征消除法"><a href="#递归特征消除法" class="headerlink" title="递归特征消除法"></a>递归特征消除法</h5><h4 id="Embedded"><a href="#Embedded" class="headerlink" title="Embedded"></a>Embedded</h4><h5 id="基于惩罚项的特征选择法"><a href="#基于惩罚项的特征选择法" class="headerlink" title="基于惩罚项的特征选择法"></a>基于惩罚项的特征选择法</h5><h5 id="基于树模型的特征选择法"><a href="#基于树模型的特征选择法" class="headerlink" title="基于树模型的特征选择法"></a>基于树模型的特征选择法</h5><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><h4 id="主成分分析法（PCA）"><a href="#主成分分析法（PCA）" class="headerlink" title="主成分分析法（PCA）"></a>主成分分析法（PCA）</h4><p>分析PCA很好的文章：<a href="https://www.cnblogs.com/hadoop2015/p/7419087.html" target="_blank" rel="noopener">https://www.cnblogs.com/hadoop2015/p/7419087.html</a><br>PCA是从特征的角度协方差角度： 求出协方差矩阵的特征值和特征向量，然后将特征向量按特征值的大小排序取出前K行组成矩阵P（这个P就是我们对角化协方差矩阵的时所使用的P, 具体的可以看看矩阵对角化的过程）， 这个P就是一组正交变化基， 然后将原始的矩阵X，左乘P，也就是将X变换到P组成的正交基中，然后PX＝Y就是降维后的矩阵。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X &#x3D; array[:,0:8]</span><br><span class="line">Y &#x3D; array[:,8]</span><br><span class="line"># feature extraction</span><br><span class="line">pca &#x3D; PCA(n_components&#x3D;3)</span><br><span class="line">fit &#x3D; pca.fit(X)</span><br><span class="line">x_new &#x3D; fit.transform(x)</span><br></pre></td></tr></table></figure>

<h4 id="线性判别分析法（LDA）"><a href="#线性判别分析法（LDA）" class="headerlink" title="线性判别分析法（LDA）"></a>线性判别分析法（LDA）</h4><p>LDA则是在已知样本的类标注， 希望投影到新的基后使得不同的类别之间的数据点的距离更大，同一类别的数据点更紧凑。</p>
<blockquote>
<p>特征工程更多参考：<br><a href="https://www.cnblogs.com/weibao/p/6252280.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibao/p/6252280.html</a><br><a href="https://www.cnblogs.com/wxquare/p/5484636.html" target="_blank" rel="noopener">https://www.cnblogs.com/wxquare/p/5484636.html</a><br><a href="http://dataunion.org/14072.html" target="_blank" rel="noopener">http://dataunion.org/14072.html</a><br><a href="https://blog.csdn.net/Dream_angel_Z/article/details/49388733" target="_blank" rel="noopener">https://blog.csdn.net/Dream_angel_Z/article/details/49388733</a></p>
</blockquote>
<h2 id="数据集的重新划分"><a href="#数据集的重新划分" class="headerlink" title="数据集的重新划分"></a>数据集的重新划分</h2><h3 id="KFold"><a href="#KFold" class="headerlink" title="KFold"></a>KFold</h3><p><a href="https://blog.csdn.net/FontThrone/article/details/79220127" target="_blank" rel="noopener">Sklearn中的CV与KFold详解</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#Stratified k-fold:实现了分层交叉切分*</span><br><span class="line">from sklearn.model_selection import StratifiedKFold</span><br><span class="line">X &#x3D; np.array([[1, 2, 3, 4],</span><br><span class="line">              [11, 12, 13, 14],</span><br><span class="line">              [21, 22, 23, 24],</span><br><span class="line">              [31, 32, 33, 34],</span><br><span class="line">              [41, 42, 43, 44],</span><br><span class="line">              [51, 52, 53, 54],</span><br><span class="line">              [61, 62, 63, 64],</span><br><span class="line">              [71, 72, 73, 74]])</span><br><span class="line"></span><br><span class="line">y &#x3D; np.array([1, 1, 0, 0, 1, 1, 0, 0])</span><br><span class="line"></span><br><span class="line">stratified_folder &#x3D; StratifiedKFold(n_splits&#x3D;4, random_state&#x3D;0, shuffle&#x3D;False)</span><br><span class="line">for train_index, test_index in stratified_folder.split(X, y):</span><br><span class="line">    print(&quot;Stratified Train Index:&quot;, train_index)</span><br><span class="line">    print(&quot;Stratified Test Index:&quot;, test_index)</span><br><span class="line">    print(&quot;Stratified y_train:&quot;, y[train_index])</span><br><span class="line">    print(&quot;Stratified y_test:&quot;, y[test_index],&#39;\n&#39;)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#结果：</span><br><span class="line">Stratified Train Index: [1 3 4 5 6 7]</span><br><span class="line">Stratified Test Index: [0 2]</span><br><span class="line">Stratified y_train: [1 0 1 1 0 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 2 4 5 6 7]</span><br><span class="line">Stratified Test Index: [1 3]</span><br><span class="line">Stratified y_train: [1 0 1 1 0 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 1 2 3 5 7]</span><br><span class="line">Stratified Test Index: [4 6]</span><br><span class="line">Stratified y_train: [1 1 0 0 1 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br><span class="line"></span><br><span class="line">Stratified Train Index: [0 1 2 3 4 6]</span><br><span class="line">Stratified Test Index: [5 7]</span><br><span class="line">Stratified y_train: [1 1 0 0 1 0]</span><br><span class="line">Stratified y_test: [1 0]</span><br></pre></td></tr></table></figure>
</div><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/">数据科学</a></div><div class="post-nav"><a class="pre" href="/2018/06/15/%E8%B0%83%E5%8F%82%E6%80%BB%E7%BB%93/">Deep learning(3)_调参总结</a><a class="next" href="/2018/05/29/Deep-Learning-2/">Deep Learning(2)</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://jononearth.com/2018/06/04/machine-learning-with-python-3/';
    this.page.identifier = '2018/06/04/machine-learning-with-python-3/';
    this.page.title = 'machine learning with python(3)';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//jononearth.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//jononearth.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://jononearth.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/code/" style="font-size: 15px;">code</a> <a href="/tags/Data-structure/" style="font-size: 15px;">Data structure</a> <a href="/tags/algorithm/" style="font-size: 15px;">algorithm</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E6%97%A0%E8%B6%A3/" style="font-size: 15px;">生活无趣</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" style="font-size: 15px;">机器学习</a> <a href="/tags/ensmeble/" style="font-size: 15px;">ensmeble</a> <a href="/tags/%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95/" style="font-size: 15px;">经典算法</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/pandas/" style="font-size: 15px;">pandas</a> <a href="/tags/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/" style="font-size: 15px;">数据科学</a> <a href="/tags/numpy/" style="font-size: 15px;">numpy</a> <a href="/tags/%E5%BB%BA%E7%AB%99/" style="font-size: 15px;">建站</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/" style="font-size: 15px;">面试题</a> <a href="/tags/%E8%B0%83%E5%8F%82/" style="font-size: 15px;">调参</a> <a href="/tags/%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D/" style="font-size: 15px;">室内定位</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90/" style="font-size: 15px;">每日一吐</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E8%AF%9D%E4%B8%9C%E6%B8%B8/">大话东游</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E8%AF%9D%E8%A5%BF%E6%B8%B8/">大话西游</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AF%86%E5%AE%A4%E9%80%83%E8%84%B1/">密室逃脱</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9C%BA%E6%A2%B0%E5%85%AC%E6%95%8C/">机械公敌</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%9E%81%E5%AE%A2%E6%97%B6%E9%97%B4/">极客时间</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%BB%91%E5%AE%A2%E5%B8%9D%E5%9B%BD/">黑客帝国</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/02/15/hello-world/">Hello World</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/01/2019%E6%80%BB%E7%BB%93%E5%A4%A7%E4%BC%9A/">2019总结大会</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/30/%E5%BB%BA%E7%AB%99%E5%A4%87%E5%BF%98/">建站备忘</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/2018-05-07-machine-leaning-index/">Machine Learning 资源汇总</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/12/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%94%E7%94%A8%E4%B9%8B%E5%AE%A4%E5%86%85%E5%AE%9A%E4%BD%8D%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%BA%90%E6%95%B4%E7%90%86/">Indoor Positioning Resources Document</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/01/%E6%AF%8F%E6%97%A5%E4%B8%80%E5%90%90201812/">每日一吐201812</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/01/2019%E6%96%B0%E5%B9%B4flag/">2019新年flag</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/09/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9/">机器学习知识点</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/16/Algorithm-1/">Data structure and algorithm(1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/08/10/%E7%9B%90%E6%B9%96%E5%88%B0%E9%BB%84%E7%9F%B36%E5%A4%A9%E4%B8%8D%E5%AE%8C%E5%85%A8%E6%94%BB%E7%95%A5/">盐湖到黄石6天不完全攻略</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> Recent Comments</i></div><script type="text/javascript" src="//jononearth.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">JonOnEarth.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>